# chapter17 코드 최적화
- 코드가 보다 빠르게 실행되도록 하는 최적화는 순환적인 과정임
  - 가장 큰 병목을 찾자
  - 그 병목을 제거하려고 시도하여라
  - 자신의 코드가 충분히 빠를 때 까지 반복하여라
- 숙련된 프로그래머일지라도 자신의 코드에서 병목을 식별하는 것을 어려운 일
- 직관에 의존하지 않고, 자신의 코드를 프로파일(profile)해야 함
- 실제 입력을 사용하고 각 개별 연산의 실행 시간을 측정한다
- 가장 중요한 병목을 단번에 찾아 그것을 제거하려고 시도할 수 있음
- 성능 개선에 대한 일반적인 조언을 제공하는 것은 어렵지만, 여러 상황에 적용될 수 있는 여섯가지 기법을 제안
- 또한 보다 빠른 코드가 정확한 코드인지를 확신하는 데 도움을 주는 성능 최적화에 대한 일반적 전략을 제시할 것임
- 병목을 모두 제거하려는 데 쉽게 빠져들 수 있는데, 그러지 말자
- 항상 코드의 목표 시간을 설정하고 그 목표까지만 최적화해라

#### 사전 준비
- 이 장에서는 R 코드의 성능을 이해하기 위해 `lineprof` 패키지를 이용할 것임

## 17.1 성능 측정
- 성능에 대한 이해를 위해 프로파일러(profiler)를 이용해라
- 수 없이 많은 종류의 프로파일러가 있음
- R은 샘플링 또는 통계적 프로파일러라고 불리는 완전히 단순한 유형을 사용
- 샘플링 프로파일러는(어느 함수가 그 함수를 호출했는지 등에 따라)몇 밀리세컨드마다 코드의 실행을 중지하고 현재 실행 중인 함수를 기록함
- 예를 들어 아래의 `f()`를 살펴보자
~~~r
library(lineprof)
f <- function(){
    pause(0.1)
    g()
    h()
}

g <- function(){
    pause(0.1)
    h()
}

h <- function(){
    pause(0.1)
}
~~~
- `Sys.sleep()` 대신 `puase()`를 사용했는데, `Sys.sleep()`은 명령하지 않는 이상 어떤 컴퓨팅 시간도 사용하지 않아 프로파일링 출력에 나타나지 않기 때문
- 0.1초마다 코드 실행을 멈추면서 f() 실행을 프로파일링했다면 다음과 같이 그 결과를 볼 수 있음
- 각 줄은 프로파일러가 한 순간을 나타내고, 함수 호출은 >로 중첩됨
- 다음 결과는 코드가 f() 실행에 0.1초, g() 실행에 0.2초, h() 실행에 0.1초를 사용한 것을 보여줌
~~~r
f()
f() > g()
f() > g() > h()
f() > h()
~~~
- 아래의 코드를 사용하여 f()를 실제로 프로파일링한다면 우리는 그렇게 명확한 결과를 얻지 못할 것임
~~~r
tmp <- tempfile()
Rprof(tmp, interval = 0.1)
f()
Rprof(NULL)
~~~
- 왜냐하면 프로파일링이 코드의 속도를 크게 떨어트리지 않으며, 정확하게 수행하기도 어렵기 떄문
- `RProf()`의 타협점은 샘플링인데, 전체 실행에 최소한의 영향을 주지만 결과적으로 확률적임
- 타이머의 정확성과 각 작업에 소요되는 시간에는 약간의 변동성이 있으므로 프로파일을 작성할 때마다 약간 다른 답이 표시됨
- `summary Rprof()`, `proftools`, `profr` 패키지 같은 여러 가지 대안이 있음

## 17.2 성능 개선하기
- 병목을 식별하기 위해 프로파일링을 사용한 후에는 그것을 빠르게 개선해야 할 필요가 있음
- 이번 절에는 널리 사용되는 많은 기법을 소개함
  - 기존의 해법을 찾아볼 것
  - 덜 작업할 것
  - 벡터화할 것
  - 병렬화할 것
  - 복사를 피할 것
  - 바이트 코드 컴파일할 것
- 마지막 기법은 C++같은 더 빠른 언어로 다시 작성하는 것
- 특정 기법을 익히기 전에, 먼저 성능 관련한 작업을 할 때 유용한 일반적인 전략과 구조적 스타일을 설명함

## 17.3 코드 조직화
- 코드를 빠르게 만드려고 할 때, 쉽게 빠질 수 있는 두 가지 함정이 있음
  - 더 빠르지만, 잘못된 코드를 작성하는 것
  - 더 빠를 꺼라 생각하지만, 실제로는 그렇지 않은 코드를 작성하는 것
- 아래에 설명 된 전략은 이런 함정을 피하는데 도움이 됨
- 병목을 해결할 때 여러 가지 접근법을 생각해 볼 수 있음. 각각의 접근법에 따른 함수를 작성하고 모든 관련 행동을 캡슐화해라. 이렇게 하면 각 접근이 올바른 결과를 반환하는지 확인하고 실행하는 데 걸리는 시간을 쉽게 확인 가능
- 이런 전략을 설명하기 위해 평균을 계산하는 두 가지 방벙을 비교해 볼 것임
~~~r
mean1 <- function(x) mean(x)
mean2 <- function(x) sum(x) / length(x)
~~~
- 시도하는 모든 것. 심지어 실패일지라도 그것을 기록할 것을 권장함
- 나중에 비슷한 문제가 발생하면 시도했던 모든 것을 보는 것이 유용할 것임
- 그런 다음 대표적인 테스트 케이스를 생성해라. 그 케이스는 문제의 본질을 포착하기에 충분히 크지만, 실행하는 데는 몇 초 밖에 걸리지 않을 정도로 작아야 함
- 접근법을 비교하기위해 여러 번 테스트 케이스를 실행해야 하기 때문에 시간이 오래 걸리는 것은 필요치 않음
- 한편으로 결과가 실제 문제로까지 확장되지 않을 수 있기 때문에 사례가 너무 작을 필요는 없음
- 모든 변형이 동일한 결과를 반환하는지 빠르게 확인하기 위해 이 테스트 케이스를 이용해라
- `stopifnot()`과 `all.equal()`을 이용하면 쉽게 해낼 수 있음
- 가능한 출력이 적은 실제 문제의 경우 확실히 우연한 답을 반환하는 것을  
방지하기 위해 더 많은 테스트가 필요할 수 있음
~~~r
x <- runif(100)
stopifnot(all.equal(mean1(x), mean2(x)))
~~~
- 마지막으로 각 변형이 실행하는 데 얼마나 오래 걸리는지 비교하기 위해 microbenchmark 패키지를 사용해라
- 문제가 더 큰 경우 2초 내라는 시간 안에 실행되도록 times 파라미터를 줄이도록 해라
- 시간 중간값에 초점을 맞춰 측정의 가변성을 평가하기 위해 3/4 분위수와 1/4 분위수를 이용해라
~~~r
microbenchmark::microbenchmark(
  mean1(x),
  mean2(x)
)

#> Unit: microseconds
#>     expr  min   lq   mean median   uq  max neval
#>  mean1(x) 17.1 17.4 18.837   17.6 17.8 76.6   100
#>  mean2(x)  2.6  2.8  3.599    3.2  3.4 34.7   100
~~~
- 실험을 시작하기 전에 병목이 더 이상 문제가 되지 않은 시점을 정의하는 목표 속도가 있어야 함
- 이런 목표를 설정하는 것은 중요한데 코드를 과최적화 하는 데 소중한 시간을 낭비할 필요가 없기 때문

## 17.4 이미 누군가 그 문제를 해결하지는 않았는가? 
- 코드를 정리하고, 생각한 모든 변형을 파악하고 나면 다른 사람들이 이미 한 것을 살펴보는 것이 자연스러움
- 다른 패키지를 찾아보는 것은 좋은 방법 중 하나인데, 다음의 두 곳에서 시작하는 것이 좋음
  - CRAN Task views 
  - CRAN 페이지에 나와있는 Rcpp의 의존성을 역변환해라

## 17.5 가능한 한 적게 작업해라
- 함수를 더 빠르게 만드는 가장 쉬운 방법은 함수를 덜 사용하도록 하는 것
- 이를 수행하는 한 가지 방법은 특정 유형의 입력 또는 출력에 더 알맞게 조정된 함수를 사용하는 것
- 예를 들어 다음과 같은 것들이 있음
  - `rosSums()`, `colSums()`, `rowMeans()`, `colMeans()`는 벡터화되었기 때문에 `apply()`를 사용하는 동등한 기동보다 빠름
  - `vapply()` 는 출력 유형을 미리 정하기 때문에 `sapply()` 보다 빠름
  - 벡터에 단일 값이 들어있는지 알고 싶은 경우 `any(x == 10)`이 `10 %in% x` 보다 훨씬 빠름  
  그 이유는 논리 조건이 집합에 포함을 테스트하는 것보다 간단하기 때문
  - 이 지식을 직접 경험하려면 대체 함수가 있음을 알아야 하는데, 좋은 어휘력을 가질 필요가 있음
- 4장에서 시작하여 R 코드를 규칙적으로 읽어 어휘력을 높이도록 하자
- 코드를 읽을 수 있는 좋은 곳으로 R-help Maling list와 stackoverflow가 있음
- 일부 함수는 입력을 특정 유형으로 강제 형변환함
- 입력이 올바른 유형이 아닌 경우 함수는 추가 작업을 병행해야 함
- 이 대신에 데이터를 그대로 사용하는 함수를 찾거나 데이터를 저장하는 방식을 변경해라
- 이 문제의 가장 일반적인 사례는 데이터 프레임에 `apply()`를 사용하는 것
- `apply()`는 입력을 항상 메트릭스로 변환함. 이 오류는 (데이터 프레임이 메트릭스 보다 일반적이기 때문에) 범하기 쉬울 뿐만 아니라 속도도 느림
- 다른 함수는 문제에 대한 자세한 정보를 제공하면 작업량이 줄어들 수 있음
- 문서를 주의 깊게 읽고 다양한 인자로 실험하는 것은 항상 가치가 있음
~~~r
read.csv() : colClasses()로 알려진 열 유형을 지정해라
factor()   : 알려진 계급을 level로 지정
cut()      : 라벨이 필요하지 않다면 labels = FALSE로 라벨을 생성하지 않거나 더 잘 사용하려면 문서의 see also에서 언급한 것처럼 `findInterval()` 을 사용해라
unlist(x, use.names = FALSE) 가 unlist() 보다 훨씬 빠름
interaction() : 기존 데이터에서 필요한 것이 교집합이라면 drop = TRUE 를 사용 
~~~
- 때때로 메소드 디스패치를 사용하여 함수를 더 빠르게 만들 수 있음
- 메소드를 호출하는 경우 메소드 탐색을 한 번만 수행하면 그 비용을 피할 수 있음
  - S3의 경우 `generic()` 대신 `generic.class()`를 호출할 수 있음
  - S4의 경우 `findMethod()` 메소드를 사용하여 메소드를 찾아서 변수에 저장한 다음, 그 함수를 호출하여 이 작업을 수행할 수 있음
- 예를 들어, 작은 벡터에 대해 `mean.default()`를 호출하는 것은 `mean()`을 호출하는 것보다 훨씬 빠름
~~~r
x <- runif(1e2)

microbenchmark(
  mean(x),
  mean.default(x)
)

#> Unit: microseconds
#>            expr   min   lq   mean median   uq  max neval
#>         mean(x)  14.8 15.2 16.367   15.4 15.6 79.5   100
#>  mean.default(x)  4.5  4.8  5.978    5.3  5.6 45.6   100
~~~
- 이 최적화는 약간 위험한데, `mean.default()`는 거의 두 배나 빠르지만, x가 수치형 벡터가 아니라면 오류 발생. x가 무엇인지 확실하게 안다면 그것을 사용해야 함
- 특정 유형의 입력을 처리하고 있다는 것을 알면 더 빠른 코드를 작성하는 또 다른 방법이 될 수 있음
- 예를 들어, `as.data.frame()`은 각 요소를 데이터 프레임으로 강제형변환 한 후 `rbind()` 로 그것들을 함께 묶기 때문에 아주 느림
- 동일한 길이의 벡터로 된 이름 있는 리스트가 있으면 직접 데이터 프레임으로 변환할 수 있음
- 이 경우 입력에 대해 강력한 가정을 할 수 있다면 기본값보다 약 20배 빠른 메소드를 작성 할 수 있음
~~~r
quickdf <- function(l){
  class(l) <- "data.frame"
  attr(l, "row.names") <- .set_row_names(length(l[[1]]))
  l
}

l <- lapply(1:26, function(i) runif(1e3))
names(l) <- letters

microbenchmark(
  quick_df   = quickdf(l),
  as.data.frame = as.data.frame(l)
)

#> Unit: microseconds
#>          expr   min    lq    mean median   uq     max neval
#>      quick_df  25.6 26.75 131.271   28.3 28.8 10156.8   100
#>  as.data.frame 12.9 13.65  15.499   15.3 15.9    55.9   100
~~~
- 상충 관계에 대해 다시 한 번 주의해라
- 이 메소드는 위험하기 때문에 빠르다. 입력이 적절하지 않으면 잘못된 값을 리턴한다
~~~r
quickdf(list(x = 1, y = 1:2))
~~~  
- 다음 사례는 인접한 값 사이의 차이만을 계산하기를 원하는 경우 점진적인 `diff()` 함수의 단순화를 보여줌
- 각 단계마다 하나의 인자를 특정한 경우로 바꾸고, 그 함수가 여전히 동작하는지 확인
- 초기 함수는 길고 복잡하지만, 인자를 제한함으로써 거의 두 배 빠른 속도로 만들 뿐만 아니라 이해하기도 쉽다
- 먼저 `diff()` 코드를 자신만의 스타일로 다시 만든다
~~~r
# diff function
# -> lag         : vector value 간 간격
# -> differences : 간격 계산시 몇 번의 depths? 

diff1 <- function(x, lag = 1L, differences = 1L){
  ismat <- is.matrix(x)
  xlen <- if (ismat) dim(x)[1L] else length(x)
  if (length(lag) > 1L || length(differences) > 1L) || (lag < 1 || differences < 1))
  stop("'lag' and 'differences' must be integers >= 1")

  if (lag * differences >= xlen){
    return(x[0L])
  }

  r <- unclass(x)
  i1 <- -seq_len(lag)
  if (ismat){
    for (i in seq_len(differences)) {
      r <- r[i1, , drop = FALSE] -
      r[nrow(r):-(nrow(r) - lag + 1L), , drop = FALSE]
    }
  } else {
    for (i in seq_len(differences)){
      r <- r[i1] - r[-length(r):-(length(r) - lag  + 1L)]
    }
  }

  class(r) <- oldclass(x)
  r
}
~~~
- 다음으로, 벡터의 입력을 가정한다. 이것은 `is.matrix()` 테스트와 메트릭스 서브세팅을 사용하는 메소드를 제거할 수 있게 해줌
~~~r
diff2 <- function(x, lag = 1L, differences = 1L){
  xlen <- length(x)
  if (length(lag) > 1L || length(differences) > 1L) || (lag < 1 || differences < 1))
  stop("'lag' and 'differences' must be integers >= 1")

  if (lag * differences >= xlen){
    return(x[0L])
  }

  i1 <- -seq_len(lag) 
    for (i in seq_len(differences)){
      x <- x[i1] - x[-length(x):-(length(x) - lag  + 1L)]
    }
    x
}
~~~
- 이제 `difference = 1L` 이라고 가정한다. 이것은 입력 테스트를 단순화 하고 for 루프를 제거
~~~r
diff3 <- function(x, lag = 1L, differences = 1L){
  xlen <- length(x)
  if (length(lag) > 1L || (lag < 1L)
  stop("'lag' must be integers >= 1")

  if (lag >= xlen){
    return(x[0L])
  }

  i1 <- -seq_len(lag) 
  x[i1] - x[-length(x):-(length(x) - lag  + 1L)]
}
~~~
- 이런식으로 스스로 제약적인 상황을 만들어가면서 코드를 단순화 하면 성능 개선을 할 수 있음
- 더 적은 작업을 수행하는 마지막 사례는 보다 간단한 데이터 구조를 사용하는 것
- 예를 들어, 데이터 프레임의 행을 사용하여 작업할 때 데이터 프레임보다 행 인덱스를 사용하는 것이 더 빠름
- 예를 들어 데이터 프레임의 두 열 사이의 상관 관계에 대한 부트스트랩 추정치를 계산하기를 원한다면 두 가지 기본적인 접근 방식이 있음
- 즉 데이터 프레임 또는 개별 벡터로 작업할 수 있음. 개별 벡터가 더 빠를 수 있음

## 17.6 벡터화
- R을 얼마나 사용했는가는 상관없이, 코드를 벡터화하라는 충고를 들었을 것이다
- 그런데 그것이 실제로 의미하는 것은 무엇인가?
- 코드를 벡터화하는 것은 비록 자주 거치는 단계이긴 하지만 for 루프를 피하는 것만을 말하는 것은 아님
- <b>벡터화(vectorising)</b>는 스칼라가 아니라 벡터에 대해 생각하는, 문제에 대한 <b>전체 객체 접근법(whole object approach)</b>에 관한 것
- 벡터화된 함수에는 두 가지 주요 특성이 있음
  - 벡터화는 많은 문제를 간단하게 만든다. 벡터의 구성 요소에 대해 생각할 필요 없이, 전체 벡터에 대해서만 생각하면 됨
  - 벡터화된 함수에서 루프는 R대신 C로 작성된다. C에서의 루프는 오버헤드가 훨씬 적기 때문에 훨씬 빠름
- 벡터화는 빠른 R 코드 작성에도 중요
- 이것은 단순히 `apply()`, 또는 `lapply()`, `Vectorise()`를 사용하는 것을 의미하지는 않음
- 이들 함수는 그 인터페이스를 개선하지만, 근본적으로 성능을 변경하지는 않음
- 성능을 위해 벡터화를 사용한다는 것은 C로 구현된 기존 R 함수를 찾아 문제에 가장 밀접하게 적용하는 것을 의미 
- 많은 공통적인 성능 병목에 적용되는 벡터화된 함수는 다음과 같음
  - `rowSums()`, `colSums()`, `rowMeans()`, `colMeans()`. 이런 벡터화된 메트릭스 함수는 항상 `apply()`를 사용하는 것보다 빠름. 때때로 이런 함수를 이용하여 벡터화된 다른 함수를 구축할 수 있음
  ~~~
  rowAny <- function(x) rowSums(x) > 0
  rowAll <- function(x) rowSums(x) == ncol(x)
  ~~~
  - 벡터화된 서브세팅을 이용하면 속도를 크게 향상 시킬 수 있음  
    lookup 테이블과 직접 매칭하고 병합하는 기법을 기억해라
  - 한번에 여러 값을 대체하기 위해 서브세팅 할당을 사용할 수 있는 것도 기억하자  
    x가 벡터, 매트릭스 또는 데이터 프레임이라면 `x[is.na(x)] <- 0`은 모든 결측값을 0으로 대체 함
  - 매트릭스 또는 데이터 프레임의 다양한 위치에서 값을 추출하거나 바꾸려면 정스 매트릭스로 서브세팅해라. 
  - 연속된 값을 범주형 값으로 변환하는 경우, `cut()`과 `findInterval()`을 사용하는 방법을 알고 있어야 함
  - `cumsum()`, `diff()` 와 같은 벡터화된 함수에 대해 알고 있어야 함
- 벡터화의 일반화된 사례로 <b>행렬 대수(matrix algebra)</b>가 있음 
- 행렬 대수를 사용하여 문제를 해결하는 방법을 찾아 낼 수 있다면 빠르게 문제를 풀 수 있음
- 행렬 대수 문제를 풀 수 있는 능력의 수준은 경험을 바탕으로 함
- 벡터화의 단점은 연산이 어떻게 확장될지 예측하기가 어렵다는 것
- 다음 사례는 문자 서브세팅을 사용하여 리스트에서 1개, 10개, 100개의 요소를 찾는데 걸리는 시간을 측정한다
- 실제로 다음 사례는 100개의 요소를 찾는데 1개의 요소를 찾는데 걸리는 시간보다 약 9배의 시간이 걸린다는 것을 보여줌
~~~r
lookup <- setNames(as.list(sample(100, 26)), letters)

x1 <- "j"
x10 <- sample(letters, 10)
x100 <- sample(letters, 100, replace = T)

microbenchmark(
  lookup[x1],
  lookup[x10],
  lookup[x100]
)

#> Unit: microseconds
#>         expr    min     lq     mean median      uq    max neval
#>   lookup[x1]  1.801  1.901  2.27998  2.001  2.1010 25.602   100
#>  lookup[x10]  4.300  4.500  4.77597  4.601  4.9005 12.002   100
#>  lookup[x100] 12.101 12.701 13.88505 13.002 13.8010 32.901  100
~~~
- 벡터화가 모든 문제를 해결해 줄 순 없으며, 기존 알고리즘을 벡터화된 접근법을 사용하는 알고리즘으로 억지로 구현하려고 하는 것 보다 C++로 벡터화된 함수를 작성하는 것이 더 좋음

## 17.7 사본 회피
- R 코드가 느린 경우 그 치명적인 원인은 루프로 객체의 크기를 증가시키는 것에 있음
- 더 큰 객체를 만들기 위해 `c()`, `append()`, `cbind()`, `rbind()` 또는 `paste()` 를 사용할 때 마다 R은 먼저 새로운 객체를 위한 공간을 할당해야 하고, 이전 객체를 새로운 공간으로 복사함
- for 루프와 같은 이런 것을 여러번 반복한다면 큰 비용을 부담할 수 있게 됨
- 다음에 이 문제를 보여주는 작은 사례가 있음
- 우리는 먼저 임의의 문자열을 생성하고, `collapse()`를 사용해 순환적인 루프로 그것들을 결합하거나, `paste()`를 사용하여 단일 경로로 결합
- 문자열의 수가 증가함에 따라 `collapse()` 의 성능이 상대적으로 나빠지는 것에 주의하자
- 100개의 문자열을 결합하는 것은 10개의 문자열을 결합하는 것보다 30배나 더 오래 걸림
~~~r
random_string <- function(){
  paste(sample(letters, 50, replace = T), collapse = "")
}

strings10  <- replicate(10, random_string())
strings100 <- replicate(100, random_string())

collapse <- function(xs){
  out <- ""
  for (x in xs){
    out <- paste0(out, x)
  }
  out
}

microbenchmark(
  loop10  = collapse(strings10),
  loop100 = collapse(strings100),
  vec10   = paste(strings10, collapse = ""),
  vec100  = paste(strings100, collapse = "") 
)

#>   expr    min      lq     mean median      uq     max neval
#>  loop10  126.5  132.60  153.831  142.2  153.35   489.7   100
#>  loop100 3337.3 3427.25 3808.337 3553.9 3663.80 17843.8   100
#>   vec10   21.8   23.35   30.195   25.0   31.85   111.7   100
#>  vec100  131.3  134.25  147.998  140.2  151.50   445.5   100
~~~
- 루프에서 객체를 수정하는 것(예를 들어 x[i] <- y)는 x의 클래스에 따라 사본을 생성할 수 있음

## 17.8 바이트 코드 컴파일레이션
- R 2.13.0에서는 일부 코드의 속도를 높일 수 있는 바이트 코드 컴파일러가 도입됨
- 컴파일러를 사용하면 쉽게 속도를 향상시킬 수 있음
- 비록 그것이 스스로 작성한 함수에 잘 맞지 않더라도, 많은 노력과 시간을 투자하지는 않을 것
- 다음 사례는 11.1절의 순수 R 버전의 `lapply()`를 보여 줌
- 컴파일링은 베이스 R에 의해 제공되는 C 버전만큼 아직 빠르지는 않지만 상당한 속도 향상을 제공 
~~~r
lapply2 <- function(x, f, ...){
  out <- vector("list", length(x))
  for (i in seq_along(x)){
    out[[i]] <- f(x[[i]], ...)
  }
  out
}

lapply2_c <- complier::cmpfun(lapply2)

x <- list(1:10, letters, c(F, T), NULL)
microbenchmark(
  lapply2(x, is.null),
  lapply2_c(x, is.null),
  lapply(x, is.null)
)
~~~

## 17.9 사례 연구: t-검정
- 다음 사례 연구는 위에서 설명한 기술 중 일부를 사용하여 t-검증을 더 빠르게 수행하는 방법을 보여줌
- 1,000개의 실험을 실행했다고 가정하고, 각 실험은 50명의 개인에 대한 데이터를 수집함
- 각 실험의 처음 25명은 그룹 1에 할당되고 나머지는 그룹 2에 할당
- 먼저 이 문제를 소개하기 위한 임의의 데이터를 생성해보자
~~~r
m <- 1000
n <- 50
X <- matrix(rnorm(m * n, mean = 10, sd = 3) , nrow = m)
grp <- rep(1:2, each = n / 2)
~~~
- 이런 형식의 데이터인 경우 `t.test()`를 사용하는 두 가지 방법이 있음
- 수식 인터페이스를 사용하거나 각 그룹에 하나씩 두 개의 벡터를 제공할 수 있음
- <b>실행 시간은 수식 인터페이스가 상당히 느린 것으로 나타남</b>
~~~r
system.time(for(i in 1:m) t.test(X[i, ] ~ grp)$stat)
#> 사용자  시스템 elapsed 
#>  2.65    0.03    2.95 


system.time(
  for(i in 1:m) t.test(X[i, grp == 1], X[i, grp == 2]$stat)
)

#> 사용자  시스템 elapsed 
#>  0.58    0.01    0.65
~~~
- 더 빨리 만들 수 있는 방법은 무엇인가? 첫째, 우리는 더 적은 작업을 하도록 수정하여 볼 수 있음
- `stats:::t.test.default()`의 소스 코드를 보면 t 통계량을 계산하는 것보다 훨씬 많은 것을 볼 수 있음
- 또한 p-값을 계산하고 인쇄할 출력 형식을 지정함
- 우리는 코드를 빠르게 하기 위해 이런 것들을 제거해 볼 수 있음
~~~r 
my_t <- function(x, grp){
  t_stat <- function(x){
    m   <- mean(x)
    n   <- length(x)
    var <- (sum(x - m) ^ 2) / (n - 1)
    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(x[grp == 1])
  g2 <- t_stat(x[grp == 2])

  se_total <- sqrt(g1$var / g1$n  + g2$var / g2$n)
  (g1$m - g2$m) / se_total #- statistics 값 도출
}
system.time(t2 <- apply(X, 1, my_t, grp = grp))

stopifnot(all.equal(t1, t2))
~~~
- 이는 약 6배의 속도 향상을 제공한다
- 이제는 매우 간단한 함수가 되었으므로, 벡터화를 하면 함수를 더 빠르게 만들 수 있음
- 함수 밖에서 배열을 반복하는 대신, `t_stat()`을 수정하여 값 행렬을 처리할 것임
- 그러므로 `mean()`은 `rowMeans()`가 되고, `length()`는 `ncol()`이 되며 `sum()`은 `rowSums()`가 됨
- 나머지 코드는 그대로 유지됨
~~~r
rowstat <- function(X, grp){
   t_stat <- function(X){
    m   <- rowMeans(X)
    n   <- ncol(X)
    var <- (rowSums(X - m) ^ 2) / (n - 1)
    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(X[, grp == 1])
  g2 <- t_stat(X[, grp == 2])

  se_total <- sqrt(g1$var / g1$n  + g2$var / g2$n)
  (g1$m - g2$m) / se_total #- statistics 값 도출
}
system.time(t3 <- rowstat(X, grp))
#> 사용자  시스템 elapsed 
#>    0       0       0 

stopifnot(all.equal(t1, t3))
~~~ 
- 이것이 훨씬 빠름. 이전 작업보다 40배 이상 빨라졌으며, 처음보다 약 1000배 빠름

## 17.10 병렬화
- 병렬 처리는 여러 코어를 사용하여 문제 하나를 여러 부분으로 나누어 동시에 해결
- 머신의 계산 시간을 줄이지는 않지만 더 많은 컴퓨터 자원을 사용하기 때문에 사용자 시간을 절약할 수 있음
- 여기에서 보여 주고 싶은 것은 병렬 컴퓨팅을 단순히 당혹스러운 병렬 문제라고 부르는 것에 적용하는 것
- 당혹스러운 병렬 문제는 독자적으로 해결할 수 있는 많은 간단한 문제를 말함
- 아주 좋은 사례로서 `lapply()`가 있는데 다른 요소들과 독립적으로 각 요소에서 동작하기 때문
- Linux와 Mac에서 `lapply()`를 병렬 처리하는 것은 매우 쉬움
- 왜냐하면 단지 `lapply()`를 `mclapply()`로 변경하면 되기 때문
~~~r
library(parallel)
cores <- detectCores()
cores

pause <- function(i){
  function(x) Sys.sleep(i)
}

system.time(lapply(1:10, pause(0.25)))
 #> 사용자  시스템 elapsed 
 #>  0.00    0.00    2.67 

#- linux mac 에서 사용가능
system.time(mclapply(1:10, pause(0.25), mc.cores = cores))
~~~
- Windows에서는 약간의 작업이 더 필요
- 먼저 로컬 클러스터를 설정 한 다음 `parLapply()` 를 사용해야 함
~~~r
cluster <- makePSOCKcluster(cores)
system.time(parLapply(1:10, pause(0.25)))
#> 사용자  시스템 elapsed 
#>   0.00    0.00    0.51 
~~~
- `mcapply()`와 `makePSOCKcluster()`의 가장 큰 차이는 `makePSOCKcluster()`에 의해 생성된 프로세스는 새로운 세션으로 시작하는 반면  
  `mcapply()`에 의해 생성된 개별 프로세스는 현재 프로세스를 상속받는다는 것
- 이것은 대부분의 실제 코드가 어떤 설정이 필요하다는 것을 의미
- `clusterEvalQ()`를 사용하여 각 클러스터에서 임의의 코드를 실행한 후 필요한 패키지를 로드하고, `clusterExport()`를 사용하여  
  현재 세션의 객체를 원격 세션에 복사하라
~~~r
x <- 10
psock <- parallel::makePSOCKcluster(1L)
clusterEvalQ(psock, x)

clusterExport(psock, "x")
clusterEvalQ(psock, x)
#> [[1]]
#> [1] 10
~~~
- 병렬 컴퓨팅에는 통신 오버헤드가 있음. 분배될 것들이 매우 작으면 병렬 처리는 약이 되기보다 독이 될 수 있음
- 컴퓨팅과 통신 비용의 균형이 점점 복잡해지기 때문에(단지 로컬 컴퓨터의 코어 뿐만 아니라) 컴퓨터의 네트워크를 통해 계산을 배포할 수도 있지만  
  이 책의 범위를 벗어남


## 용어 정리
- 오버헤드(overhead)
  - 어떤 처리를 하기 위해 들어가는 간접적인 처리 시간 + 메모리 등을 말함
  - 예를 들어 A라는 처리를 단순하게 실행했다면 걸리는 시간 10초, 안정성을 고려하고 부가적인 B라는 처리를 수행했을 때 15초가 걸린다면, 오버헤드는 총 5초
- 세션(session)
  - 클라이언트와 웹서버가 간 네트워크 연결이 지속 유지되고 있는 상태를 말함
  - 즉 사용자가 브라우저를 열어 서버에 접속한 뒤 접속을 종료할 때 까지의 시점을 말함
  - HTTP 프로토콜은 비접속형 프로토콜이므로, 매 접속시마다 새로운 네트워크 연결이 이루어지는데, 세션이 연결유지를 가능하게 해줍니다
  - 클라이언트가 웹서버에 Request를 보내면, 해당 서버의 엔진이 클라이언트에게 유일한 ID를 부여하는데 이 ID를 세션이라고 부릅니다