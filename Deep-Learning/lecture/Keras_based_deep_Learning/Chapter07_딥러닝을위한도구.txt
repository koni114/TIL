<Chapter07 딥러닝을 위한 도구>
여기서 알 수 있는 것.
- 케라스의 함수형 API 사용하여 그래프 구조를 띤 모델 만들기
- 하나의 층을 다른 입력에 같이 사용
- 케라스 모델을 파이썬 함수처럼 사용
- 케라스 callback
- 텐서보드 사용
- 배치 정규화, 잔차 연결, 하이퍼파라미터 최적화, 모델 앙상블을 통한 모범 사례.

## -------------------------------------------------------------- ##

7.1 Sequential 모델을 넘어서: 케라스의 함수형 API

Sequential 모델은 네트워크 입력과 출력이 하나라고 가정.
but, 이런 가정이 맞지 않는 경우도 많음.
ex) 개별 입력이 여러 개 필요하거나 출력이 여러 개 필요
     층을 차례대로 쌓지 않고 층 사이를 연결하여 그래프처럼 만드는 경우.

예시 1   
중고 의류 시장 가격을 예측하는 딥러닝 모델(입력 값이 여러 개)
input data -> 메타데이터(의류 브랜드, 연도 등), 사용자가 제공하는 text 설명, 제품 사진
각 데이터를 완전 연결 모듈, RNN 모듈, 컨브넷 모듈에 연결하여 가중 평균을 한다!
-> 이를 다중 입력 모델이라고 함.

예시 2
소설이나 짧은 글이 있을 때 자동으로 장르별 구분
글을 쓴 대략의 시대 예측
2개의 모델을 따로 훈련할 수 있지만 이 속성들은 통계적으로 독립적이지 않기 때문에 
동시에 장르와 시대를 함께 예측하도록 해야 더 좋은 모델을 만들 수 있음.
why ? 두 개의 예측 값은 상관관계 때문에 소설 시대를 알면, 장르의 공간에서 정확하고 풍부한
         표현을 학습하는데 도움을 줌.

최근 개발된 신경망 구조는 선형적이지 않은 network topology가 필요
ex) 인셉션 모듈을 사용하는 인셉션 개열의 네트워크
     -> 이 모듈에서 입력은 나란히 놓인 여러개의 합성곱 층을 거쳐 하나의 텐서로 출력이 합쳐짐

** network topology(위상 수학(?))
비순환 유향 그래프 같은 네트워크 구조
쉽게 말하면, 원자 구조를 생각하면됨.. 그런 구조들

최근에는 모델에 잔차 연결을 추가하는 경향도 있음
ResNet 계열의 네트워크들이 이런 방식을 사용하기 시작.
텐서를 상위 층의 출력 텐서에 더해서 아래층의 표현이 네트워크 위쪽으로 흘러갈 수 있도록 함(아래 층 값이 윗 층의 값에 영향을 미치는 구조)
-> 하위 층에서 학습된 정보가 데이터 처리 과정에서 손실되는 것을 방지

결과적으로 다중 입력 모델, 다중 출력 모델, 그래프 구조를 띤 모델이 필요하지만
케라스의 Sequential 클래스를 사용해서는 만들지 못함.

** 7.1.1 함수형 API 소개
함수처럼 층을 사용하여 텐서를 입력받고 출력함

input_tensor = Input(shape = (32, )) 
dense = layers.Dense(32, activation = 'relu')
output_tensor = dense(input_tensor)

Model 객체를 사용한 컴파일, 훈련, 평가 API는 Sequential 클래스와 같음

** 7.1.2 다중 입력 모델
함수형 API는 다중 입력 모델을 만드는 데 사용할 수 있음
서로 다른 입력 가지를 합치기 위해 여러 텐서를 연결할 수 있는 층을 사용
ex) 텐서를 더하거나, 이어 붙이는 방법.
keras.layers.add / keras.layers.concatenate 등

* 간단한 다중 입력 모델
ex) 질문 응답 모델
 
참고 텍스트 -> Embedding -> LSTM
				-> Concatenate -> Dense -> 응답
질문           -> Embedding -> LSTM 

* Embedding
범주형 자료를 연속형 벡터 형태로 변환시키는 것을 embedding이라고 함.

입력이 2개인 모델의 훈련 방법은 2가지
- 넘파이 배열의 리스트를 주입
ex) model.fit({[text, question], answers, epochs = 10, batch_size = 128)

- 입력 이름과 넘파이 배열로 이루어진 딕셔너리를 모델의 입력으로 주입할 수 있음
ex) model.fit({'text' : text, 'question': question}, answers, epochs=10, batch_size = 128)

7.1.3 다중 출력 모델
함수형 API를 사용하여 다중 출력 모델을 만들 수 있음
데이터에 있는 여러 속성을 동시에 예측하는 네트워크
ex) 소셜 미디어에서 익명 사용자의 포스트를 입력으로 받아 그 사람의 나이,
     성별, 소득 수준 등을 예측

소셜 미디어 포스트 -> 1D 컨브넷  -> Dense  -> 나이
			       -> Dense  -> 소득
  			       -> Dense  -> 성별
 

네트워크 출력마다 다른 손실함수를 지정해 주어야 함 
ex) 나이 예측은 스칼라 회귀 문제 이지만, 성별 예측은 이진 클래스 문제라 훈련 방식이 다름
경사 하강법은 하나의 스칼라 값을 최소화하기 때문에 모델을 훈련하려면 이 손실들을
하나의 값으로 합쳐야 함
-> 가장 간단한 방법은 모두 더하는 방법

케라스에서는 compile method에 리스트나 딕셔너리를 사용하여
출력마다 다른 손실을 지정할 수 있음.
-> 계산된 손실 값은 전체 손실 하나로 더해지고 훈련 과정을 통해 최소화 됨

model.compile(optimizer = 'rmsprop',
	        loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'])

손실 값이 많이 불균형하면 모델이 개별 손실이 가장 큰 작업에 치우쳐 
표현을 최적화함 -> 다른 작업들은 손해를 입을 수 있음

손실 값이 최종 손실에 기여하는 수준을 지정할 수 있음
ex) 손실 값의 스케일이 다를 때 유용
     나이 회귀 작업에 사용되는 MSE 손실은 3~5사이를 가지고 
     성별 분류 작업에 사용되는 크로스엔트로피 손실은 0.1 정도로 낮을 때,
 	
    각각 가중치를 통해 균형을 맞춰 줄 수 있음!

model.compile(optimizer = 'rmsprop',
	        loss = ['mse', 'categorical_crossentropy', 'binary_crossentropy'],
	        loss_weights = {'age' = 0.25, 'income' : 1., 'gender' : 10.} 
)

7.1.4 층으로 구성된 비순환 유향 그래프
함수형 API를 사용하면, 내부 토폴로지가 복잡한 네트워크도 만들 수 있음
케라스의 신경망은 층으로 구성된 어떤 비순환 유향 그래프도 만들 수 있음

* 비순환 유향 그래프(directed acyclic graph)
무수히 많은 간선과 노드가 있을 때, 특정 노드 v에서 v로 오는 방법이 없음.
but, v1 -> v4 로 가는 간선은 존재!
https://ko.wikipedia.org/wiki/%EC%9C%A0%ED%96%A5_%EB%B9%84%EC%88%9C%ED%99%98_%EA%B7%B8%EB%9E%98%ED%94%84 

** 인셉션 모듈(Inception Module)
인셉션은 합성곱 신경망에서 인기 있는 네트워크 구조.
네트워크 안의 네트워크 구조에서 영감을 받아 2013~2014년에 크리스티안 세게디와 그의 구글 동료들이 만듬.
가장 기본적인 인셉션 모듈 형태는 3~4개의 가지를 가짐
-> 입력 값에서 크게 굵은 가지가 4개가 있다는 의미. 

1X1 합성곱(점별 합성곱)으로 시작해서 3X3 합성곱이 뒤따르고 마지막에 전체 출력 특성이 합쳐짐
이런 구성은 네트워크가 따로따로 공간 특성과 채널 방향의 특성을 학습하도록 도움
(각 네트워크 채널마다 특유의 특성을 학습한다는 의미)
-> 한꺼번에 학습하는 것 보다 효과가 높음

더 복잡한 인샙션 모듈은 풀링 연산, 여러 가지 합성곱 사이즈, 공간 합성곱이 없는 가지로 구성될 수 있음.

** 인셉션V3  

		Conv2D, 1X1 strides = 2                                            --> 

		Conv2D 1X1   -->  Conv2D 3X3 strides = 2                   -->
입력 ->                                                                                                         Concatenate --> 출력
		avgPool2D 3X3 strides = 2 --> Conv2D 3X3                   --> 

		Conv2D 1X1 --> Conv2D 3X3 --> Conv2D 3X3 strides =2 -->


** 1X1 합성곱의 목적
추출된 패치(커널)가 하나의 타일(1X1)로 이루어졌을 때. 
이 합성곱 연산은 모든 타일의 벡터를 하나의 Dense 층에 통과시키는 것과 동일.
입력 텐서의 채널 정보를 혼합한 특성을 계산. 공간 방향으로는 정보를 섞지 않음(**핵심). 
1X1은 인셉션 모듈의 특징.
채널 방향의 특성 학습과 공간 방향의 특성 학습을 분리하는데 도움을 줌.
채널이 공간 방향으로 상관관계가 크고 채널 간에는 독립적이라고 가정하면 납득할 만한 전략

keras.applications.inception_v3.InceptionV3

** Xception
극단적인 inception을 말함.
채널 방향의 학습과 공간 방향의 학습을 극단적으로 분리한다는 
아이디어에 착안하여 인셉션 모듈을 깊이별 분리 합성곱(SeparableConv2D)으로 바꿈.
합성곱은 깊이별 합성곱(각 입력 채널에 따로따로 적용되는 공간 방향 합성곱**) 다음에 점별 합성곱(1X1)이 뒤따름.
인셉션 모델의 극한 형태로, 공간 특성과 채널 방향 특성을 완전히 분리.

엑셉션 모델은 3X3 커널을 사용한 layers.SeperableConv2D 합성곱 사용
입력 채널에 대해 따로따로 3X3 합성곱을 수행(채널을 늘리지 않음). 그다음 1X1 점곱을 수행(출력 채널을 늘림)
 SeparableConv2D의 출력 채널은 128개에서 2048개까지 네트워크가 깊어질수록 늘어남.
keras.applications.xception.Xception에 포함되어 있음.

인셉션V3 모델과 거의 동일한 파라미터 개수를 가지지만, 
실행 속도가 더 빠르고 ImageNet 이나  다른 대규모 데이터셋에서 정확도가 더 높음.
-> 모델 파라미터를 좀 더 효율적으로 사용하기 때문.
	익셉션 네트워크 파라미터 수 : 2380만 개
	엑셉션 네트워크 파라미터 수 : 2280개

** 잔차연결(residual connection)
2015년 이후 등장한 많은 네트워크 구조에 있는 그래프 형태의 네트워크 컴포넌트.
대규모 딥러닝 모델에서 흔히 나타나는 두 가지 문제인 그래디언트 소실과 표현 병목을 해결.
일반적으로 10개 이상 층을 가진 모델에 잔차 연결을 추가하면 도움이 됨.

잔차 연결은 하위 층의 출력을 상위 층의 입력으로 사용
순서대로 놓인 네트워크를 질러가는 연결이 만들어짐.
하위 층의 출력이 상위 층의 활성화 출력에 연결되는 것이 아니라, 더해 짐.
-> 두 출력의 크기가 동일해야 함.

크기가 다르면 선형 변환을 사용하여 하위 층의 활성화 출력을 목표 크기로 변환함.
출력 특성의 크기(채널 수)가 다를 수 있다. 이를 맞춰 주는 것. 
ex) 선형 변환 예 
     활성화 함수를 사용하지 않는 Dense 층.
     합성곱의 특성 맵이라면, 활성화 함수가 없는 1X1 합성곱.

** 딥러닝의 표현 병목
Sequential한 모델에서 해당 층은 바로 이전 층의 활성화 출력 정보만 사용.
어떤 층이 너무 작으면, 이 활성화 출력에 얼마나 많은 정보를 채울 수 있느냐에 모델 성능이 좌우.
이러한 문제를 residual connection은 어느정도 해결해 줌.

** 그래디언트 소실 문제.
핵심 알고리즘인 역전파는 출력 손실에서 얻은 피드백 신호를 하위 층에 전파.
but, 피드백 신호가 깊이 쌓인 층을 통과하면서 신호가 아주 작아지거나, 완전히 사라질 수 있음.
이를 그래디언트 소실(vanishing gradient) 문제라고 함.

이 문제는 심층 신경망과 긴 시퀀스를 처리하는 순환 신경망에서 모두 나타남.
주 네트워크 층에 나란히 단순한 선형 정보를 실어 나름.
-> 이는 그래디언트가 깊게 쌓인 층을 통과하여 전파하도록 도움.

잔차 연결에서 하위 층의 출력과 상위 층의 출력을 단순히 더했으므로 그래디언트가 그대로 잔차 연결을 따라
하위 층으로도 전달 됨.
잔차 연결을 따라 내려온 그래디언트는 주 네트워크 층을 거쳐 줄어든 그래디언트와 더해짐.

7.1.5 층 가중치 공유
함수형 API의 중요한 기능 -> 층 객체를 여러 번 재사용 가능!
즉, 층 객체를 두 번 호출하면 새로운 층 객체를 만들지 않고 각 호출에 동일한 가중치를 재사용.

공유가지를 가지는 모델을 만들 수 있음.
이런 가지는 같은 가중치를 공유하고, 같은 연산을 수행.
-> 같은 표현을 공유하고 이런 표현을 다른 입력에서 함께 학습.

ex) 두 문장 사이의 의미가 비슷한지 측정하는 모델을 가정.
이 모델은 2개의 입력(비교할 문장 2개)를 받아 0과 1사이의 점수를 출력.
0 -> 문장이 관련 없다!를 의미함.
1 -> 문장이 관련 있다!를 의미함.

이런 모델은 대화 시스템(dialog system)에서 
자연어 질의에 대한 중복 제거를 포함하여 많은 애플리케이션에서 유용하게 사용될 수 있음.

이 문제에서 두 입력 시퀀스가 바뀔 수 있음
why? 두 입력의 유사도는, 교환 법칙이 성립.

즉, 입력 문장을 처리하는 2개의 독립된 모델을 학습하는 것은 이치에 맞지 않음.
대신 하나의 LSTM 층으로 양쪽을 모두 처리하는 것이 좋음.

--> 샴 LSTM, 공유 LSTM 이라고 함.


7.1.6 층과 모델.
함수형 API는 모델을 층처럼 사용할 수 있음. 모델을 '커다란 층'으로 생각.
입력 텐서로 모델을 호출해서 출력 텐서를 얻을 수 있다는 뜻

y = model(x)
y1, y2 = model([x1, x2]) -> 모델에서 입력 텐서와 출력 텐서가 여러 개이면 텐서의 리스트 호출.

모델 객체를 호출 할 때 모델의 가중치가 재사용. -> 층 객체를 호출할 때와 동일.

간단한 실전 예
듀얼 카메라에서 입력을 받는 비전 모델.
두 카메라가 몇 센티미터 간격을 두고 나란히 있음. -> 이런 모델은 깊이를 탐지할 수 있음.
왼쪽 카메라와 오른쪽 카메라의 시각적 특징을 합치기 위해 2개의 독립된 모델을 사용할 필요가 없음.
두 입력에 저 수준 처리 과정이 공유 될 수 있음.

** 정리
차례대로 층을 쌓는 것 이상이 필요할 때는 Sequential API를 사용하지 않음.
함수형 API를 사용하여 다중 입력, 다중 출력, 복잡한 네트워크 토폴로지를 갖는 케라스 모델 만드는 방법.
다른 네트워크 가지에서 같은 층이나 모델 객체를 여러 번 호출하여 가중치를 재사용하는 방법.

## -------------------------------------------------------------- ##

** 7.2 캐라스 콜백과 텐서보드를 사용한 딥러닝 모델 검사와 모니터링
- 모델 내부에서 일어나는 일을 조사하고 제어하는 방법을 살펴보자.

7.2.1 콜백을 사용하여 모델의 훈련 과정 제어
첫 번째 실행에서 과대 적합이 일어나기까지 에포크로 계속 훈련함.
-> 이런 방식은 낭비가 많음.

더 좋은 처리 방법은 손실이 가장 적은 위치 +- 오차 범위 내에 멈추는 것!
이는 캐라스 콜백을 사용하여 구현할 수 있음

콜백 리스트를 만듬.
콜백은 모델의 fit() 메서드가 호출될 때 전달되는 객체
훈련하는 동안 모델은 여러 지점에서 콜백을 호출

콜백은 모델의 상태와 성능에 대한 모든 정보에 접근하고,
훈련 중지, 모델 저장, 가중치 적재 또는 모델 상태 변경 등을 처리할 수 있음.

- 모델 체크포인트 저장 : 훈련 중 모델의 가중치 저장 가능
- 조기 종료(early stopping) : 검증 손실이 커지지 않을 때 훈련 중지
- 훈련 중 하이퍼파라미터 값을 동적으로 조정 : 옵티마이저의 학습률 같은 경우
- 훈련과 검증 지표를 로그에 기록하거나 모델이 학습한 표현이 업데이트 될 때마다 시각화.

callback 함수 사용 방법
1. callbacks_list(명은 아무렇게나)에 내가 사용하고자 하는 callback function 구현.
2. 해당 callbacks_list를 model_fit할 때 callbacks parameter에 전달.
3. 이 때 callback 함수가 제대로 구현되려면 validation_data도 같이 할당 해주어야 함. ** 

keras.callback module은 많은 콜백 내장 함수를 포함하고 있음.

** ModelCheckPoint 와 EarlyStopping Callback

- EarlyStopping callback
정해진 에포크 동안 모니터링 지표가 향상되지 않을 때 훈련 중지 할 수 있음.

- ModelCheckPoint
EarlyStopping callback과 같이 사용하는데, 훈련하는 동안 모델을 계속 저장해 줌.

- ReduceLROnPlateau callback
검증 손실이 줄어들지 않을 때 학습률을 작게 조정할 수 있음.

** 자신만의 콜백 만들기
내장 콜백은 좀 더 응용해서 사용하고 싶을 때 적용 가능.
keras.callbacks.Callback 클래스 상속받아 사용

훈련하는 동안 호출될 여러 지점을 나타내기 위해 다음의 메소드를 구현함.

on_epoch_begin - 각 에포크가 시작할 때 호출
on_epoch_end   - 각 에포크가 끝날 때 호출

on_batch_begin  - 각 배치 처리가 시작되기 전에 호출
on_batch_end    -  각 배치 처리가 끝난 후에 호출

on_train_begin    - 훈련이 시작될 때 호출
on_train_end      - 훈련이 끝날 때 호출

-> 즉, 배치, 에포크, 훈련 단위에서 호출 방식을 setting 할 수 있음
-> 훈련 > 에포크 > 배치

이 메서드들은 모두 logs 매개변수와 함께 호출.
이 매개변수에는 배치, 에포크에 대한 훈련과 검증 측정값이 담겨 있는 딕셔너리가 전달됨

콜백은 다음 속성을 참조할 수 있음
self.model : 콜백을 호출하는 모델 객체
self.validation_data : 메서드에 전달된 검증 데이터

** 7.2.2 텐서보드 소개: 텐서플로의 시각화 프레임워크
좋은 연구, 좋은 모델을 만드려면 내부적으로 일어나는 일을 잘 알아야함
실험 결과는 텐서보드에서 확인 할 수 있음.

텐서보드란?
브라우저 기반 시각화 도구
텐서플로 백엔드로 케라스를 설정한 경우에만 케라스 모델을 사용할 수 있음
훈련 모델의 내부에서 일어나는 모든 것을 시각적으로 모니터링할 수 있도록 돕는 것.
텐서플로 백엔드로 케라스를 설정한 경우에만 케라스 모델에서 사용 가능.

발전 루프

텐서보드 기능
훈련하는 동안 측정 지표 시각적 모니터링
모델 구조를 시각화
활성화 출력과 그래디언트의 히스토그램을 그림
3D로 임베딩 표현

텐서보드를 시작하기 전에 로그 파일이 저장될 디렉터리를 만들어야 함.
TensorBoard 콜백 객체와 함께 훈련 진행 하면됨.
callbacks = [
    keras.callbacks.TensorBoard(
    log_dir         = 'my_log_dir',          # 로그 파일이 기록될 위치
    histogram_freq  = 1,                    # 1 에포크마다 활성화 출력의 히스토그램을 기록
    embeddings_freq = 1,                  # 1 에포크마다 임베딩 데이터 기록.
    embeddings_data = train[:100]       # 책에는 없지만 추가 해야함.  
    )
]

텐서플로를 설치했다면, 텐서보드 유틸리티는 자동으로 설치 됨.
anaconda console에서 tensorboard --logdir=my_log_dir 명령어 수행.
브라우저에서 http://localhost:6006 주소에 접속하면 모델의 훈련 결과를 확인할 수 있음.

Embedding 탭에서 입력 어휘 사전에 있는 단어 2000개의 
임베딩 위치와 공간상 관계를 조사할 수 있음

텐서보드는 우리가 선택한 차원 축소 알고리즘을 사용(주성분 분석 또는 t-SNE) 2D 또는 3D로 축소!

Graphs 탭은 케라스 모델을 구성하는 저수준 텐서플로 연산의 그래프를 시각화함

텐서플로 연산의 그래프 외에 케라스의 keras.utils.plot_model 유틸리티는 모델의 층 그래프를
깔끔하게 그려 주는 기능을 제공.
-> 이를 사용하려면 파이썬의 pydot, pydot-ng, graphvz 라이브러리 필요.

## -------------------------------------------------------------- ##

** 7.3 모델의 성능을 최대로 끌어올리기
잘 작동하고 머신러닝 경진 대회에서 우승하는 모델을 만들기 위해 꼭 알아야 할 기법들 소개

** 고급 구조 패턴(디자인 패턴)
꼭 알아야 할 디자인 패턴 3가지
- 잔차 연결(Residual Connection)
- 정규화
- 깊이별 분리 합성곱

** 배치 정규화(batch Normalization)
정규화는 머신 러닝 모델에 주입되는 샘플들을 균일하게 만드는 광범위한 방법.
모델이 학습하고 새로운 데이터에 잘 일반화 하도록 도움.
* 이전 예제에서는 데이터 주입 전에 정규화를 적용한 후 모델을 학습시켰지만, 
  실제로 네트워크 변환마다 정규화를 고려해야 함.
  why? 정규화된 데이터에 대한 결과 또한 정규성을 띤다고 보기는 어려움. 

배치 정규화는 아이오페, 세게디가 제안한 층의 한 종류.
(Keras는 BatchNormalization 클래스로 제공)

훈련하는 동안 평균과 분산이 바뀌더라도 이를 적응하여 데이터를 정규화함.
훈련과정에 사용된 배치 데이터의 평균과 분산에 대한 지수 이동 평균(exponential moving average)를 내부에 유지.

배치 정규화에서 입력 배치의 평균과 분산은 지수 이동 평균으로 계산하여, 전체 데이터 셋의 평균과 표준편차를 대신함.
이 값은 테스트 데이터에 배치 정규화가 적용될 때 사용.

지수 이동 평균(exponential moving average)
 V = V * momentum + v_new x (1 - momentum)
케라스의 BatchNormalization 클래스의 momentum 기본값은 0.99

배치 정규화의 주요 효과는 잔차 연결과 흡사하게 그래디언트의 전파를 도와줌.
why? 입력에 비하여 활성화 함수의 출력이 매우 작거나 커지면, 변화율이 급격히 작아져 역전파되는 그래디언트도 매우 줄어들게 됨.
결국 더 깊은 신경망 층을 구성할 수 있음
역으로 말하면, 깊은 신경망 층들은 배치 정규화 층을 많이 사용!

ResNet50, 인셉션V3, 엑셉션 등의 고급 컨브넷 구조는 BatchNormalization 층을 많이 사용.

배치 정규화 층은 주로 합성곱이나 완전 연결 층 다음에 사용
배치 정규화 층 추가 방법.
conv_model.add(layers.BatchNormalization())

BatchNormalization 클래스는 정규화할 특성 축을 지정하는 axis 매개변수가 있음
기본값은 마지막 축(마지막 값..)을 나타내는 -1임
data_format 을 "channel_last" 로 하여 Dense, Conv1D, RNN, Conv2D 층을 사용할 때는 맞는 값.
channel_first로 사용하는 경우에는 1로 해야 함.

** 배치 재정규화
배치 정규화의 최근 발전.
추가적인 비용을 들이지 않고 배치 정규화보다 더 효율적임.

** 깊이별 분리 합성곱(depthwise separable convolution)
Conv2D를 대체하면서 가볍고, 빠르고, 모델의 성능을 더 높일 수 있는 층.
SeparableConv2D.

계산 방법
1. 입력 채널별로 따로따로 공간 방향의 합성곱을 수행
2. 점별 합성곱(1X1 합성곱)을 통해 출력 채널을 합침
어떤효과? 공간 특성의 학습과 채널 특성의 학습을 분리하는 효과를 냄. 
공간적으로는 상관관계가 크고, 채널 별로는 독립적이라고 한다면 타당함.

제한된 데이터로 작은 모델을 처음부터 훈련시킬 때 더욱 중요.

** 7.3.2 하이퍼파라미터 최적화
가능한 결정 공간을 자동적, 조직적, 규칙적 방법으로 탐색해야함.
가능성 있는 구조를 탐색해서 실제 가장 높은 성능을 내는 것을 찾아야 함.

하이퍼파라미터 최적화 과정.
- 하이퍼파라미터를 자동 선택
- 선택된 하이퍼파라미터로 모델 제작
- 훈련 데이터에 학습하고 검증 데이터에서 최종 성능을 측정
- 다음으로 시도할 하이퍼파라미터를 선택
- 과정 반복

주어진 하이퍼파라미터에서 얻은 검증 성능을 사용하여, 다음 번에 시도할 하이퍼파라미터를 
선택하는 것이 핵심
다양한 알고리즘 사용 가능 - ex) 베이지안 최적화, 유전 알고리즘, random search 등..

but, 모델의 가중치보다 계산하기가 어려움.
why? 
1. feedback 신호를 계산하는 것은 비용이 많이 듬
-> 모델을 처음부터 다시 훈련해야 하기 때문..
2. 하이퍼파라미터 공간은 일반적으로 분리되어 있는 결정들로 채워짐(연속적이지 않다는 것인가..?)
   연속적이지 않고 미분 불가능. 따라서 경사 하강법을 사용 못하고, 
   훨씬 비효율적인 그래디언트-프리 최적화 기법을 사용함.

** 하이퍼파라미터를 최적화 할 때, 검증 데이터에 과대 적합되지 않게 조심해야함.
why? 하이퍼파라미터를 최적화 한다는 것은 검증 데이터의 y값을 더 좋게 만든다는 의미이므로!

** 모델 앙상블(model ensemble)
여러 개 다른 모델을 합쳐서 더 좋은 예측을 만듬.
예측을 만들기 위해 조금씩 다른 면을 바라봄. 데이터의 모든 면이 아니고 부분 특징.
케라스는 Auto-Keras 를 open함.

분류기 예측을 합치는 가장 쉬운 방법은 추론할 때 나온 예측을 평균 내는 것.
** 분류기가 어느 정도 비슷하게 좋을 때 잘 작동함.
분류기중 하나가 월등히 나쁘면 최종 예측은 앙상블에 있는 가장 좋은 분류기만큼 좋지 않을 수 있음.

좋은 앙상블 가중치를 찾기 위해 랜덤 서치나 넬더-미드 방법 같은 간단한 최적화 알고리즘을 사용할 수 있음.
검증 데이터에서 찾은 최적의 가중치로 단순하게 가중 평균하는 방법이 좋은 기본 값.

앙상블의 핵심은 분류기의 다양성.
잘 동작하는 한가지 방법은 트리 기반 모델(randomForest, gradientBoostingTree)이나 심층 신경망을 앙상블 하는 것.













