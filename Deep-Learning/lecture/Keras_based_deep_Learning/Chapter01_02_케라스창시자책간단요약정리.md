## Chapter01, Chapter02
### 딥러닝 간단 역사
* 기존의 딥러닝의 그래디언트를 전파하는 것이 가장 큰 문제였음
* 2009 ~ 2010년경 몇 가지 간단하지만 중요한 딥러닝 알고리즘이 개선되면서 그래디언트를 잘 전파하게 되고, 상황이 바뀜
* 크게 3가지
  * 신경망 층에 더 잘맞는 활성화 함수(activation function)
  * 층별 사전 훈련(pretraining)을 불필요하게 만든 가중치 초기화 방법
  * RMSProp, Adam과 같은 더 좋은 최적화 방법

* 2014 ~ 2016년 사이에 그래디언트를 더욱 더 잘 전파할 수 있는
batch normalization, residual connection, depthwise, separable, convolution 같은 고급 기술이 개발 됨

### 딥러닝은 지속될까?
* 딥러닝이라는 자체가 20년 뒤에도 사용될지는 미지수지만, 비슷한 형태로 계속 사용될 것이라고 예측
why? 크게 3가지라고 설명
  * 단순함 : 딥러닝은 특성 공학이 필요하지 않아 불안정한 많은 엔지니어링 과정을 엔드 - 투 - 엔드로 훈련시킬 수 있는 모델로 바꾸어 줌
  * 확장성 : GPU 또는 TPU에서 쉽게 병렬화 할 수 있음
  * 다용도와 재사용성 : 딥러닝 모델은 처음부터 다시 모델링 하지 않고, 추가되는 데이터로도 훈련할 수 있음
  딥러닝 모델은 다른 용도로 사용될 수 있어, 재사용이 가능

### Chapter02 시작하기 전에, 신경망의 수학적 요소
* 손글씨 숫자 분류를 학습하는 예제를 통해 신경망의 수학적 구성 요소를 이해해 보자
* 문제 정의 -> 흑백 손글씨 숫자 이미지(28 X 28 픽셀)를 10개의 범주(0~9까지)로 분류하는 것
* 간단 용어 정리
  * 클래스 : 분류 문제의 범주
  * 샘플 : 데이터 포인트
  * 레이블 : 특정 샘플의 클래스
* 작업 순서
  * 훈련데이터를 네트워크에 주입
  * 네트워크는 이미지와 레이블을 연관시킬 수 있도록 학습
  *  마지막으로 test_image에 대한 예측을 네트워크에 요청
  * 요청된 값과 실제 값과 비교

* 신경망의 핵심 요소는 층(layer)
* 층은 주어진 문제에 더 의미 있는 *표현(representation)을 입력된 데이터로 부터 추출 함

* 신경망이 훈련 준비를 마치기 위해서  컴파일 단계에 포함될 세 가지 필요
  * 손실 함수(loss function) : 훈련 데이터에서 신경망의 성능을 측정하는 방법으로 네트워크가 옳은 방향으로 학습될 수 있도록 도와 줌
  * 옵티마이저(optimizer) : 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘
  * 평가지표(metrics) : 훈련과 테스트 과정을 모니터링 할 지표

* 신경망은 입력 데이터의 스케일에 민감하기 때문에, 스케일링 과정 필요
* 이미지의 경우 보통 픽셀의 최댓값인 255로 나누어 사용
* 레이블을 범주형으로 인코딩 해야 함(one-hot encoding 등을 해야 된다는 말인 듯)
  * ex ) [0, 2] -> [[1,0,0],[0,0,1]]

* 훈련시 2개의 정보 출력 : 손실(loss value) 과 정확도(accuracy)

### 텐서(tensor) 란?
* 머신러닝의 기본 구성 요소
* 데이터를 위한 컨테이너(container). 거의 항상 수치형 데이터를 다루므로, 숫자를 위한 컨테이너
* 임의의 차원 개수를 가지는 행렬의 일반화된 모습(여기서 차원(dimension)을 종종 축(axis) 라고 부름)

#### 스칼라(scalar, 0D 텐서)
* 하나의 숫자만 가지고 있는 텐서
* 스칼라 축의 개수는 0, 보통 텐서의 축 개수를 rank라고 함

#### 벡터(vector, 1D 텐서)
* 숫자의 배열을 벡터라고 함. 또는 1D 텐서
* 딱 하나의 축을 가짐
* 벡터가 5개의 원소를 가지고 있는 경우, 5차원 벡터라고 함(중요!)
* <b/>주의! 5D 텐서와 5D 벡터를 혼동하지 말자</b>
  * 여기서의 차원수(Demensionality)는 특정 축을 따라 놓인 원소의 개수이거나, 축의 개수이므로 혼동 X !

#### 행렬(Matrix, 2D 텐서)
* 벡터의 행렬(Matrix) 또는 2D 텐서라고 함
* 2개의 축 존재(보통 행과 열이라고 부름)

#### 3D 텐서와 고차원 텐서
* 행렬들을 하나의 새로운 배열로 합치면 숫자가 채워짐
* 직육면체 3D 형태로 해석할 수 있는 3D 텐서가 만들어짐

* 딥러닝에서는 보통 0D ~ 4D까지 다룸
* 동영상 같은 경우, 5D 까지 가기도 함

### 텐서(tensor)의 핵심 속성
* 축의 개수(rank) : 축은 앞에서 이미 배웠음, numpy library 의 ndim 속성에 저장.
* 크기(shape) : 각 축에서의 차원을 의미. 즉 tuple의 길이!
* 데이터의 타입 : 텐서에 포함된 데이터 타입(numpy에서는 dtype에 저장)
  * 일반적으로 텐서의 타입은 float32, uint8(8bit 정수형 의미), float64 등,, 간혹가다 char 타입 있음
* 텐서는 사전에 할당되어 연속된 메모리에 저장되어야 하므로
넘파이 배열은 가변 길이의 문자열을 지원하지 않음

### 넘파이로 텐서 조작하기
* 정중앙에 위치한 14 x 14 픽셀 조각을 이미지에서 잘라 내려면?
train_images[:, 7:-7, 7:-7]

### 배치(batch) 데이터
* 일반적으로 딥러닝에서 사용되는 모든 데이터 텐서의 첫 번째 축(Axis)은 샘플 축(sample axis) 임
--> 여기서 샘플 축은 데이터의 개수(ex) 이미지의 개수) 등을 의미하는 데이터의 수를 말함
* 일반적으로 딥러닝 모델은 한꺼번에 전체 데이터 셋을 처리하지 않고, 데이터를 작은 배치로 나눔
* 보통 첫 번째 축을 배치 축 또는 배치 차원 이라고 부름

### 텐서의 실제 사례
* 우리가 사용할 데이터는 대부분 다음 중 하나에 속함!
* 벡터 데이터 : (samples, features) 크기의 2D 텐서
* 시계열 데이터 또는 시퀀스(sequence) 데이터 : (samples, timesteps, features) 크기의 3D 텐서
* 이미지 데이터 : (samples, height, width, channels) 또는 (samples, channels, height, width) 크기의 4D 텐서
* 동영상 데이터 : (samples, frames, height, width, channels) 또는 (samples, frames, channels, height, width) 크기의 5D 텐서

### 벡터 데이터
* 대부분의 경우에 해당!
* 이런 데이터셋에서는 하나의 데이터 포인트가 벡터로 인코딩될 수 있으므로 배치 데이터는 2D 텐서로 인코딩 됨
* 첫번째 축은 샘플 축(sample axis), 두번째 축은 특성 축(feature axis) 임
* 2개의 예
  * 사람의 나이, 우편 번호, 소득으로 구성된 10만명 인구 통계 데이터
  -> (100000, 3) 크기의 텐서에 저장 됨
  * 공통 단어 2만 개로 만든 사전에서 각 단어가 등장한 횟수로 표현된 텍스트 문서 500개
  -> (500, 20000) 크기의 텐서에 저장 됨

### 시계열 데이터 또는 시퀀스 데이터
* 시간이 중요할 때는 시간 축을 추가한 3D 텐서에 저장
* 2개의 예
  * 주식 가격 데이터 셋 : 1분마다(주식 가격, 지난 1분 동안 최고 가격, 최소 가격) 3개의 스칼라를 저장
  즉 1분마다 데이터는 3차원 벡터로 인코딩 되고, 하루 동안 거래는(390, 3) 크기의 2D 텐서로 저장
 250일치의 데이터는 (250, 390, 3) 크기의 3D 텐서로 저장
 * 트윗 데이터 셋 : 각 트윗은 128개의 알파벳으로 구성된 280개 문자 시퀀스 --> (280, 128)
 100만개의 트윗으로 구성된 데이터셋은 (1,000,000, 280, 128) 크기의 텐서에 저장
 --> 한 행이 알파벳 하나가 됨

### 이미지 데이터
* 전형적으로 높이(세로), 너비(가로), 컬러 채널의 3차원으로 이루어짐
* 128개의 컬러 이미지 배치 --> (128, 256, 256, 3)
* 이미지 텐서의 크기를 지정하는 방식은 2가지
  * 텐서 플로우에서 사용 : (samples, height, width, color_depth)
  * 씨아노에서 사용 :  (samples, color_depth, height, width)
* 케라스에서는 2가지 모두 지원

### 비디오 데이터
* 비디오 <- 여러 프레임 <- 여러 이미지로 구성
* 결과적으로, (samples, frames, height, width, color_depth) 5D에 저장 가능
* 1개의 예
  * 60초 짜리144 x 256 유투브 비디오 클립을 초당 4 프레임으로 샘플링 하는 경우, --> 240 프레임,
  이런 비디오가 4개 있다고 가정할 때,
  * (4, 240, 144, 256, 3)

## 소주제 - 신경망의 톱니바퀴 : 텐서 연산
* 심층 심경망이 학습한 모든 변환 --> 수치 데이터 텐서에 적용하는 몇 종류의 텐서 연산으로 나타낼 수 있음
* Dense 층을 쌓아 만듬. 케라스 층은 다음과 같이 나타낼 수 있음
  * Keras.layers.Dense(512, activation = 'relu')
* 이 층은 2D텐서를 입력으로 받고 입력 텐서의 또 다른 새로운 표현인 또 다른 2D 텐서를 반환하는 함수처럼 사용 가능

* output = relu(dot(W, input) + b) 각 연산은 3가지 연산이 들어가 있음
  *  W와 input 텐서와의 곱
  * 해당 결과 텐서와 b와의 덧셈 연산
  * 이를 relu 연산으로 수행

### 원소별 연산(element-wise operation)
* relu 연산, 더하기 연산에 해당
* 텐서에 있는 각 원소에 독립적으로 적용
* 원소별 연산은 넘파이에서 빠른 속도로 해결 가능

### 브로드캐스팅(broadcasting)
* 작은 텐서가 큰 텐서의 크기에 맞추어 지는 것
* 브로드캐스팅은 두 단계로 이루어 짐
  * 큰 텐서의 ndim에 맞도록 작은 텐서에 축이 추가
  * 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복

### 텐서 점곱(tensor product)
* 입력 텐서의 원소들을 결합 --> (연산으로 텐서가 작아진다는 말)
* 넘파이, 케라스, 씨아노, 텐서플로에서 원소별 곱셈은 * 연산자 사용
* 텐서 점곱은 dot 연산자가 조금씩 다르지만, 일반적으로 dot 연산자 사용
* ndim이 1보다 큰 경우, 교환 법칙은 성립하지 않음

#### 텐서 크기 변환(tensor reshaping)
* 특정 크기에 맞게 열과 행을 재배치 한다는 뜻
* 자주 사용하는 크기 변환은 전치(transposition) -> tensor[i, ] -> tensor[, i]

#### 텐서 연산의 기하학적 해석
* 텐서 연산이 조작하는 텐서의 내용은 기하학적 공간에 있는 좌표 포인트로 해석 가능
* 일반적으로 아핀 변환(affine transformation), 회전, 스케일링 등 기본적인 기하학적 연산은 텐서 연산으로 표현될 수 있음
* 예를 들어 theta 회전은 2x2 행렬 R = [u, v]를 점곱하여 구할 수 있음
* -> u = [cos(theta), sin(theta)], v = [-sin(theta), cos(theta)]

#### 딥러닝의 기하학적 해석
* 꾸겨진 종이공을 펼치는 것을 딥러닝이 하는 일
* 심하게 꼬여 있는 매니폴드에 대한 깔끔한 표현을 찾는 일
* 딥러닝 --> 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식이므로,
 꾸겨진 종이공을 넓게 색종이로 만드는 일에 유능
 * *매니폴드 : 저차원의 유클리디안 거리로 볼 수 있는 고차원 공간(ex) 2차원 색종이로 볼 수 있는 꾸겨진 3차원 종이 공)

## 소주제 - 신경망의 엔진 : 그레디언트 기반 최적화
* 다음의 식을 다시 상기해보자
* output = relu(dot(W, input) + b)
* 텐서 W와 b는 층의 속성처럼 볼 수 있음
  * 가중치(weight) 또는 훈련되는 파라미터(parameter)라고 부름
  * 또는 커널(kernel), 편향(bias)라고 부름
* 초기에는 <b>가중치 행렬이 무작위 난수</b>로 만들어져 있음(random initialization phase)
* 피드백 신호에 의해 점진적으로 조정 됨. 이 훈련이 머신 러닝 학습의 핵심

* 훈련 반복 루프(loop)
  * 훈련 샘플 x와 이에 상응하는 y 배치 추출
  * x를 이용하여 네트워크 실행 y_pred를 구함
  * y_pred 와 y의 차이를 측정하여 배치에 대한 손실 계산
  * 배치에 대한 손실이 감소되도록 가중치 업데이트!

### 핵심은 네트워크의 모든 가중치 업데이트
* How? 가장 쉬운 방법은 모든 가중치를 고정하고 관심 있는 하나만 바꿔가면서 손실 함수의 value check
-> 이 방법은 모든 가중치 행렬의 원소마다 두 번의 정방향 패스를 계산해야 하므로 비효율적
* 신경망에 사용된 모든 연산이 미분 가능하다는 장점을 이용하여, 네트워크 가중치에 대한 손실의 gradient를 계산하는 것이 훨씬 좋은 방법!

### 텐서 연산의 변화율 : 그래디언트(gradient)
* 그래디언트는 텐서 변환의 변화율 --> 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것
* W를 사용하여 타깃의 예측 y_pred를 계산하고 손실, 즉 타깃 예측 y_pred 와 타깃 y 사이에 오차를 계산
  * loss_value = loss(y_pred, y)
* x,y가 고정일 때, 이 함수는 y를 W와 매핑시키는 함수로 볼 수 있음
* W의 현재 값을 W0라고 할 때, 포인트 W0에서 f의 변화율 -> W와 같은 크기의 텐서인 gradient(f)(W0)
* <b>이 텐서의 각 원소 gradient(f)(W0)[i, j] 는 W0[i, j]를 변경했을 때 loss_value가 바뀌는 방향과 크기를 나타냄 -</b>
* gradient(f)(W0) -> W에서 함수 f(W) = loss_value 의 그래디언트
W0에서 f(W)의 기울기로 나타내는 텐서로 해석
* 즉 그래디언트의 반대 방향으로 움직이면, f(W)의 값을 줄일 수 있음
ex) W1 = W0 - step * gradient(f)(W0)

### 확률적 경사 하강법(gradient descent algorithm)
* 가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 방법을 의미
* gradient(f)(W) = 0을 풀면 해결. 이는 N개(네트워크 가중치 개수)의 변수로 이루어진 다항식
* 신경망에서는 N이 수백개, 수천개 이므로, 이를 해석적으로 푸는 것은 불가

* 따라서 앞선 가중치 업데이트 아이디어를 적용,
--> 랜덤한 배치 데이터에서 현재 손실 값을 토대로 하여 조금씩 파라미터를 수정 하는 것
그래디언트의 반대방향으로 가중치 업데이트를 하면 손실이 매번 조금씩 감소 할 것임

* 적용 순서
  * 훈련 샘플 배치 x와 이에 상응하는 타깃 y를 추출
  * x로 네트워크를 실행하고 예측 y_pred를 구함
  * y_pred와 y의 손실 값 계산
  * 손실 함수의 gradient를(역방향 Pass..) 계산
  * gradient의 반대방향으로 step 값을 곱해 조금씩 값을 감소
 * 이 방법이 미니 배치 확률적 경사 하강법(mini-batch stochastic gradient descent = 미니 배치 GSD)에 해당
 왜 확률적이냐 ? --> 배치 X를 무작위로 추출하기 때문!
 보통 확률적이다 라고 하는 것은 무작위의 과학적 표현에 해당

* step 값을 적절하게 setting 하는 것이 중요. 너무 크면 손실 함수 곡선에서 완전히 다른 위치로 바뀜
너무 작으면, local minimum에 걸림

* 배치 SGD의 변종 중 하나는 배치 선택에 있음
즉, 반복마다 하나의 샘플과 하나의 타깃을 뽑는 것일수도 있고(이게 원래 진정한 SGD)
극단적으로 모든 X 데이터를 배치 데이터로 setting해 반복을 수행 할 수도 있음(배치 SGD)

* 실제로는 굉장히 고차원 공간에서 경사 하강법을 수행하게 됨
(보통 설명할 때는 1차원 공간에서 SGD를 많이 설명하지만 헷갈리지 말자)
2D 손실 함수의 표면을 따라 진행하는 경사 하강법을 시각화해 표현할 수 있지만,
딥러닝은 10000차원 이상을 표현해야 하는데, 이를 공간에 표현하는 것은 불가능
--> 즉, 저차원 표현으로 얻은 직관이 실전과 맞지 않는다는 것을 명심하자. 이는 딥러닝 분야에서 여러 이슈를 일으키는 근원

* 어떤 이슈?
  * 신경망 알고리즘이 local minimum에 쉽게 접근 할 것이라고 생각하지만, 고차원 공간에는 안장점(saddle point)로 나타나고, 지역 최소값은 매우 드뭄. 수치적으로 보면 

* 이런 가중치를 계산 할 때 여러 변종들, 즉 여러 방법들을 적용해서 계산하는데,
이런 변종들을 모두 최적화 방법(optimization method) 또는 옵티마이저(optimizer)라고 함

### 모멘텀(momentum) 알고리즘
* 과거 그래디언트가 가지고 있는 어떤 방향을 현재 그래디언트에 보정하려는 방식
* 이런 모멘텀은 SGD에 있는 2가지 문제점, 수렴 속도와 지역 최소값을 해결
  * 지역 최소값에 도달했을 때 왼쪽, 오른쪽으로 이동하려고 해도 손실함수는 증가하므로, 결과적으로 갇히게 됨
  --> 문제 발생. 이를  해결할 수 있는 방법으로 모멘텀 알고리즘 적용
* 손실 함수 위에 축구공을 굴리는 것을 생각하자. 국소 최소값ㅊ에 도착했을 때, 축구공이 힘을 받아 국소 최소값을 벗어날 힘을 가지고 있다고 하면 전역 최소값에 도달할 수 있음
* 즉, 모멘텀은 현재 기울기(가속도) 값 뿐만 아니라 속도까지 고려하여 각 단계에서 공을 굴리는 개념

### 변화율 연결 : 역전파 알고리즘
* 3개의 텐서 연산 a, b, c와 가중치 행렬 W1, W2, W3로 구성된 네트워크 f를 예를 들어보자
* f(W1, W2, W3) = a(W1, b(W2, c(W3))) --> 즉, 단계적으로 변환을 3번 한다는 얘기
* 이러한 함수 내에 함수가 들어가 있는 함수를 미분 할 때, chain Rule을 적용하여 풀어 낼 수 있음
  * f( g(x) ) ' = f'( g(x) ) * g'(x)
* 연쇄 법칙을 신경망의 그래디언트 계산에 적용하여 역전파 알고리즘이 탄생
* 역전파는 최종 손실 값에서부터 시작.
* 손실 값에 각 파라미터가 기여한 정도를 계산하기 위해 연쇄 법칙을 이용하여 최상위 층에서 최하위 층까지 거꾸로 진행
* 향후 몇 년 동안은 텐서플로처럼 기호 미분(symbolic differentiation)이 가능한 최신 프레임워크를 사용하여 신경망 구현할 것
  * 변화율이 알려진 연산(+, -, x, %...등)들로 연결되어 있으면 네트워크 파라미터와 그래디언트 값을 매핑하는 그래디언트 함수를 계산할 수 있다는 의미
* 결과적으로 정확한 역전파 공식을 구현하지 않아도 되고, 어떻게 돌아가는지 이해만 하면 됨
