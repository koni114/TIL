< Chapter 03  신경망 시작하기 >

세 가지 기본 예제에 적용해보기
- 영화 리뷰를 긍정 또는 부정으로 분류하기(이진 분류)
- 신문 기사를 토픽으로 분류하기(다중 분류)
- 부동산 데이터를 바탕으로 주택 가격 예측하기(회귀)

** 3.1 신경망의 구조.
## ---------------------------- ##
* 층(layer) : 신경망의 구성 단위
층은 하나 이상의 텐서를 입력으로 받아 텐서를 출력하는 데이터 모듈
어떤 종류의 층은 상태가 없지만, '가중치'라는 상태가 있을 수 있음
가중치에는 학습한 지식이 담겨있음

층마다 적절한 텐서 포멧과 데이터 처리 방식이 다름
- (samples, features) 의 2D 텐서가 저장된 간단한 벡터 데이터 
   -> fully connected payer나 dense layer라고 불리우는 densely connected layer에 처리되는 경우가 많음
* fully connected layer : 이전 레이어의 모든 노드가 다음 레이어의 모든 노드에 연결된 레이어
  ( 동의어 : dense layer, densely connected layer)

- (samples, timesteps, feature) 크기의 3D 텐서로 저장된 시퀀스 데이터
 -> LSTM 같은 순환 층(recurrent layer)에 의해 처리
** 순환층(recurrent layer)
-> 현재 층에 대한 결과를 기억하고, 이 기억을 새로운 입력 데이터가 들어왔을 때
    활용하여 다시 개선하는 형태의 층을 말함

- 4D 텐서로 저장되는 이미지 데이터
  -> 2D 합성곱 층(2D convolution layer)에 의해 처리
** 2D 합성곱 층(2D convolution layer)
입력을 받아 dot product를 진행을 하고, 선택에 따라 non linear 연산 수행

케라스는 호환 가능한 층(입력과 출력 형태가 맞는 층) 파이프라인을 구성함으로써
딥러닝 모델을 만듭니다.
* 층 호환성(layer compatibility)
-> 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환하는 사실을 말함

케라스는 모델에 추가된 층을 자동으로 상위 층에 맞추어줌
-> input_shape를 지정해 주지 않아도 알아서 찾아서 수행됨

** 모델 : 층의 네트워크
딥러닝 모델은 층으로 만든 Directed Acyclic Graph.
** 비순환 유향 그래프(Directed Acyclic Graph) : 그래프의 edge에 방향이 있고,
   cycle 하지 못하는 그래프를 말함

가장 일반적인 예
하나의 입력을 하나의 출력으로 매핑하는 층을 순서대로 쌓는 것.

우리는 다양한 네트워크 구조를 보게 될 것이지만, 자주 등장하는 것들은 다음과 같음
- 가지(branch)가 2개인 네트워크
- 출력이 여러 개인 네트워크
- 인샙션(Inception) 블록

네트워크 구조는 가설 공간(hypothesis space)를 정의함
즉 네트워크 구조를 선택함으로써 가능성 있는 공간(가설 공간)을 입력데이터에서
출력데이터로 매핑하는 일련의 특정 텐서 연산으로 제한하게 됨

딱 맞는 네트워크 구조를 찾아내는 것은 과학보다는 예술에 가까움
-> 연습을 통해 길러야 함!

** 손실 함수와 옵티마이저: 학습 과정을 조정하는 열쇠

* 손실 함수
여러 개의 출력을 내는 신경망은 여러 개의 손실 함수를 가질 수 있음(출력 당 하나씩)
-> 하지만 경사 하강법 과정은 하나의 스칼라 손실 값을 기준으로 함
    (쉽게 말하면, 정확도 판단 기준은 1개로 한다~라는 의미 

문제에 맞는 옳바른 목적함수를 선택하는 것은 굉장히 중요(**)
-> 네트워크가 손실을 최소화하기 위해 편법을 사용할 수 있음

분류, 회귀, 시퀀스 예측 같은 일반적인 문제에는 올바른 손실 함수를 선택하는 
간단한 지침이 존재! 
- 2개 클래스 분류 문제 : 이진 크로스엔트로피
- 3개 이상의 클래스 분류 문제 : 범주형 크로스엔트로피
- 회귀 문제 : 평균 제곱 오차
- 시퀀스 학습 문제 : CTQ(connection Temporal Classification)

## ---------------------------- ##

3.2 케라스 소개
모든 종류의 딥러닝 모델을 간편하게 만들고 훈련시킬 수 있는 파이썬을 위한 딥러닝 프레임워크 
케라스의 특징
- 동일한 코드로 CPU와 GPU에서 실행
- 사용하기 쉬운 API를 가지고 있어 딥러닝 모델의 프로토타입을 빠르게 만들 수 있음
- CNN, RNN 지원, 이를 자유롭게 조합해서 사용 가능
- 어떤 네트워크 구조도 만들 수 있음(GAN 등 어떤 딥러닝 모델에도 적합)

케라스는 딥러닝 모델을 만들기 위한 고수준의 구성 요소를 제공하는 모델 수준의 라이브러리
텐서 조작이나 미분 같은 저수준의 조작은 다루지 않음
-> 대신 케라스의 backend engine 에서 제공하는 최적화되고 특화된 텐서 라이브러리 사용

케라스는 텐서플로를 사용하기 때문에 CPU와 GPU에서 모두 작동
CPU에서 실행될 때, Eigen이라고 불리는 저수준 텐서 연산 라이브러리 이용
GPU에서는 NVIDIA CUDA 심층 신경망 라이브러리라고 불리는 고도로 최적화된 딥러닝 연산 라이브러리 사용

** 케라스를 사용한 개발: 빠르게 둘러보기
모델을 정의하는 방법은 2가지
1. Sequential 클래스(층을 순서대로 쌓아 올린 네트워크)
2. 함수형 API(완전히 임의의 구조를 만들 수 있는 비순환 유향 그래프)
-> 해당 예제는 notebook 확인!

모델 구조가 정의된 후에는 Sequential 클래스로 제작했는지, 함수형 API로 제작했는지
상관 없음.

컴파일 단계에서 학습과정이 설정 됨
모델이 사용할 옵티마이저, 손실 함수, 측정 지표를 지정

마지막으로, 입력 데이터의 넘파이 배열을 모델의 fit() 메서드에 전달함으로써
학습 과정이 이루어짐

## ---------------------------- ##

3.3 딥러닝 컴퓨터 셋팅
최신 NVIDIA GPU에서 딥러닝 코드 실행 권장
최신 GPU를 사용하면 2배, 5배, 10배 정도 속도가 빨라짐

컴퓨터에 GPU 카드를 설치하고 싶지 않다면, 
대안으로 AWS EC2 GPU 인스턴스나 구글 클라우드 플랫폼 고려

## ---------------------------- ##

3.4 영화 리뷰 분류: 이진 분류 예제
리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정으로 분류

** IMDB 데이터셋
인터넷 영화 DB로부터 가져온 양극단의 리뷰 5만 개로 이루어진 IMDB 데이터셋 사용
데이터셋은 훈련 데이터 25000개와 테스트 데이터 25000 개
각각 50%는 부정, 50%는 긍정 리뷰로 구성
케라스에 포함되어 있음
각 리뷰가 숫자 시퀀스로 변환되어 있음
각 숫자는 사전에 있는 고유한 단어를 나타냄

* 데이터 준비 
신경망에 리스트를 주입할 수는 없음. 리스트를 텐서로 바꿔야 함
2가지 방법
1. 같은 길이가 되도록 리스트에 패딩(Padding)을 추가.
   (samples, sequence_length) 크기의 정수 텐서로 변환(2D tensor)
   이 정수 텐서를 다룰 수 있는 층을 신경망의 첫 번째 층으로 사용(Embedding 층을 말함)
  -> 가장 긴 리뷰는 2,494개의 단어로 이루어져 있으므로, 훈련 데이터를 변환한 텐서의 크기는 (25000, 2494)
2. 리스트를 one-hot encoding하여 0과 1의 벡터로 변환(one-hot encoding = categorical encoding)
   ex) [3,5] 인 영화 리뷰 데이터 1개를 3,5의 위치 값은 1 나머지는 0인 10000차원 벡터로 변환!
   부동 소수 벡터 데이터를 다룰 수 있는 Dense 층을 신경망의 첫 번째 층으로 사용
   -> 텐서의 크기 (25000, 10000)

* 신경망 모델 만들기
입력 데이터가 벡터고, 레이블은 스칼라(0또는1) 임(앞으로 볼 수 있는 문제 중, 가장 간단)
-> 이런 문제에 잘 작동하는 네트워크의 종류는 
    relu 활성화 함수를 사용한 완전 연결 층을 그냥 쌓은 것
이 때의 완전 연결 층은, Desce(16, activation = 'relu'))
16의 의미는 은닉 유닛(hidden unit)의 개수.! -> 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 됨
16개의 은닉 유닛이 있다고 하는 것은 가중치 행렬 W의 크기가 (input_dimension, 16)이라는 뜻
-> 즉, 입력 데이터와 W를 dot product를 수행하면 16차원으로 표현된 공간으로 투영
은닉 유닛을 늘리면 더욱 복잡한 표현을 학습할 수 있지만, 계산 비용이 커지고 원하지 않은 패턴을 학습할 수도 있음 
-> overfitting 발생 가능 커짐

Dense 층을 쌓을 때 두 가지 중요한 구조상의 결정이 필요
- 얼마나 많은 층을 사용할 것인가?
- 각 층에 얼마나 많은 은닉 유닛을 둘 것인가?

* 은닉 유닛(hidden unit)
- 어렵지 않다. 신경망 모형을 생각했을 때 한 층에서의 하나의 동그라미라고 생각하자.

* 활성화 함수(activation function)
relu와 같은 활성화 함수는 비선형성(non-linearity)라고 부름.
이러한 활성화 함수가 없다면 선형적인 연산으로 구성됨 
-> 그렇다면 층 마다 입력에 대한 선형 변환만을 학습할 것이고, 이 층의 가설 공간은 입력 데이터를 16차원으로 바꾸는 가능한 선형 변환의 집합
    -> 이 가설 공간은 제약 조건이 많으며, 선형 층을 깊게 쌓아도 하나의 선형 연산이기 때문에 층을 여러 개로 구성하는 장점이 없음
    ->  층을 추가해도 가설 공간이 확장 되지 않음 
따라서, 가설 공간을 풍부하게 만들어 층을 깊게 만드는 장점을 살리기 위해서 비선형성 또는 활성화 함수를 추가해야 함.
가장 많이 쓰는 함수가 relu, 음수 처리 방식이 달라지는 prelu, elu 등도 있음

마지막으로 손실 함수와 옵티마이저를 사용해야 하는데, 이진 분류에서는 '크로스엔트로피'를 주로 사용!

* 크로스엔트로피(crossentropy)
확률 분포간의 차이를 측정 하는 계산법(?). 이진 분류에 사용하는 방법

* 훈련 검증
처음 본 데이터에 대해 모델의 정확도를 측정하기 위해서는 원본 데이터에서 10000개의 검증 세트를 만들어야 함

## ---------------------------- ##

3.5 뉴스 기사 분류 : 다중 분류 문제
로이터(Reuter) 뉴스를 46개의 상호 배타적인 토픽으로 분류하는 신경망을 만들어보자
로이터 데이터셋
각 토픽은 훈련 세트에 최소한 10개의 샘플을 가지고 있음
어떤 토픽은 다른 것에 비해 데이터가 많음

** 모델 구성
이 토픽 분류 문제는 앞선 영화 분류 문제와 비슷
but!! 새로운 제약 사항이 추가 -> 출력 클래스의 개수가 2개에서 46개로 증가(차원 크기가 훨씬 커짐)
어떻게 해야할까?
영화 분류에서 사용했던 Dense 층을 쌓으면, 각 층은 이전 층의 출력에서 제공한 정보만 사용 가능
-> 한 층이 분류 문제에 필요한 일부 정보를 누락하면 그다음 층에서 이를 복원할 방법이 없음!
    즉, 각 층이 모두 bottleneck이 될 수 있음
영화 분류에서는 16개의 은닉 unit을 setting 했지만, 46개의 클래스를 구분하기에는 너무 제약이 큼.
따라서 규모를 더 키워야 할 필요성존재
16 -> 64로 증가!


** 레이블을 벡터로 변경
--> 영화 리뷰와는 다르게 레이블을 벡터로 변경해주어야 함
      y 레이블이 46개 존재하므로, 이를 0,1 one-hot vector로 변경해 주어야 함

** 레이블과 손실을 다루는 다른 방법
레이블을 인코딩하는 다른 방법은 정수 텐서로 변환하는 것. -> ex) [ 3, 10,  1, ...,  3,  3, 24]
일반적으로 사용하는 손실 함수 : categorical_crossentropy는 레이블이 범주형 인코딩(one-hot encoding)되어 있을 것이라고 기대!
정수 레이블을 사용할 때는 sparse_categorical_crossentropy를 사용해야 함
-> 이 손실 함수는 인터페이스만 다를 뿐이고, 수학적으로는 categorical_crossentropy와 동일

** 충분히 큰 중간층을 두어야 하는 이유( 중요!! )
마지막 출력이 46차원이기 때문에 중간층이 히든 유닛이 46개보다 많이 적어서는 안됨
why ? 이런 손실의 대부분은 많은 정보(46개의 분할 초평면을 복원하기에 충분한 정보)를 중간층의 저차원 표현 공간으로 압축하려고 했기 때문 **

## ---------------------------- ##

3.6 주택 가격 예측: 회귀 문제
보스턴 주택 가격 데이터셋
1970년 중반 보스턴 외곽 지역의 범죄율, 지방세율 등의 데이터가 주어졌을 때
주택 가격의 중간 값을 예측.
데이터 포인트 : 506개
train : 404, test : 102
** 입력 데이터의 feature는 scale이 다름

상이한 스케일을 신경망에 주입하게 되면, 문제가 됨
why? 특성의 스케일이 다르면 전역 최소 점을 찾아가는 경사 하강법의 경로가 스케일 큰 특성에 영향을 많이 받음.
-> 표준화 수행(중앙 : 0, 표준편차 1)

** 표준화 수행시 정말 조심해야 할 점(중요 ** )
테스트 데이터를 표준화 할 때, 절대로 테스트 데이터 자체를 표준화 시켜서는 안됨.
테스트 데이터를 훈련 데이터의 mean, std를 이용해서 표준화 시켜주어야 함!

표준화 뿐만 아니라, 모든 데이터 변환이 마찬가지.

** 모델 구성
샘플 개수가 적기 때문에 64개의 유닛을 가진 2개의 은닉 층으로 작은 네트워크 구성.
-> overfitting을 방지하기 위한 방법
회귀는 마지막 층에 활성화 함수가 없음
활성화 함수의 역할은 값을 제한해주는 역할이 있는데, 회귀에서는 값 그대로를 예측해야 하므로, 활성화 함수가 필요 없음
회귀 모형이므로, 손실 함수를 mse를 사용!
지표는 mae(mean absolute error : 예측 값과 실제 값의 절대값 차)를 사용
ex) MAE가 0.5면, 500달러 정도 차이남을 의미!(더 직관적으로 이해하기 위해서 MAE를 보자)

** K-fold 검증을 통한 훈련 검증
- 데이터의 수가 적고, 검증 세트를 새로 편성 할 때마다 각각의 값 들이 차이가 많이나므로,
   K-Fold 검증 수행
이 책에서는 일반적으로 K가 4또는 5라고 얘기함
최종 점수는 해당 검증 점수의 평균

** 지수이동평균(exponential moving average)
일반적으로 시계열 데이터를 부드럽게 만들기 위해서 사용
현재 계산한 [ 이동평균 = 이전 계산된 이동평균 * factor + (1 - factor) ] 식을 이용해서 계산!







