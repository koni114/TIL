< Chapter_05 : 컴퓨터 비전을 위한 딥러닝 >

## -------------------------------------------------------------- ##

** 5.1 합성곱 신경망 소개
컨브넷(convnet) 이라고 불리우는 합성곱 신경망(convolutional neural network)은 컴퓨터 비전에 사용 됨
컨브넷은 (image_height, image_width, image_channels) 크기의 입력 텐서를 가짐
-> MNIST에서는 이미지 포멧인 (28, 28, 1) 크기의 입력을 처리하도록 convnet을 설정해야 함

** 간단한 컨브넷 만들기
model = models.Sequential()
model.add(layers.Conv2D(32 (--> filter) , (3, 3),  activation = 'relu', input_shape = (28, 28, 1))) # input으로 (28, 28, 1)을 setting 한 것 확인
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3),  activation = 'relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3),  activation = 'relu'))

Conv2D, MaxPooling2D 층의 출력 -> (height, width, channels) 크기의 3D 텐서,
너비와 높이는 네트워크가 깊어질수록(상위 층으로 갈 수록) 작아지는 경향이 있음
-> 출력층에 가까워질수록 너비와 높이가 작아진다는 말..

항상 조심! -> model.add를 통해 층을 추가할 때, 순서가 꼬이지 않게 조심하자..

** 합성곱 연산
dense layer(완전 연결 층)과 합성곱 층 사이의 근본적인 차이
-> Dense 층은 입력 특성 공간에 있는 전역 패턴을 학습
    합성곱 층은 지역 패턴을 학습 -> 이미지일 경우 작은 2D 윈도우로 입력해서 패턴을 찾음

이 핵심 특징은 컨브넷에 두 가지 흥미로운 성질을 제공

1. 학습된 패턴은 평행 이동 불변성(translation invariant)을 가짐
ex) 이미지 오른쪽 아래 모서리에서 어떤 패턴을 학습했다면, 왼쪽 위 모서리에서도 똑같은 패턴을 인식할 수 있음
dense layer에서는 새로운 위치에서 나타나는 같은 패턴은 다른 패턴으로 인식함
잘 생각해보면, 우리가 볼 때 평행 이동으로 인해 다르게 인식되지 않음.
-> 적은 샘플로 일반화 능력을 가질 수 있게 됨

2. 패턴의 공간적 계층 구조를 학습할 수 있음
첫 번째 합성곱 층이 edge같은 작은 지역 패턴을 학습하고,
두 번째 합성곱 층이 첫 번째 합성곱 층의 특성으로 구성된 더 큰 패턴을 학습
-> 이런 방식을 이용하여 convnet은 복잡하고 추상적인 시각적 개념을 효과적으로 학습할 수 있음
* 우리가 보는 세상은 근본적으로 공간적 계층 구조를 가짐!

합성곱 연산은 특성 맵(feature Map) 이라고 불리우는 3D 텐서에 적용
이 텐서는 2개의 공간 축(높이와 너비)과 깊이 축(채널 축이라고도 함)으로 구성
 -> RGB(컬러) 이미지는 channel이 3, MNIST와 같은 흑백은 1을 가짐

합성곱 연산은 이러한 feature Map에서 작은 패치들을 추출하고
모든 패치에 같은 변환을 적용하여 출력 특성 맵(output feature map)을 만듬

** 패치(Patch) 
합성곱 연산에서 input/output에 해당하는 이미지 보드라고 생각하면 될 듯.
https://dambaekday.tistory.com/3

output feature Map도 높이와 너비를 가진 3D 텐서.
깊이(channel)는 층의 매개변수로 결정되기 때문에 그때 그때 다름
-> 깊이 축은 더 이상 RGB 입력처럼 특정 컬러를 의미하지 않음
    일종의 filter 수를 의미함

** 합성곱 층에서의 filter는 합성곱 층에서 사용하는 모델 파라미터를 의미 
   Conv2D의 첫 번째 매개변수가 출력 특성 맵의 차원을 결정                    model.add(layers.Conv2D(64 ( < -- ), (3, 3),  activation = 'relu'))
   필터는 입력 데이터의 어떤 특성을 인코딩한 결과.
   ex) 하나의 필터가 '입력에 얼굴이 있는지'를 인코딩 할 수 있음            -> 위의 예제 같은 경우, 64개의 필터가 있음. 

MNIST 예제에서는 첫 번째 합성곱 층이 (28, 28, 1) 크기의 맵을 입력으로 받아
(26, 26, 32) 크기의 특성 맵을 출력함
-> 32개의 출력 채널은 각각 (26, 26) 크기의 배열 값을 가짐
    이 값은 입력에 대한 필터의 응답 맵(response map) 임

* 출력 맵 = 응답 맵

즉 특성 맵이란, 깊이 축에 있는 각 차원은 하나의 특성이고,
2D 텐서(output[:, :, n])는 입력에 대한 이 필터 응답을 나타내는 2D 공간상의 맵

합성곱은 핵심적인 2개의 파라미터로 구성

1. 입력으로부터 뽑아낼 패치의 크기(-> 해석이 이상함.. 출력으로 뽑아낼 때 사용되는 패치의 크기 인듯..!) 
 -> 3X3, 5X5 크기를 사용. 
 -> 아 패치는 특성에 따라서 다른 특징들을 가지고 있음. (https://untitledtblog.tistory.com/150 참조) 

2. 특성 맵의 출력 깊이: 
합성곱으로 계산할 필터의 수. MNIST 예제 같은 경우 32로 시작해서 64로 끝남.

Conv2D 층에서 2개의 파라미터가 전달됨.
ex) Conv2D(특성 맵의 출력 깊이, (패치의 높이, 패치의 폭))

3D  입력 특성 위를 3X3 또는 5X5 크기의 윈도우가 sliding 하면서
모든 위치에서 3D 특성 패치를 추출하는 방식으로 합성곱이 작용
이때 3D 특성 패치는 출력 깊이만큼 생성 됨.   

** 이해가 어려울 수 있다. 확실히 이해하자.
ex) 입력 특성 맵(5, 5, 2) 이라고 해보자. 너비 5, 높이 5, 입력 깊이가 2이다.
     이를 3X3 패치로 출력 깊이가 3인 출력 특성 맵을 만든다고 하면, -> ( Conv2D(3, (3, 3))
     이 때 입력 패치는 (3, 3, 2) 인 패치가 3개(깊이 3) 적용 된 것임.
     결과 적으로 출력 특성 맵은 (3, 3, 3) 이 나옴.
    -> 그림으로 이해하자.. p 177


출력 높이와 너비는 입력 높이와 너비와 다를 수 있음. 이유는 크게 2가지
1. 경계 문제, input feature Map에 패딩을 추가하여 대응할 수 있음( -> 출력의 크기를 다르게 하고 패딩을 추가하여 특성치를 변경..?)
2. 스트라이드(stride)의 사용 여부에 따라 다름.


** 패딩(Padding)
입력과 동일한 높이와 너비를 가진 출력 특성 맵을 얻기 위하여 입력 특성 맵의 가장자리에 적절한 개수의 행과 열을 추가 하는 것.
이 때 추가되는 행과 열은 0이므로 제로 패딩(zero padding)이라고도 부름.
3X3 윈도우라면, 좌,우,위,아래 1행 추가.
5X5 윈도우라면, 좌,우,위,아래 2행 추가


Conv2D층에서 padding 매개변수로 설정 가능.
padding = 'valid'  -> 패딩을 사용하지 않음
padding = 'same' -> 입력과 동일한 높이와 너비를 가진 출력을 만들기 위해 패딩함
default -> 'vaild'

** 합성곱 스트라이드(stride)
출력 크기에 영향을 미치는 다른 요소인 스트라이드
쉽게 말하면, 3X3, 5X5의 윈도우가 몇칸을 건너띄고 이동할 것인가?를 결정하는 파라미터.
기본 default가 1인데, 1이면 연속적으로 윈도우가 이동하는 것(우리가 생각하는 기본 이동)
스트라이드가 2라고 하면, 5X5 입력 특성 맵에서 3X3 입력 패치를 이용하면, 2X2 가 나온다!
-> 그림으로 이해하자.. p 179

스트라이드 합성곱은 실전에서 드물게 사용
하지만 특정 모델에서는 유용하게 사용될 수 있음.(https:/goo.gl/qvNTyu 블로그 참조)
특성을 다운 샘플링 하기 위해서는 최대 풀링(max pooling)을 사용하는 경우가 많음

** 최대 풀링 연산
특성 맵을 다운샘플링하는 것이 최대 풀링의 역할임
입력 특성 맵에서 윈도우에 맞는 패치를 추출하고 각 채널별로 최대값을 추출!
합성곱과 개념적으로는 비슷하지만, 추출한 패치에 학습된 선형 변환을 적용하는 대신 하드코딩된 최댓값 추출 연산을 사용. 

합성곱과의 최대 차이점
최대 풀링 : 2X2 윈도우와 스트라이드 2를 사용하여 특성 맵을 절반 크기로 다운 샘플링
합성곱    : 3X3 윈도우와 스트라이드 1을 사용함.

** 왜 이런식으로 특성 맵을 다운샘플링 하는 것일까? 
반대로 풀링 연산을 제외하고, 합성 곱으로만 이루어진 Convnet이 있다고 해보자. 
해당 ConvNet은 2가지 문제를 가지고 있음

1. 특성의 공간적 계층 구조를 학습하는데 도움이 되지 않음.
3번째 층의 3X3 윈도우가 가지고 있는 정보량은 최초 입력 특성 맵의 7X7 윈도우밖에 안된다.
(-> 7X7 크기를 합성곱 하면 5X5로 바뀌고, 5X5를 다시 합성곱 하면, 3X3으로 바뀜)
* 마지막 합성곱 층의 특성이 전체 입력에 대한 정보를 가지고 있어야 함

2. 최종 특성 맵은 22 x 22 x 64 = 30976개의 가중치를 가짐
   -> 합성곱만 계속 하게되면 줄어드는 비율이 -2만큼씩이므로, 너무 적은 비율로 줄어든다.
   이 ConvNet을 펼친 후, Dense 층과 연결한다면, 15.8백만 개의 개중치 파라미터가 생김.
   작은 모델치고는 너무 많은 가중치이고 심각한 과대적합이 발생함.

결과적으로 다운샘플링 하는 이유는,
1. 특성 맵의 가중치 개수를 줄이기 위함
2. 연속적인 합성곱 층이 점점 커진 윈도우를 통해 바라보도록 만들어 필터의 공간적인 계층 구조를 구성.

입력 패치의 채널별 평균값을 계산하는 평균 폴링(average pooling)을 사용할 수도 있음.
but, 최대 폴링이 더 잘 작동하는 편임 !
      why? 특성이 특성 맵의 각 타일에서 어떤 패턴이나 개념의 존재 여부(있다, 없다) 여부를 인코딩 하는 경향이 있기 때문.
              따라서 특성의 평균 값보다 여러 특성 중 최대값을 사용하는 것이 더 유용함.

* 가장 보편적인 서브샘플링 방법?
스트라이드가 없는 합성곱으로 조밀한 특성 맵을 만들고, 작은 패치에 대해서 최대로 활성화된 특성을 고르는 것
스트라이드 합성곱으로 듬성듬성 윈도우를 슬라이딩하거나 입력 패치를 평균해서 특성 정보를 놓치거나 희석시키는 것보다 나음

## -------------------------------------------------------------- ##

5.2 소규모 데이터셋에서 밑바닥부터 컨브넷 훈련하기
매우 적은 데이터에서 이미지 분류 모델을 훈련하는 일은 흔한 경우.
여기서 말하는 '적은' 샘플이랑 수백 개 ~ 수만 개 사이!(...?)

실용적인 예제로, 4000개의 고양이와 강아지 사진으로 구성된 데이터셋에서
강아지와 고양이 이미지를 분류해보자!

훈련 데이터 : 2000개
검증 데이터 : 1000개
테스트 데이터 : 1000개

2000개의 훈련 샘플을 그대로 사용하여(규제 하지 않고,) 기준이 되는 기본 성능을 만들자.
(아마도 71%의 분류 정확도를 가짐. 이슈는 과대적합)
이런 모델을 데이터 증식(data augmentation)을 통해 정확도를 82% 향상시켜보자

작은 데이터셋에 딥러닝을 적용하기 위한 핵심 기술 2가지
1. 사전 훈련된(pretrained Model) 네트워크로 특성을 추출(90%의 정확도)
2. 사전 훈련된 네트워크를 세밀하게 튜닝(92%의 정확도)

-> 이런 3가지 전략은 작은 데이터셋에서 분류 문제를 수행할 때 반드시 알아야 하고, 도구 상자에 포함되어 있어야 함

** 작은 데이터셋 문제에서 딥러닝의 타당성
보통 딥러닝은 많은 데이터가 필요하다고 이야기함.
-> 부분적으로 맞음. 딥러닝의 근본적인 특징은 훈련 데이터에서 특성 공학 없이 흥미로운 특성을 찾을 수 있는 것임
    하지만, 많은 데이터는 상대적임.
    - 훈련하려는 네트워크의 크기와 깊이에 상대적
      ConvNet은 지역적이고 평행 이동으로 변하지 않는 특성을 학습하기 때문에 
      지각에 관한 문제에서 매우 효율적으로 데이터를 사용
      -> 매우 작은 이미지 데이터셋에서 어떤 종류의 특성 공학을 사용하지 않고 
      컨브넷을 처음부터 훈련해도 납득할 만한 결과를 만들 수 있음

** 데이터 전처리
JPEG 파일로 되어 있는 경우, 부동 소수 타입의 텐서로 적절하게 전처리 되어 있어야 함.

1. 사진 파일을 읽는다
2. JPEG 콘텐츠를 RGB 픽셀 값으로 디코딩 함
3. 부동 소수 타입 텐서로 변환
4. 0 ~ 255 사이의 스케일을 0 ~ 1 사이로 조정(신경망은 작은 값을 선호)

-> Keras는 자동으로 처리하는 유틸이 존재
keras.preprocessing.image에 이미지 처리를 위한 헬퍼 도구들도 있음
ImageDataGenerator 클래스는 디스크에 있는 이미지 파일을 전처리된 
배치 텐서로 자동 바꿔주는 파이썬 제너레이터를 만들어줌.

** python generator
제너레이터는 반복자처럼 작동하는 객체.
for, in 연산자에 사용할 수 있음
-> preprocessinGenerator를 만들면, for, in 연산자와 함께 사용할 수 있다는 얘기

제너레이터는 배치를 무한정 만들어내므로, for, in 연산자 내 generator를 사용할 때
반드시 break 문을 사용해야 함

**  fit_제너레이터(generator)
fit_generator 메서드는 fit 메서드와 동일하되 데이터 제너레이터 사용
첫 번째 입력변수로 파이썬 제너레이터 입력
데이터가 끝없이 생성되기 때문에 케라스 모델에 하나의 에포크를 정의하기 위해 
제너레이터에서 얼마나 많은 샘플을 뽑을 것인지 알려주어야 함
-> step_per_epoch 매개변수에서 이를 설정. step_per_epoch = 100 이면, 1 epoch에 2000개의 데이터 학습!

validation_data 매개변수를 전달할 수 있음
파라미터 값은 데이터 제너레이터도 가능하지만, 넘파이 배열의 튜플도 가능.
validation_data로 제너레이터를 전달하면 검증 데이터의 배치를 끝없이 반환.
step_per_epoch 처럼, validation_steps 매개변수에 지정해야 함


** 데이터 증식(data agumentation) 사용하기
데이터 증식은 기존의 있는 훈련 샘플로부터 더 많은 훈련 데이터를 생성하는 방식
-> 그럴듯한 이미지를 여러가지 랜덤한 변환을 적용하여 샘플을 늘림
-> 데이터 증식을 통해 증식된 데이터와 함께 모델을 훈련시키고, 모델의 일반화를 만들어냄

generator를 통해 데이터 증식을 진행 할 때 전체 data volume이 증가하는 것이 아니라,
전체 volume에서 증식 부분이 추가되는 개념. 

케라스에서는 ImageDataGenerator가 읽은 이미지에 여러 종류의 랜덤 변환을 적용하도록 설정할 수 있음.

rotation_range = 랜덤하게 사진을 회전시킬 각도 범위 (0 ~ 180 사이)
                       회전 각도는 -rotation_range ~ +rotation_range

width_shift_range, height_shift_range = 사진을 수평과 수직으로 랜덤하게 평행 이동시킬 범위
(전체 너비와 높이에 대한 비율).
				   1보다 큰 실수이거나 정수일 때는 픽셀 값으로 간주
 				   실수가 입력되는 경우, [-width_shift_range, +width_shift range) 가 됨 (폐구간, 개구간 확인)
				   
shear_range = 랜덤하게 전단 변환을 적용할 각도 범위. keras 에서의 전단 변환은 y좌표가 증가함에 따라 조금씩 가로 방향으로 이동하는 가로 방향 전단 변환.
	        -> y축 음수 위치에서 오른쪽 방향으로 이미지를 밀어 평생사변형으로 바뀐다고 생각하면 직관적임. 

zoom_range = 랜덤하게 사진을 확대할 범위
                    -> 실수가 입력되면 1-zoom_range ~ 1+zoom_range 사이로 확대 또는 축소가 됨.

horizontal_flip = 랜덤하게 이미지를 수평으로 뒤집음.  수평 대칭을 가정할 수 있을 때 사용(ex) 풍경/인물 사진)
                      -> 좌우 반전

fill_mode = 회전이나 가로/세로 이동으로 인해 새롭게 생성해야 할 픽셀을 채울 전략
                -> nearest : 인접한 픽셀을 사용
                -> constant : cval 매개변수의 값을 사용
                -> reflect, wrap도 있음


* 전단 변환(shear transformation) -> 직사각형 형태의 이미지를 한쪽 방향으로 밀어서
 평행 사변형 형태의 이미지로 변환시키는 변환 방법.

적은 수의 원본 이미지에서 결국 증식된 것이기 때문에 여전히 입력 데이터들 사이에 상호 연관성이 큼.
-> 새로운 정보를 만들어 낼 수 없고, 단지 기존 정보의 재조합만 가능.
-> 과대적합을 제거하기에 충분치 않을 수 있으므로, DropOut을 추가 할 수 있음

** 검증 데이터는 절대 증식되어서는 안됨!

## -------------------------------------------------------------- ##

5.3 사전 훈련된 컨브넷(pretrained network) 사용하기

작은 이미지 데이터셋에 딥러닝을 적용하는 일반적으로 매우 효과적인 방법은 
사전 훈련된 네트워크를 이용하는 것.
pretrained network는 대규모 이미지 분류 문제를 위해 대량의 데이터셋에서 미리 훈련되어 저장된 네트워크
새로운 문제가 원래 작업과 완전히 다른 클래스에 대한 것이여도 이런 특성은 많은 컴퓨터 비전 문제에 유용 
ex) 동물이나 생활 용품으로 이루어진 원본 데이터를 가지고 만든 
모델을 가구 아이템을 식별하는 것 같은 용도로 사용 가능!

학습된 특성을 다른 문제에 적용할 수 있는 유연성은 딥러닝의 핵심 장점

VGG16 구조를 통해 실습!

** VGG16 model
캐런 시몬연과 엔드류 지서먼이 2014년에 개발
간단하고, ImageNet 데이터셋에 널리 사용되는 컨브넷 구조. 
사실 오래되었고, 최고 성능에는 못미치고 다른 모델보다는 조금 무거움. 
이전 실습한 모델과 개념이 거의 유사하기 때문에 사용 
합성곱 층 16개, 완전 연결 층 3개로 이루어져 있음

keras.applications module에 들어가 있음

pretrained network를 사용하는 두 가지 방법 존재
- 특성 추출(feature extraction)
- 미세 조정(fine tuning)

** 특성 추출
사전에 학습된 네트워크의 표현을 사용하여 새로운 샘플에서 흥미로운 특성을 뽑아 내는 것.
이런 특성을 사용하여 새로운 분류기를 처음부터 훈련.

ConvNet은 이미지 분류를 위해 두 부분으로 구성

1. 합성곱 기반 층

특성 추출은 사전에 훈련된 네트워크의 합성곱 기반 층을 선택하여 새로운 데이터를 통과 시키고,
출력으로 새로운 분류기(제일 마지막 출력 층)를 훈련시킴
일반적으로 합성곱 층만 재사용 함! (완전 연결 분류기는 잘 재사용하지 않음)
합성곱 층에 의해 학습된 표현이 더 일반적이여서 재사용이 가능하기 때문

합성곱 층 vs 완전 연결 분류기
합성곱 층 : 사진에 대한 일반적인 콘셉트의 존재 여부를 기록한 맵
	  주어진 컴퓨터 비전 문제에 상관없이 유용하게 사용 가능

완전 연결 분류기 : 모델이 훈련된 클래스 집합에 특화되어 있음
		분류기는 전체 사진에 어떤 클래스가 존재할 확률에 관한 정보만 담고 있음
		** 완전 연결 층에서 찾은 표현은 더 이상 입력 이미지에 있는 객체의 위치 정보를 가지고 있지 않음
		-> 공간 개념을 제거

합성곱 층에서 추출 한 표현의 일반성 수준은 "모델의 층의 깊이"에 달려 있음(몇 번째 합성곱 층이냐 ? 에 달려있다는 의미)
모델의 하위 층 : 지역적이고 매우 일반적인 특성 맵을 추출(edge, 색깔, 질감 등 )
모델의 상위 층 : 추상적인 개념 추출(강아지 눈, 고양이 귀 등)

* 상위 층은 출력 층과 가까운 층을 말하며, 하위 층은 입력 층과 가까운 층을 말함

** VGG16 Model 만들기
3개의 매개변수 전달
1. weight        :  모델을 초기화할 가중치 체크포인트 지정(..?)

2. include_top  :  네트워크 최상위 완전 연결 분류기를 포함할지 안 할지를 지정
	          기본값은 ImageNet의 클래스 1000개에 대응되는 완전 연결 분류기를 포함
	          별도의(고양이와 강아지를 분류할 수 있는..) 완전 연결 층을 추가할 것이므로, 포함 X

3. input_shape :  네트워크에 주입할 이미지 텐서의 크기(선택 사항)
	          지정하지 않으면, 어떤 크기의 입력도 처리 가능
	          -> 만약 지정해 주려면 합성곱 층 위에 올라가야 하므로, (224, 224, 3)이 되어야 함 

최종 특성 맵의 크기는 (4, 4, 512)임.
이 특성 위에 완전 연결 층을 놓을 것인데, 이 때 2가지 방법이 가능 


- 데이터 증식을 사용하지 않는 빠른 특성 추출
새로운 데이터 셋에서 합성곱 기반 층을 실행하고 출력을 넘파이 배열로 디스크에 저장.
다음 데이터를 독립된 완전 연결 분류기에 입력으로 사용
합성곱 연산은 비용(= 시간)이 매우 오래 걸리는데, 이 방법은 합성곱 기반 층을 
한 번만 실행하면 되기 때문에 빠르고 비용이 적게 듬.

but, 데이터 증식을 사용할 수 없음.

- 두 번째 방법
준비한 모델(conv_base) 위에 Dense 층을 쌓아 확장.( --> 거의 대부분 이 방법 사용 하는 듯..) 
다음 입력 데이터에서 엔드-투-엔드로 전체 모델을 실행. ( 처음 입력 층 ~ 분류기 까지 전부)
모델에 노출된 모든 입력 이미지가 매번 합성곱 기반 층을 통과하기 때문에 데이터 증식을 사용할 수 있음
-> 비용이 많이 듬.
                                 
** 데이터 증식을 사용하지 않는 빠른 특성 추출 실습
소스 코드 확인! 
약 90% 검증 정확도에 도달
많은 비율로 드롭아웃을 사용했음에도 훈련을 시작하면서 거의 바로 과대 적합되고 있는 것을 확인 할 수 있음
데이터가 적은데도 데이터 증식을 할 수 없기 때문! 

** 데이터 증식을 사용한 특성 추출
VGG16 합성곱 기반 층은 14714688개의 매우 많은 파라미터를 가지고 있음
합성곱 기반 층 위에 추가한 분류기는 200만 개의 파라미터를 가짐

모델을 훈련하기 전에 합성곱 기반 층을 동결하는 것이 아주 중요
** 동결(freezing) : 훈련하는 동안 가중치가 업데이트 되지 않도록 막는다는 뜻.
-> 이렇게 하지 않으면 합성곱 기반 층에 사전 학습된 표현이 훈련하는 동안 수정 되버림!

특히 맨 위의 Dense 층(입력 층 쪽 Dense 층)은 랜덤하게 초기화되었기 때문에 
매우 큰 가중치 업데이트 값이 네트워크에 전파됨
-> 결과적으로 기존 가중치가 크게 훼손됨

keras에서는 trainable 속성을 False로 설정하여, 네트워크를 동결할 수 있음
model.trainable = False


** 5.3.2 미세 조정(fine-tuning)
모델을 재사용하는 데 널리 사용되는 또 하나의 기법 중 하나.
특성 추출에 사용했던 동결 모델의 상위 층 몇 개를 동결에서 해제하고 
모델에 새로 추가한 층과 함께 훈련하는 것
주어진 문제에 조금 더 밀접하게 재사용 모델의 표현을 조정하는 것이기 때문에 fine-tuning 이라고 함

랜덤하게 초기화된 상단 분류기를 훈련하기 위해서는 VGG16의 합성곱 기반 층을 동결해야 함.
분류기가 미리 훈련되지 않으면 훈련되는 동안 너무 큰 오차 신호가 네트워크에 전파됨.

네트워크 미세 조정하는 단계는 다음과 같음 
1. 사전 훈련된 기반 네트워크 위에 새로운 네트워크를 추가
2. 기반 네트워크를 동결
3. 새로 추가한 네트워크를 훈련
4. 기반 네트워크에서 일부 층의 동결을 해제
5. 동결을 해제한 층과 새로 추가한 층을 함께 훈련

이번 실습에서는 제일 마지막 합성곱 층 3개를 미세 조정
얼마나 많이 합성곱 층을 미세 조정할 것인지는 다음 사항을 고려!

- 하위 층은 재사용 가능한 특성들을 인코딩, 상위 층은 좀 더 특화된 특성을 인코딩. 
  즉 새로운 문제에 재활용하도록 수정이 필요한 것은 상위 층. 하위 층으로 갈수록 
  미세 조정에 대한 효과 감소
- 훈련해야 할 파라미터가 많을수록 과대적합의 위험이 커짐

미세 조정 후, 결과를 확인해보면, 손실 곡선은 오히려 더 악화되었지만 정확도는 1% 향상 됨
-> 그래프는 개별적인 손실 값의 평균을 그린 것.
    정확도에 영향을 미치는 것은 손실 값의 분포이지 평균이 아님.
    정확도는 모델이 예측한 클래스 확률이 어떤 임계 값을 넘었는지에 대한 결과이기 때문.

** 정리
- ConvNet은 컴퓨터 비전에서 가장 뛰어난 머신 러닝 모델. 작은 데이터셋에서도 좋은 성능을 냄
- 작은 데이터셋에서는 과대 적합이 문제인데, data augmentation은 과대 적합을 막을 수 있는 좋은 방법
- 특성 추출 방식으로 새로운 데이터셋에 기존 ConvNet을 쉽게 재사용 가능. 

## -------------------------------------------------------------- ##

5.4 ConvNet 학습 시각화
딥러닝 모델을 black-box라고 얘기하지만, ConvNet은 전혀 아니다!
-> ConvNet은 시각적인 개념을 학습한 것이기 때문에 시각화하기 아주 좋음.

** 시각화 기법 3가지

- 컨브넷 중간층의 출력(중간층에 있는 활성화)을 시각화
  -> 연속된 컨브넷 층이 입력을 어떻게 변형시키는지 이해.
  -> 개별적인 컨브넷 필터의 의미를 파악하는데 도움

- 컨브넷 필터를 시각화
  -> 컨브넷의 필터가 찾으려는 시각적인 패턴과 개념이 무엇인지 상세하게 이해하는 데 도움.

- 클래스 활성화에 대한 히트맵(heatMap)을 이미지에 시각화
  -> 이미지의 어느 부분이 주어진 클래스에 속하는 데 기여했는지 이해하고, 
      이미지에서 객체 위치를 추정하는데 도움이 됨 

** 히트맵(heatMap)
히트맵에서 사용되는 전형적인 컬러맵은 파란색(낮은 값), 녹색, 빨간색(높은 값)을 사용하는 jet 컬러맵.

** 1. 중간층의 활성화 시각화하기.
어떤 입력이 주어졌을 때 여러 합성곱과 풀링 층이 출력하는 특성 맵을 그리는 것.
(층의 출력이 활성화 함수의 출력이여서 종종 활성화(activation)라고 부름)

네트워크에 의해 학습된 필터들이 어떻게 입력을 분해하는지 보여 줌.(필터를 지나고 난 뒤의 이미지를 확인할 수 있기 때문!)
너비, 높이, 깊이 3개의 차원에 대해 특성 맵을 시각화 하는 것이 좋음
각 채널은 비교적 독립적인 특성을 인코딩 하므로, 특성 맵의 각 채널 내용을 독립적인 2D 이미지로 그리는 것이 괜찮음! ( -> 각 층마다 특성 개수만큼 이미지로 그려지겠지 ?) 

확인하고 싶은 특성 맵을 추출하기 위해, 이미지 배치를 입력으로 받아 모든 합성곱과 풀링 층의 
활성화를 출력하는 케라스 모델을 만들 것
-> 케라스의 Model 클래스 사용( <-> Sequential 사용)

모델 객체를 만들 때 2개의 파라미터 필요
- 입력 텐서
- 출력 텐서

Model 클래스를 사용하면 Sequential과는 달리 여러 개의 출력을 가진 모델을 만들 수 있음.

각 층마다의 활성화를 시각화 하면 생기는 특징들
- 첫 번째 층은 여러 종류의 edge 감지기를 모아 둔 것 처럼 보임
  -> 이 단계의 활성화에는 초기 사진의 모든 정보가 유지

- 상위 층으로 갈수록 '고양이 눈', '고양이 귀' 처럼 고수준의 개념을 인코딩하기 시작함 
  -> 이미지의 시각적 콘텐츠에 관한 정보가 점점 줄어들고, 이미지의 클래스에 관한 정보가 점점 증가!

- 비어 있는 활성화가 깊어짐에 따라 늘어남
  첫 번째 층에서는 모든 필터가 입력 이미지에 활성화되었지만, 층을 올라가면서 활성화되지 않는 필터들이 생김.
  필터에 인코딩된 패턴이 입력 이미지에 나타나지 않았다는 것을 의미(입력 이미지에 값이 없다보니 커널을 거친 필터도 당연히 안보인다!)

중간층의 활성화 시각화를 통해 알 수 있는 점

층에서 추출한 특성은 층의 깊이를 더해 갈수록 더 추상적이게 됨
심층 신경망은 입력되는 원본 데이터에 대한 정보 정제 파이프라인처럼 작동함

** 2. 컨브넷 필터 시각화 하기
각 필터가 반응하는 시각적 패턴을 그려 보는 것.
빈 입력 이미지에서 시작해 특정 필터의 응답을 최대화하기 위해 컨브넷 입력 이미지에 경사 상승법을 적용
-> 입력 데이터를 최대로 하게 되면, 해당 데이터가 필터를 거쳤을 때 최대로 반응한다는 의미와 동일.
    즉, 이 필터가 어떤 경우일 때 최대로 응답하는지를 알 수 있게 된다.

전체 과정
특정 합성곱 층의 한 필터 값을 최대화하는 손실 함수 정의
이 활성화 값을 최대화하기 위해 입력 이미지를 변경하도록 확률적 경사 상승법을 사용

** 소스 분석
1. ImageNet에 사전 훈련된 VGG16 네트워크에서 block3_conv1 층 필터 0번의 활성화를 손실로 정의
2. 경사상승법을 구현하기 위해 모델의 입력에 대한 손실의 그래디언트가 필요

** 경사 상승법 과정을 부드럽게 하기 위해 사용하는 한 가지 기법
그래디언트 텐서를 L2 노름(텐서에 있는 값을 제곱한 합의 제곱근)으로 나누어 정규화하는 것!
-> 이렇게 하면 입력 이미지에 적용할 수정량의 크기를 항상 일정 범위 안에 놓을 수 있음.
-> 이 방법을 그래디언트 클리핑(gradient clipping) 이라고 함.

** 그래디언트 클리핑(gradient clipping)
그래디언트 텐서를 L2노름으로 나누어 정규화 하는 것.
L2 노름으로 나눈 그래디언트의 L2 노름은 1이 됨.
keras.optimizers 모듈 아래 있는 옵티마이저를 사용할 땐 clipnorm, clipvalue 매개변수를 설정하여 자동으로 그래디언트
클리핑을 수행할 수 있음!

clipnorm 매개변수 값이 그래디언트의 L2 노름보다 클 경우 각 그래디언트의 L2 노름을 clipnorm 값으로 정규화 함.
clipvalue 매개변수를 지정하면 그래디언트 최대 절대값은 clipvalue가 됨

경사 상승법은 keras.optimizers 모듈 아래 있는 옵티마이저를 사용할 수 없고, 직접 학습 단계를 구현해야함
keras.backend.function() 함수는 입력 값을 받아 지정된 출력 텐서들을 얻을 수 있는 keras.backend.Function 객체를 만들어 줌

필터 시각화를 통해 컨브넷 층이 바라보는 방식을 이해할 수 있음
컨브넷의 각 층은 필터의 조합으로 입력을 표현할 수 있는 일련의 필터를 학습!
-> 이는 푸리에 변환을 사용하여 신호를 일련의 코사인 함수로 분해할 수 있는 것과 비슷

이 컨브넷 필터들은 모델의 상위 층으로 갈수록 점점 더 복잡해지고 개선됨!
- 모델에 있는 첫 번째 층(block1_conv1)의 필터는 간단한 대각선 방향의 에지와 색깔을 인코딩
- block2_conv1 필터는 에지와 색깔의 조합으로 만들어진 간단한 질감을 인코딩
- 더 상위 층의 필터는 깃털, 눈, 나뭇잎 등 자연적인 이미지에서 찾을 수 있는 질감을 닮아 가기 시작.

** 클래스 활성화의 히트맵 시각화하기.
이미지의 어느 부분이 컨브넷의 최종 분류 결정에 기여하는지 이해하는데 도움이 됨
분류에 실수가 있는 경우, 컨브넷의 결정 과정을 디버깅 하는데 도움을 줌
일반적으로 ** 클래스 활성화 맵(Class Activation Map, CAM) 시각화라고 부름

입력 이미지에 대한 클래스 활성화의 히트맵을 만듬
CAM은 특정 출력 클래스에 대해 입력 이미지의 모든 위치를 계산한 2D 점수 그리드.
클래스에 대해 위치가 얼마나 중요한지 알려줌

예제는 Grad-CAM : Visual Expanations from Deep Networks via Gradient-based localization 에 기술되어 있는 것.
입력 이미지가 주어지면 합성곱 층에 있는 특성 맵의 출력을 추출.
특성 맵의 모든 채널 출력에 채널에 대한 클래스의 그래디언트 평균을 곱함.

** '입력 이미지가 각 채널을 활성화하는 정도' 에 대한 공간적인 맵을 '클래스에 대한 각 채널의 중요도'로
가중치를 부여하여 '입력 이미지가 클래스를 활성화하는 정도'에 대한 공간적인 맵을 만드는 것!

## -------------------------------------------------------------- ##

**  요약
- 컨브넷은 시각적인 분류 문제를 다루는데 최상의 도구!
- 컨브넷은 우리가 보는 세상을 표현하기 위한 패턴의 계층 구조와 개념을 학습
- 학습된 표현은 쉽게 분석할 수 있음! 컨브넷은 블랙박스가 아닙니다!
- 이미지 분류 문제를 풀기 위해 자신만의 컨브넷을 처음부터 훈련시킬 수 있음
- 과대적합을 줄이기 위해 데이터를 증식하는 방법을 배움
- 사전 훈련된 컨브넷을 사용하여 특성 추출과 미세 조정하는 방법을 배움 
- 클래스 활성화 히트맵을 포함하여 컨브넷이 학습한 필터를 시각화 할 수 있음!