# Machine Learning 상식 정리
- 텐서 플로우 블로그(핸즈온 머신러닝)를 보고 기억하고 있어야 할 것들 간단 정리

## 머신러닝의 간단 정의
- 지속적인 프로그래밍 없이 스스로 학습하여 특정한 결과를 도출하는 연구 분야

## 머신러닝의 효과가 뛰어난 분야
- 기존 솔루션으로는 지속적인 수정과 유지보수가 필요한 문제
- 전통적인 해결방식으로는 너무 복잡하여 해결할 수 없는 문제 
- 유동적인 환경에서의 문제 해결 : 데이터 추가시 지속적인 학습이 가능 
- 복잡한 문제와 대량의 데이터에서 새로운 통찰 얻기

## 머신러닝 시스템의 종류
- 머신러닝 시스템의 종류는 굉장히 많지만, 다음과 같이 크게 분류할 수 있음

### 학습하는 동안의 감독 형태와 정보량에 따른 분류
- 지도 학습
- 비지도 학습
  - ex) 블로그 방문자에 대한 계층적 군집 분석
- 준지도 학습(semi-supervised Learning)
  - 대부분의 데이터가 레이블이 없고 몇 개만 레이블이 있는 경우 학습할 경우 준지도 학습이라고 함
  - ex) 구글 포토 호스팅 서비스
- 강화 학습(reinforcement Learning)
  - 학습하는 시스템을 에이전트라고 부르며 환경을 관찰하여 행동을 실행하고 그 결과 보상 또는 벌점을 받는 형태
  - 시간이 지나면 최고의 보상을 받기위한 정책(policy)라고 부르는 최상의 전략을 스스로 학습함

### 배치 학습과 온라인 학습
- 온라인 학습
  - 모델 학습에 필요한 데이터를 부분적으로 사용하여 모델을 만들어내는 학습 방법
  - 데이터를 순차적으로 미니배치라고 부르는 작은 묶음 단위 또는 한개씩 학습시킴
  - 온라인 학습이라는 용어에 혼동x, 보통 오프라인에서 학습함
- 배치 학습  
  - 모델 학습에 필요한 데이터를 한꺼번에 사용해 모델을 만들어내는 학습 방법
  - 모델이 데이터를 기반으로 점진적으로 학습할 수 없음
  - 많은 컴퓨팅 자원이 필요
  
### 사례 기반 학습과 모델 기반 학습 
- 머신러닝 시스템은 어떻게 모델이 일반화 되는가에 따라 달라질 수 있음
- 사례 기반 학습
  - 데이터의 사례를 기억하여 학습하는 형태
  - 유사도 측정을 통해 새로 들어온 데이터를 일반화함
- 모델 기반 학습
  - 데이터를 학습하여 가장 잘 일반화할 수 있는 가중치(파라미터) 값을 정하는 모델 기반 학습
  - 이 모델을 만들어 예측에 사용

## 나쁜 데이터란? 
- 충분하지 않는 양의 데이터
- 대표성이 없는 데이터  
  - 샘플링 잡음, 샘플링 편향 등을 조심해야 함
- 낮은 품질의 데이터  
  - 많은 data noise, data 결측, 이상치 등..
- 관련 없는 특성
  - feature selection, feature extraction 등을 수행

## 나쁜 모델
- 훈련 데이터 과대적합(overfitting)
  - overfitting을 줄이려면, 데이터 증가시키기  
  - 모델 규제(regularization)
  - 데이터 잡음 줄이기
  - 모델 제약을 가하여 단순화 시키기  
- 훈련 데이터 과소적합(underfitting)
  - 특성 추가
  - 파라미터가 더 많은 모델로 변경
  - 규제 줄이기

## 테스트와 검증
- 테스트 데이터로 오류율을 계산하는 것을 <b>일반화 오차(generalization error), 또는 외부 샘플 오차(out-of-sample error)라고 함</b>
- <b>하이퍼파라미터 튜닝 검증 위하여 테스트 데이터를 여러번 사용하게 되면 테스트 데이터에 과적합될 우려가 있으므로, 하이퍼파라미터 튜닝시 validation set 사용해야 함</b>
- 가장 좋은 방법은 테스트 세트로 단 한번의 테스트를 수행하는 것
- <b>데이비드 윌퍼트는 데이터에 관해 완벽하게 어떠한 가정도 하지 않으면 특정 모델을 선택할 어떠한 근거도 없다고 말함</b> --> NFL(No Free Launch) 이론이라고 함

## 큰그림 보기
- 제곱항을 합한 것의 제곱근(RMSE)의 계산은 <b>유클리디안 노름</b>이라고 함
- 절댓값의 합을 구하는 노름은 <b>맨해튼 노름</b>이라고 함
- 노름의 지수가 클수록 이상치 값에 민감함

## 테스트 세트 만들기
- 큰그림을 본 다음에 테스트 세트를 만들어서 절대 처다보지 말아라
- 만약 테스트 세트를 들여다 본다면 우리의 뇌는 굉장히 빠르게 과대적합 하기 때문에 테스트 데이터에 적합한 모형을 자꾸 찾으려고 할 수 있음. 즉 자꾸 테스트 데이터에 대한 일반화 오차를 추정하면 매우 낙관적인 추정이 되며, 향후 시스템을 론칭했을 때 기대와는 다른 성능이 나올 수 있음. --> 이를 <b>데이터 스누핑(snooping) 편향</b>이라고 함
- 테스트 세트는 절대로 변경되면 안됨. 즉 새로운 데이터가 들어왔을 때도 테스트 데이터셋은 변경되면 안됨
- 샘플에 변경 불가능한 고유한 식별자가 있어야 하고, 이러한 식별자를 활용하여 테스트셋이 변경되지 않도록 해야함.  
ex) 식별자의 해시값을 계산하여 해시의 마지막 바이트 값이 51보다 작거나 같은 샘플만 고름  
ex) 위도와 경도 같은 절대 변하지 않는 값으로 id를 생성함
- 무작위 샘플링을 진행할 때 데이터가 적으면 샘플링 편향이 발생할 우려가 있음  
  즉 계층적 샘플링을 수행해야 할 수 있음
- <b>도메인 전문가가 특정 특성(수치형)이 예측에 중요하다고 알려주었을 때, 이러한 수치형 데이터의 분포의 특성이 테스트 셋에도 잘 반영되도록 편성해야 함</b>  
ex) 중요한 피처를 계층으로 나누고 해당 계층에서 테스트 셋 비율만큼 샘플링 수행

## 데이터 이해를 위한 탐색과 시각화
- 상관계수는 선형 관계만 탐색 할 수 있으며, 기울기와는 상관이 없음
- 산점도를 보았을 때 특정한 일직선이 나타난다면 제거하는 것이 좋을 수 있음을 알자
- 특성 조합으로 실험을 해보아야 함
  - ex) 특정 구역의 방 개수는 얼마나 많은 가구가 사는지에 따라서 중요도가 달라짐  
    따라서 가구당 방개수로 파생변수를 생성하는 것이 중요
- <b>향후 테스트 데이터에 대해서 누락값이 발생할 수 있으므로, 모든 수치형 데이터에 대한 대체 값(중앙값이면 중앙값, 평균값이면 평균값 등)을 전부 저장해 두는 것이 좋음</b>

## 머신러닝 알고리즘을 위한 준비
- min-max scaling(정규화)과 표준화
- 정규화는 반드시 값이 0 ~ 1 사이에 위치 한다는 특징이 있음 
- 표준화는 값의 범위가 무한대이므로, 특정 모델에서 문제가 발생할 수 있음  
  예를 들어 딥러닝 모형은 수치값이 0~1 사이임을 기대함
- 표준화는 이상치에 덜 민감함. 예를 들어 정규화에서 갑작스레 큰 값인 100이 있다면 대부분의 값이 0에 가깝게 될 우려가 있음