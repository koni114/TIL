# Xgboost
## boosting
- boosting은 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법
- 부스팅 방법의 아이디어는 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것
- 부스팅 방법에는 여러가지가 있지만 가장 인기 있는 것은 <b>Adaboost</b>와 <b>gradient boosting</b>임

## Adaboost
- 이전 예측기를 보완하는 새로운 예측기를 만드는 방법은 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것
- 이렇게 하면 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰지게 됨. 이것이 Adaboost에서 사용하는 방식

### Adaboost 예시
- Adaboost 분류기를 만들 때 먼저 알고리즘 기반이 되는 첫 번째 분류기를 훈련 세트에 훈련시키고 예측을 만듬
- 그 다음에 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높임(이 가중치는 오류율 계산때 사용)
- 두 번째 분류기는 업데이트된 가중치를 사용해 훈련 세트에서 훈련하고 다시 예측을 만듬
- 그 다음에 다시 가중치 업데이트를 하는 식으로 계속됨
- 이런 연속된 학습 기법은 경사 하강법과 비슷한 면이 있음
- 경사 하강법은 비용을 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면 Adaboost는 점차 더 좋아지도록 예측기를 추가함
- 모든 예측기가 훈련을 마치면 이 앙상블은 bagging이나 pasting과 비슷한 방식으로 예측을 만듬
- 하지만 가중치가 적용된 훈련 세트의 전반적인 정확도에 따라 예측기마다 다른 가중치가 적용됨
- boosting의 단점은 하나의 학습기가 훈련되고 연속적으로 모델이 학습되는 구조이므로, 병렬화를 할 수 없음

### Adaboost 예측기별 가중치 계산 방법
1. 각 샘플 가중치(W(i))는 1/m으로 초기화
2. 첫 번째 예측기가 학습되고 가중치가 적용된 에러율 r(1)이 훈련 세트에 대해서 계산됨
3. j번째 예측기의 샘플 가중치가 적용된 에러율은,
   = 못맞춘 샘플 가중치의 합 / 전체 샘플 가중치의 합
4. 예측기의 가중치 값은 다음 식으로 계산됨  
   = 학습율 파라미터 * log(1 - r(j) / r(j))  
  --> 예측기의 정확도가 높을수록 가중치가 더 높아지게됨
5. 데이터 샘플의 가중치를 업데이트함  
  - W(i)  
    - 해당 샘플을 예측기가 맞췄을 경우 : W(i)   
    -  해당 샘플을 예측기가 맞추지 못했을 경우 : W(i) * exp(예측기 가충치 값) 
6. 그런다음 모든 샘플의 가중치를 정규화  
   - 샘플의 가중치의 총 합으로 각 가중치 값들을 나눔

- 마지막으로 새 예측기가 업데이트된 가중치를 사용해 훈련되고 전체 과정이 반복됨
- 새 예측기의 가중치가 계산되고 샘플의 가중치를 업데이트해서 또 다른 예측기를 훈련시키는 식
- 이 알고리즘은 지정된 예측기 수에 도달하거나 완벽한 예측기가 만들어지면 중지됨
- 예측을 할 때 Adaboost는 단순히 모든 예측기의 예측을 계산하고 예측기 가중치 값들을 더해 예측 결과를 만듬
- 가중치 합이 가장 큰 클래스가 예측 결과가 됨

## Gradient boosting
- Adaboost처럼 gradient boosting은 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가함
- Adaboost와 다른 점은 Adaboost는 샘플의 가중치를 수정하는 대신, 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시킴
- 결정 트리를 기반 예측기로 사용하는 모형을 <b>그래디언트 트리 부스팅(Gradient Tree Boosting)</b> 이라고 함 
- 새로운 샘플에 대한 예측을 만드려면 모든 트리의 예측을 더하면 됨
- `learning_rate` 매개변수가 각 트리의 기여 정도를 조절함.  
  이를 0.1처럼 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 일반적으로 예측의 성능은 좋아짐
- 이는 <b>축소(shrinkage)</b>라고 부르는 규제 방법
- 트리가 훈련할 때 사용할 훈련 샘플의 비율을 지정할 수 있는 `subsample` 매개변수도 지원함  
  이런 기법을 <b>확률적 그레디언트 부스팅(stochastic gradient boosting)</b>이라고 함


## extreme gradient Boosting
- Gradient Boosting 알고리즘을 분산환경에서도 실행할 수 있도록 구현해놓은 라이브러리
- 유연성이 좋음. 평가 함수를 포함하여 다양한 커스텀 최적화 옵션을 제공
- Greedy-algorithm을 사용한 가지치기가 가능. 따라서 과적합이 잘 일어나지 않음
- 또한 과적합 방지를 위해 규제가 포함되어 있음
- early stopping을 내부적으로 제공함
- xgboost는 트리를 만들때 CART(Classification and Regression Trees) 기반 트리를 생성
- 즉 수식으로는 어떠한 의사결정트리가 주어졌을 때, 해당 트리의 score(information gain)를 계산할 수 있고, 이러한 score 기반으로 나무를 greedy algorithm에 의거하여 가지를 계속 펼쳐나가고(<b>Split Finding</b>), score가 (-)되는 시점에 가치지기를 수행하는 방식

### XGBoost 일반 파라미터
- eta : learning Rate, 트리에 가지가 많을 수록 과적합이 일어나기 쉬움. 매 부스팅 step마다  
  weight를 주어 부스팅 과정에서 과적합이 일어나지 않게 함
- gamma : information Gain 식에서 -r로 표현된 바 있음. 이것이 커지면, 트리 깊이가 줄어들어 보수적인 모델이 됨. 디폴트 값은 0
- max_depth : 한 트리의 maximum depth. 숫자를 키울수록 모델의 복잡도가 커짐. 과적합 하기 쉬움. 디폴트 값은 6, 이렇게 되면 노드 개수는 최대 2^6 = 64개
- lambda(L2 reg-form) : L2 Regularization Form에 달리는 Weight. 커질수록 보수적인 모델이 됨
- alpha(L1 reg-form)  : L1 Regularization Form. 숫자가 커질수록 보수적인 모델이 됨 
- num_rounds : boosting 라운드를 결정. 랜덤하게 생성되는 모델이니만큼 이 수가 적당한 것이 좋음

### 파라미터
* 파라미터를 제대로 이해하고 있어야 시행 착오가 적음
* 이진 분류 문제에 linear-regression을 적용해 놓고 L1, L2 파라미터를 조정하고 있다면 안됨!
* 파라미터 순위 팁
  * eta -> lambda -> alpha  ....


#### R - XGBTree package 파라미터 종류
* gamma : 이 값이 커지면 트리 깊이가 줄어들어 보수적인 모델이 됨
* max depth : maximum depth
* min child weight : 특정 순도에 도달하면 가지치기를 종료.
* eta : learning rate, boosting step 마다 weight를 주어 부스팅 과정에서 과적합이 일어나지 않도록 함
* subsample : 각 트리마다의 train data sampling 비율, 너무 적게 주면 under_fitting 발생
* colsample_bytree : 각 트리마다의 feature 샘플링 비율

## 참고 블로그
- https://brunch.co.kr/@snobberys/137