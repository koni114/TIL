# LightGBM

## LightGBM 배경
- 다중 분류, 클릭 예측, 순위 학습에 주로 사용되는 GBDT(Gradient Boosting Decision Tree)는 굉장히 유용한 머신러닝 알고리즘이며, XGBoost나 pGBRT 등 효율적인 기법의 설계를 가능하게 함
- 하지만 고차원이고 대량 데이터 분석에는 만족스러운 결과를 내지 못하는 경우도 있는데, 그 이유는 모든 가능한 분할점에 대해서 information gain을 평가하기 위해 전체 데이터를 SCAN 하기 때문. 이는 굉장히 시간 소모적임
- LightGBM은 해당 문제를 해결하기 위하여 2가지 최신 기술을 도입함
- 첫 번째는 <b>GOSS(Gradient Based One Side Sampling)</b>이며, 기울기(gradient)가 큰 개체가 정보 획득에 있어 더 큰 역할을 한다는 아이디어에서 비롯된 기술  
gradient가 큰 개체들은 유지되며, 작은 개체들은 일정 확률에 의해 랜덤하게 제거됨
- 두 번째는 <b>EFB(Exclusive Feature Bunding)</b>으로, 변수 개수를 줄이기 위해 상호 배타적인 변수들을 묶는 기법. one-hot encoding된 변수와 같이 Sparse한 변수 공간에서는 서로 상호 배타적인 경우가 많음  
따라서 본 테크닉은, 최적 묶음 문제를 그래프 색칠 문제로 치환하고 일정 근사 비율을 갖는 greedy algorithm으로 문제를 해결

## Preliminaries
- GBMT는 Decision Tree의 앙상블 모델임. 즉 각각의 반복에서 GBMT는 잔차 오차를 적합함으로써 DecisionTree를 학습시킴 
- 이 학습 과정에서 가장 시간이 많이 걸리는 과정이 바로 최적의 분할점을 찾는 것인데, 이를 위한 대표적인 방법에는 <b>Pre-sorted(사전 정렬) 알고리즘</b>과 <b>Histogram-based 알고리즘</b>이 있음
  - <b>Pre-sorted(사전 정렬) 알고리즘</b>  
  사전 정렬한 변수에 대해 가능한 모든 분할점을 나열함으로써 간단하게 최적의 분할점을 찾을 수 있지만 효율적이지 못함
  - <b>Histogram-based 알고리즘</b>  
  연속적인 변수를 이산적인 구간으로 나누고 이 구간을 학습하여 학습과정 속에서 feature histogram을 구성
- 학습 데이터 양을 줄이기 위해 가장 쉽게 생각할 수 있는 방법은 Down sampling이 될 것임
- 이는 만약 데이터 개체의 중요도(Weight)가 설정한 임계값을 넘지 못할 경우 데이터 샘플들이 filtering되는 것을 의미함
- SGB(stochastic Gradient Boosting)의 경우 약한 학습기를 학습시킬 때 무작위 부분집합을 사용하지만, SGB를 제외한 Down sampling 방식은 AdaBoost에 기반하였기 때문에 바로 GBDT에 적용할 수 없음
- 왜냐하면 Adaboost와는 달리 GBDT에는 데이터 개체에 기본 가중치가 존재하지 않기 때문
- 비슷한 방식으로 feature 수를 줄일 수 있는데, 영향력이 약한(weak) feature를 제거하는 것이 자연스러울 것임
- 그러나 이러한 접근법은 변수들 사이에 큰 중복요소들이 있을 것이라 가정하는데, 실제로 그렇지 않을 수 있음
- 실제상황에서 사용되는 큰 데이터셋은 sparse한 데이터일 확률이 높음
- Pre-sorted 알고리즘에 기반한 GBMT인 경우 0을 무시함으로써 학습 비용을 절감할 수 있지만, Histogram-based 알고리즘에 기반한 GBMT는 효율적인 희소값 최적화 방안이 없음
- 그 이유는 Histogram-based 알고리즘은 피처 값이 0이든, 1이든, 데이터 개체마다 피처 구간(bin)을 뽑아내야 하기 때문
- 따라서 Histogram-based 알고리즘에 기반한 GBMT가 희소 변수를 효율적으로 최적화할 방안이 강구됨
- 이를 해결하기 위한 방안으로 GOSS와 EFB인 것임
- GOSS는 데이터 개체 수를 줄이고, EFB는 피처 수를 줄이는 방법론임 

## GOSS(Gradient Based One Side Sampling)
- AdaBoost에서 Sample Weight는 데이터 개체의 중요도를 알려주는 역할을 수행함
- GBDT에서는 기울기(Gradient)가 이 역할을 수행함
- 각 데이터 개체의 기울기가 작으면 훈련 오차가 작다는 것을 의미하므로, 이는 학습이 잘 되었다는 뜻
- 이후 이 데이터를 그냥 제거한다면 데이터의 분포가 변화할 것이므로, 다른 접근법(GOSS)이 필요
- GOSS의 아이디어는 직관적임. 큰 Gradient(훈련이 잘 안된)를 갖는 데이터 개체들은 모두 남겨두고, 작은 Gradient를 갖는 데이터 개체들에서는 무작위 샘플링을 진행하는 것  
좀 더 상세히 설명하자면 다음과 같음
  - 데이터 개체들의 Gradient의 절대값에 따라 데이터 개체들을 정렬함
  - 상위 100a% 개의 개체를 추출함
  - 나머지 개체들 집합에서 100b% 개의 개체를 무작위로 추출함
  - 정보 획득을 계산할 때, 위의 2-3 과정을 통해 추출된 Sampled Data를 상수(1-a/b)를 이용하여 증폭시킴 
- 적절한 a,b 값을 구하는 방법은 차후 과제로 남겨져 있음

##  EFB: Exclusive Feature Bundling
- 희소한 변수 공간의 특성에 따라 배타적인 변수들을 하나의 변수로 묶을 수 있음
- 그리고 이를 배타적 변수 묶음(Exclusive Feature Bundle)이라고 부름
- 정교하게 디자인된 변수 탐색 알고리즘을 통해, 각각의 변수들로 했던 것과 마찬가지로 변수 묶음들로부터도 동일한 변수 히스토그램들을 생성할 수 있게 됨
- 이제 1) 어떤 변수들이 함께 묶여야 하는지 정해야 하며, 2) 어떻게 묶음을 구성할 것인가에 대해 알아볼 것
- 변수들을 가장 적은 수의 배타적 묶음으로 나누는 문제는 NP-hard 문제
- 그래프 색칠 문제를 본 논문의 문제로 환원(reduction)함. 그래프 색칠 문제는 NP-hard이므로 우리는 결론은 추론 가능

### 문제
- G = (V, E)라는 임의의 그래프가 있다고 하자. G의 발생 행렬(Incidence Matrix)의 행들이 우리 문제의 변수에 해당.  
위 정리에서 최적의 묶음 전략을 찾는 것은 NP-hard라고 하였는데, 이는 다항 시간 안에 정확한 해를 구하는 것이 불가능하다는 의미.  
따라서 좋은 근사 알고리즘을 찾기 위해서는 최적 묶음 문제를 그래프 색칠 문제로 치환해야 한다.  
이 치환은 변수(feature)들을 꼭짓점(vertices)으로 간주하고 만약 두 변수가 상호배타적일 경우 그들 사이에 변(edge)을 추가하는 방식으로 이루어진다.  
이후 Greedy 알고리즘을 사용한다.

### 1)에 관한 알고리즘 설명
- 각 변(edge)마다 가중치가 있는 그래프를 구성하는데, 여기서 가중치는 변수들간의 충돌(conflicts)을 의미한다. 여기서 충돌이란 non-zero value가 동시에 존재하여 상호배타적이지 않은 상황을 의미
- 그래프 내에 있는 꼭짓점 차수에 따라 내림차순으로 변수들을 정렬
- 정렬한 리스트에 있는 각 변수를 확인하면서 이들을 작은 충돌(γ로 제어함)이 있는 기존 묶음에 할당하거나, 새로운 묶음을 만든다.
- 이 알고리즘의 시간 복잡도는 변수들의 개수의 제곱에 해당
- 이는 나름 괜찮은 수준이지만 만약 변수들의 수가 매우 많다면 개선이 필요하다고 판단된다
- 본 논문은 그래프를 직접 구성하지 않고 0이 아닌 값의 개수에 따라 정렬하는 방식(0이 아닌 값이 많을 수록 충돌을 일으킬 확률이 높으므로)으로 알고리즘을 수정

### 2)에 관한 알고리즘 설명
- 가장 중요한 것은 변수 묶음들로부터 원래(original) 변수들의 값을 식별할 수 있어야 한다는 것
- Histogram-based 알고리즘은 변수의 연속적인 값 대신 이산적인 구간(bin)을 저장하므로  
   배타적 변수들을 각각 다른 구간에 두어 변수 묶음을 구성할 수 있다. 이는 변수의 원래 값에 offset을 더하는 것으로 이루어 질 수 있다

- 예를 들어 변수 묶음에 변수 2개가 속한다고 할 때,  
  원래 변수 A는 [0, 10)의 값을 취하고, 원래 변수 B는 [0, 20)의 값을 취한다.  
  이대로 두면 [0, 10) 범위 내에서 두 변수는 겹칠 것이므로,  
  변수 B에 offset 10을 더하여 가공한 변수가 [10, 30)의 값을 취하게 한다.  
  이후 A, B를 병합하고 [0, 30] 범위의 변수 묶음을 사용하여 기존의 변수 A, B를 대체한다.

### 파라미터 설명(중요도 별표)
- ** n_estimators : 반복 수행하는 트리의 개수
- *** num_leaves   : 하나의 트리가 가질 수 있는 최대 리프 노드 개수
- *** max_depth    : 트리 최대 depth 크기. num_leaves와 중요한 관계를 가지는데, 2^(max_depth) 보다 항상 작은 num_leaves를 가져야 함
- *** min_child_samples : 리프 노드가 되기 위하여 필요한 최소한의 샘플 수
- *** sub_sample : 데이터 샘플링 비율
- colsample_bytree : 선택할 feature 비율
- reg_alpha : L1 규제
- reg_lambda : L2 규제
- learning_rate : 후반부에 건드리는 것이 좋음. 초반에는 크게 반영하여 모델을 학습하고, 나중에 점차 줄여가자

### 기타 특이사항
- LightGBM은 범주형의 경우 gradient boosting의 각 단계에서 gradient 통계량으로 변환됨

## 용어 정리
- information gain
  - 상위노드의 불순도에서 하위노트의 불순도를 뺀 값.
- 상호 배타적인 feature 
  - 컬럼중 하나에만 값이 있고 나머지는 0인 경우

## 참조 블로그
- https://greeksharifa.github.io/machine_learning/2019/12/09/Light-GBM/  
- https://wkdtjsgur100.github.io/P-NP/