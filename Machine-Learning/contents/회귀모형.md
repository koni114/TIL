## 회귀 분석

### 정규화 회귀 분석
- LASSO, RIDGE, elasticNet  
  -> cv.glmnet function 사용하고, alpha -> 1, 0.5, 0 값에 따라서 달라짐

### p-value 관련 팁
- p-value가 작을 때 해당 가설이 어떤지 헷갈리는 경우가 많다. 이럴 때는  
  p-value가 유의수준보다 작은 경우는 관련이 있음을 뜻하고,    
  p-value가 유의수준보다 큰 경우는 관련이 없음을 뜻한다  

### 최소자승법(OLS: Ordinary Least Squares) 회귀의 가정 고려
- <b>해당 4가지의 가정은 선형 회귀에서만 고려 됨을 명심하자</b>
- 해당 4가지 가정을 위배하는 경우에는 종속변수의 log 변환, 변수 제거, stepwise 방법 사용 등을 생각할 수 있다
- 정규성
  - 잔차가 정규성을 만족하는지의 여부로, 정규분포를 띄는지 여부를 말한다
  - shapiro-wilk 검정, qq-plot을 통해 정규성 여부 확인 가능

- 독립성 
  - 독립성 가정은 독립 변수간의 독립성을 가정을 의미한다
  - <b>이는 즉 "다중회귀분석" 에서만 적용되는 가정임을 말함</b>
  - 독립 변수들 간의 독립성을 검정하려면 첫 번째로 데이터 수집시 생기는 의존성 등을 파악함
  - 이러한 다중공선성이 존재할 때는 특정 독립변수를 제거하거나 하는 조치를 취해주어야 함
  - 분산팽창지수(VIF)를 구해 해당 값이 높은 변수를 제거하거나, Stepwise 기법을 통해 변수를 제거해야 함
  - Durbin-Watson 검정을 통한 독립성 검정  
      --> p-value < 0.05 이면 독립성 가정 기각

- 선형성
  - 종속 변수와 독립 변수의 선형성이 성립하려면, 잔차와 예측 변수 간의 특정 형태가 띄면 안됨 ex) 2차 함수.. 등
  - 예를 들어 2차 함수 형태를 띄는 데이터가 있다고 할 때, 선형 회귀로 예측하려고 하면 종속 변수의 크기가 커질수록 잔차가 커지는 형태임  
    --> 이러한 경우 잔차와 예측 변수 간의 어떤 형태를 가지고 있음
  - 

- 등분산성
  - 잔차(Y-YHat)에 대한 등분산성이 성립해야 함
  - 잔차가 등분산성을 만족하지 않는다는 의미는 구간 별로 크기가 변하는 Y 값들이 있다는 것을 의미한다  
  - 이러한 경우 다변량 가법 회귀 스플라인 모델(MARS) 등을 고려할 수 있고, 또는 독립변수를 추가해서 문제를 해결 할 수 있다
  - ncvTest() : p-value < 0.05 이면 등분산성 위배  
  - spreadLevelPlot() : nonhorizontal 하다면, 등분산성 위배   

- 회귀의 가정 고려를 전반적으로 확인하고 싶다면, `gvlma` library의 `gvlma` function을 이용하자

~~~r
library(gvlma)
gvmodel <- gvlma(fit)
summary(gvmodel)
~~~

- outlier
  - car package의 `outlier` function을 이용하여 이상치가 있는지 확인!

- high leverage point
  - 예측 변수의 관측치    
    이상치가 있는지 확인이 가능  
    일반적으로 평균치의 2배, 3배의 선을 계산하여 확인

- 영향 관측치(influential observation)
  - 회귀 모형 계수에 영향을 미치는 값을 확인시켜 줌

- 다중 공선성(multicolinearity)
  - 독립 변수들 간의 강한 상관 관계가 나타나는 것을 의미
  - VIF(분산 팽창 지수)^2 > 4 이면 다중공선성이 있다고 판단
  - 통계모형 계수의 신뢰구간이 커짐

### 회귀 변수의 교정 방법
- 관측치 제거
  - 이상치나 영향 관측치를 계속 제거해 가면서 모형 적합을 수행
  - 관측치 제거시 매우 주의. 이상치나 영향 관측치는 큰 정보를 제공해 줄 수 있기 때문에  

- 종속 변수의 변환
  - box-cox의 lambda transformation
  - `boxTidwell()` : 예측 변수의 가장 적합한 람다 값 제공  
    --> p-value < 0.05 이면, 변환하는 것이 유의미 함
  - `spreadLevelPlot()` : 등분산성 개선을 위한 자료의 변환 제시

- 변수의 추가 또는 제거
  - 다중공선성의 변수 하나를 제거 --> 해석에 도움

- 다른 회귀 방법의 사용
  - 다중공선성 문제    --> Ridge Regression
  - 이상치, 영향관측치 --> robust Regression
  - 정규성 가정 위배   --> 비모수 회귀모형
  - 선형성 문제        --> 비선형 회귀모형
  - 독립성 가정 위반   --> 시계열모형, 다수준 회귀모형
  - 일반화 선형모형 사용

### 최선의 회귀 모형 고르기
- 모형 선택 시 anova 분석을 통해 두 모형의 차이가 유의한지 확인  
  유의하지 않으면, 변수를 제거한 모형을 선택해도 무방하다는 의미  
  --> `anova(fit1, fit2)`

- adjusted R^2
  - 단순 R^2 를 선택하게 되면 예측 변수의 개수가 증가 할수록 무조건 값이 증가하므로,  
    adjusted R^2 를 보면서 적절한 예측 변수 조합을 판단해 내자

- AIC(Akaike information criterion) 이용
  - AIC란 특정 데이터셋에 대해서 모델의 품질을 판단하는 지표 중 하나
  - AIC = -2 * log(L) + 2 * p  
    --> L : Likelihood, p : parameter 수  
  - AIC가 작을수록, 적합한 판단 모형 가능
  - AIC를 최소화 한다는 것은 우도(likelihood)를 가장 크게하면서, 변수의 개수를 가장 적게 하는 것  
  - 우도를 사용하는 것은 Bias 와 variance의 균형점을 찾는 것을 말함  
    변수의 개수가 적으면 Bias 크고, variance는 작아짐    
    변수의 개수가 많으면 Bias 작고, variance는 커짐  
  - stepwise regression을 통해 자동으로 변수를 찾게 할 수 있음  
    AIC가 가장 작고, 더 이상 작아지지 않는 순간까지!  
    중요한 것은 stepwise는 모든 경우의 수의 최적의 모형을 찾아주지는 않는다
  - 모든 부분집합 회귀 

- BIC 이용
  - BIC  = -2 log(L) + log(n) * p
  - AIC의 식을 수정 보완한 것인데, AIC보다 독립 변수의 수가 늘어날 수록 패널티가 훨씬 큼

- Mallow's Cp
  - Cp는 bias가 적은 방향으로 변수를 선택하며, 변수의 수와 비슷할수록 더 좋다고 판단하는 것

### ROC Curve
- x축 : False Positive Rate : p를 f로 잘못 예측한 확률
- y축 : True Positive Rate  : p를 p로 잘 예측한 확률
- unblanced한 data에 대해서 accuracy만 가지고 평가하기는 어렵다  
  따라서 ROC Curve를 이용  
  