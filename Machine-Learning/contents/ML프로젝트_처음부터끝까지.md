# ML Project 처음부터 끝까지
- Machine Learning 프로젝트 수행시, 진행할 주요 단계는 다음과 같음
  - 큰 그림을 봄(문제 정의, 시작 단계에서의 데이터 필요성 여부 등)
  - 데이터를 구합니다
  - 데이터로부터 통찰을 얻기 위해 탐색, 시각화
  - 머신러닝 알고리즘을 위해 데이터 준비
  - 모델 선택, 훈련
  - 모델을 상세하게 조정
  - 솔루션을 제시
  - 시스템을 론칭하고 모니터링하고 유지보수

## 큰 그림 보기
### 문제 정의
- '비즈니스의 목적이 정확히 무엇인가?' 를 생각해야 함
- 모델 만들기가 최종 목적은 아니며, 해당 모델의 결과가 어디서 사용될 것인지 생각해야 함
- 현재 솔루션이 구현되어 있다면 어떤 상태인지 정확히 파악해야 함
- 앞서 구분한 머신 러닝 시스템에 따라 문제를 정확히 정의해야 함 
### 성능 측정 지표 선택
- 이상치로 보이는 데이터가 많다고 하면 RMSE -> MAE를 선택할 수 있음
- 제곱항을 합한 것의 제곱근 계산(RMSE)는 <b>유클리디안 노름</b>에 해당 
- 절댓값의 합을 계산하는 것은 l1 노름에 해당하며 <b>맨해튼 노름</b>이라고도 함
- 노름의 지수가 클수록 이상치 값에 민감함

## 데이터 가져오기
### 테스트 세트 만들기
- 큰그림을 본 다음에 테스트 세트를 만들어서 절대 처다보지 말아라
- 만약 테스트 세트를 들여다 본다면 우리의 뇌는 굉장히 빠르게 과대적합 하기 때문에 테스트 데이터에 적합한 모형을 자꾸 찾으려고 할 수 있음. 즉 자꾸 테스트 데이터에 대한 일반화 오차를 추정하면 매우 낙관적인 추정이 되며, 향후 시스템을 론칭했을 때 기대와는 다른 성능이 나올 수 있음. --> 이를 <b>데이터 스누핑(snooping) 편향</b>이라고 함
- 테스트 세트는 절대로 변경되면 안됨. 즉 새로운 데이터가 들어왔을 때도 테스트 데이터셋은 변경되면 안됨
- 샘플에 변경 불가능한 고유한 식별자가 있어야 하고, 이러한 식별자를 활용하여 테스트셋이 변경되지 않도록 해야함.  
ex) 식별자의 해시값을 계산하여 해시의 마지막 바이트 값이 51보다 작거나 같은 샘플만 고름  
ex) 위도와 경도 같은 절대 변하지 않는 값으로 id를 생성함
- 무작위 샘플링을 진행할 때 데이터가 적으면 샘플링 편향이 발생할 우려가 있음  
  즉 계층적 샘플링을 수행해야 할 수 있음
- <b>도메인 전문가가 특정 특성(수치형)이 예측에 중요하다고 알려주었을 때, 이러한 수치형 데이터의 분포의 특성이 테스트 셋에도 잘 반영되도록 편성해야 함</b>  
ex) 중요한 피처를 계층으로 나누고 해당 계층에서 테스트 셋 비율만큼 샘플링 수행

## 데이터 이해를 위한 탐색과 시각화
- 상관계수는 선형 관계만 탐색 할 수 있으며, 기울기와는 상관이 없음
- 산점도를 보았을 때 특정한 일직선이 나타난다면 제거하는 것이 좋을 수 있음을 알자
- 특성 조합으로 실험을 해보아야 함
  - ex) 특정 구역의 방 개수는 얼마나 많은 가구가 사는지에 따라서 중요도가 달라짐  
    따라서 가구당 방개수로 파생변수를 생성하는 것이 중요
- <b>향후 테스트 데이터에 대해서 누락값이 발생할 수 있으므로, 모든 수치형 데이터에 대한 대체 값(중앙값이면 중앙값, 평균값이면 평균값 등)을 전부 저장해 두는 것이 좋음</b>

## 머신러닝 알고리즘을 위한 준비
- sklearn에서 `OneHotEncoder`를 사용하면 Sparse Matrix로 변환. 이는 수천개의 카테고리가 존재할 경우 매우 효율적임. why? 희소 행렬은 0이 아닌 원소의 위치만 저장  
만약 넘파이 배열로 바꾸고 싶다면, `.toarray()` 사용
- <b>만약 cardinality가 매우 많다면, 훈련을 느리게 하고 성능을 감소시킬 수 있음. 이 때 범주형 입력값을 수치형 입력값으로 변경하면 좋음</b>  
ex) ocean_proximity 특성을 해안까지의 거리로 변경  
ex) 국가 코드는 1인당 GDP로 바꿀 수 있음
- 각 카테고리를 임베딩(embedding)으로 학습 가능한 저차원 백터로 바꿀 수 있음

### 나만의 변환기
- sklearn이 유용한 변환기를 많이 제공하지만, 특별한 정제 작업이나 어떤 특성을 조합하는 등의 작업을 위해 자신만의 변환기를 만들어야 할 때가 있음
- 내가 만든 변환기를 sklearn의 기능과 매끄럽게 연동하고 싶을 수 있음
- `fit()`, `transform()`, `fit_transform()` 메서드를 구현한 파이썬 클래스를 만들면 됨
~~~python
from sklearn.base import BaseEstimator, TransformerMixin
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, X, y=None):
    def fit():
    def transform():
    def fit_transform():
    ...
~~~

### 특성 스케일링
- min-max scaling(정규화)과 표준화
- 정규화는 반드시 값이 0 ~ 1 사이에 위치 한다는 특징이 있음 
- 표준화는 값의 범위가 무한대이므로, 특정 모델에서 문제가 발생할 수 있음  
  예를 들어 딥러닝 모형은 수치값이 0~1 사이임을 기대함
- 표준화는 이상치에 덜 민감함. 예를 들어 정규화에서 갑작스레 큰 값인 100이 있다면 대부분의 값이 0에 가깝게 될 우려가 있음
- 스케일링은 훈련 데이터에 대해서만 `fit()` 을 적용해야 함  
  그런 다음 훈련 세트와 테스트 세트에 대해 `transform()` 메서드 적용

### 변환 파이프라인
- `Pipeline` 클래스를 이용하여 변환 파이프라인을 제작할 수 있음
~~~python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
    ('name', SimpleImputer(strategy = 'median')),
    ...
])

data_trans = num_pipeline.fit_transform(data)
~~~
- `ColumnTransformer` class를 이용하여 원하는 컬럼에 각각 파이브라인을 적용할 수 있음
- 최종 return np.array가 밀집행렬과 희소행렬이 섞여 있을 때, 밀집 정도(0이 아닌 원소의 비율)를 추정
- 밀집도가 임곗값( default --> `sparse_threshold = 0.3`)보다 낮으면 희소 행렬을 반환
- 지정하지 않은 열은 삭제됨. 만약 바꾸고 싶다면 `remainder` 하이퍼파라미터 변경 필요
- 파이썬의 `pickle` 패키지나 큰 넘파이 배열을 저장하는 데 아주 효율적인 `joblib`를 사용하여 사이킷런 모델을 간단하게 저장 가능
~~~python
import joblib
joblib.dump(my_model, "my_model.pkl")
my_model_loaded = joblib.load("my_model.pkl")
~~~

## 모델 세부 튜닝
- `GridSearchCV` 함수가 `refit=True` 로 초기화되었다면, 교차 검증으로 최적의 추정기를 찾은 다음 전체 훈련데이터를 가지고 다시 훈련시킴
- 데이터 준비 단계를 하나의 하이퍼파라미터처럼 다룰 수 있음  
  예를 들면 그리드 탐색이 확실하지 않은 특성을 추가할지 말지 자동으로 정할 수 있음  
  (ex) CombinedAttributesAdder 변환기의 add_bedrooms_per_room 하이퍼파라미터를 사용하여 특성 추가 여부를 결정)
- 비슷하게 이상치, 특성 선택 등을 자동으로 처리하는 데 그리드 탐색을 사용
- <b>데이터 준비 단계와 모델을 연결한 파이프라인을 그리드 탐색에 적용할 때 데이터 준비 단계를 캐싱하면 탐색 시간을 줄일 수 있음</b>
- 규제처럼 설정값이 연속형인 경우 `RandomSearch`가 권장됨. sklearn 0.24 버전에서 파라미터 검색 범위를 좁혀가면서 컴퓨팅 자원을 늘려가는 `HalvingGridSearchCV`와 `HalvingRandomSearchCV`가 추가됨  
- RandomSearch는 반복 횟수를 조정하는 것만으로 하이퍼파라미터 탐색에 투입할 컴퓨팅 자원을 제어할 수 있음

### 최상의 모델과 오차 분석
- 최상의 모델을 분석하면 좋은 통찰을 얻는 경우가 많음 --> variance Importance
- 시스템이 특정한 오차를 만들었다면 왜 그런 문제가 생겼는지 이해하고 문제를 해결하는 방법이 무엇인지 찾아야 함(특성 추가, 불필요한 특성을 제거, 이상치를 제거 등)

## 테스트 세트로 시스템 평가하기
- 테스트 세트로 예측 수행시 `full_pipeline` 에서 `fit_transform()` 이 아니라 `transform()` 수행!
- 일반화 오차의 추정이 얼마나 정확한지 신뢰 구간을 구할 수 있음  
  `scipy.stats.t.interval()`를 사용해 일반화 오차의 95% 신뢰 구간을 계산할 수 있음
- 하이퍼파라미터 튜닝을 많이 했다면 교차 검증을 통해 사용해 측정한 것보다 조금 성능이 낮은 것이 보통. why? 검증 데이터에서 좋은 성능을 내도록 세밀하게 튜닝되었기 때문에 새로운 데이터셋에는 잘 작동하지 않을 수 있음)

## 론칭, 모니터링, 시스템 유지보수
- 사용자가 가격 예측하기 버튼을 누르면 데이터를 포함한 쿼리(query)가 웹서버로 전송되어 웹 어플리케이션으로 전달됨
- 이 애플리케이션 코드가 모델의 `predict()` 메서드를 호출
- 모델을 사용할 때 마다 로드하지 않고 서버가 시작할 때 모델을 로드해 두는 것이 좋음 
- 또는 웹 어플리케이션이 REST API를 통해 질의할 수 있는 전용 웹 서비스로 모델을 감쌀 수 있음  
  이렇게 되면 주 애플리케이션을 건들지 않고 모델을 새 버전으로 업그레이드 하기 쉬움
- 고양이와 강아지 사진도 주기적으로 재훈련이 필요한데, 그 이유는 돌연변이가 생겨서가 아니라, 카메라가 계속 바뀌며, 이미지 포맷, 선명도, 밝기, 가로세로 비율 등이 바뀔 수 있음
- 모델의 실전 성능을 모니터링 해야하는데, 어떻게 해야할까? 
  - ex) 추천 시스템 모델이면, 추천 제품의 제품판매량을 기준으로 모니터링 진행  
  이 수치가 많이 줄어든다면 모델 자체의 성능 오류일 수 있음 
  - 모니터링 판단 기준을 사람에게 평가요청을 할 수도 있음(전문가던 비전문가던)
- <b>특히 모델이 실패했을 때 무엇을 할지 정의하고 어떻게 대비할지 관련 프로세스를 모두 준비해야함</b>
- 다음은 자동화 할 수 있는 일부 작업
  - 정기적으로 새로운 데이터를 수집하고 레이블을 담
  - 모델을 훈련하고 하이퍼파라미터를 자동으로 세부 튜닝하는 스크립트 작성  
    작업에 따라 매일 또는 매주 자동으로 이 스크립트를 실행할 수 있음
- <b>입력 데이터 세트의 품질을 평가해야 함</b>
- 모든 만든 모델을 백업해야 함. 새로운 모델이 어떤 이유로 올바르지 않게 작동하는 경우 이전 모델로 빠르게 롤백(roll back)하기 위한 방안을 만들어야 함
- 정말로 가능하다면 모든 버전의 데이터셋도 백업해야 함
