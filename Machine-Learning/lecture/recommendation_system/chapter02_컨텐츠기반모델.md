## chapter02 컨텐츠 기반 모델
- 실제 현업에서 많이 사용하는 모델

### 정의
- 컨텐츠 기반 추천시스템은 사용자가 이전에 구매한 상품중에서 좋아하는 상품들과 유사한 상품들을 추천하는 방법
- item을 벡터 형태로 만들어서 유사한 상품들을 찾아줌
- text 같은 경우는 BERT, Word2Vec, CounterVectorizer 등
- 이미지도 마찬가지로 벡터 데이터로 변환해주는 것부터 시작함
- 벡터들간의 유사도를 계산함

### 유사도 함수
- 유사도 함수 같은 경우는 매우 다양함
- 여기서는 4개만 소개하지만, 실제로는 정말 다양한 유사도 함수가 있고, 상황에 따라서 다르게 사용해야 하는 것이 이 유사도 함수라고 보면 됨

#### 유클리디안 유사도
- 유클리디안 유사도 = 1 / (유클리디안 거리 + 1e-05), 분모가 0이되는 것을 방지하기 위함
- 장점은 계산하기가 쉽지만, p와 q의 분포가 다르거나 범위가 다른 경우에는 상관성을 놓침

#### 코사인 유사도
- 실제 유사도 중 가장 많이 사용하는 방법 중 하나. 
- cos(theta) = A'B / ||A||||B|| 
- 두 벡터가 얼마나 비슷한 '방향'을 가지고 있는지를 계산하는 유사도 계산법
  theta가 0이면 1, 180이면 0으로 방향에 따라서 값이 커지고 작아짐
- 장점은 벡터의 크기가 중요하지 않은 경우에 거리를 측정하기 위한 메트릭으로 사용(ex) 문서내에서 단어의 빈도수 - 문서들의 길이가 고르지 않더라도 문서내에서 얼마나 나왔는지라는 비율을 확인하기 때문에 상관없음)
- 벡터의 크기가 중요한 경우에는 잘 작동하지 않음

#### 유클리디안 유사도 vs 코사인 유사도
- 다음의 그래프를 보면, 회색점이 들어왔을 때, 유클리디안 유사도와 코사인 유사도로 계산 했을 때 각각 다른 점과 제일 가깝다고 판단하는 것을 알 수 있음
![img](https://github.com/koni114/TIL/blob/master/Machine-Learning/img/re01.JPG)
![img](https://github.com/koni114/TIL/blob/master/Machine-Learning/img/re02.JPG)

### 피어슨 유사도
- 상관관계를 분석할 떄 많이 사용하는 유사도
- 실제 피어슨 상관계수라고 보면 됨

### 자카드 유사도
- 집합에서 얼마나 결합된 부분이 있는지를 계산
- J(A, B) =  |A n B| / |A u B|= |A n B| / (|A| + |B| + |A n B|)
- 상황에 적절하게 맞게 유사도를 선정함

### TF-IDF
- TF-IDF은 가장 기본적인 알고리즘
- 특정 문서 내에서 특정 단어가 얼마나 자주 등장하는지를 의미하는 단어 빈도(TF)와 전체 문서에서 특정 단어가 얼마나 자주 등장하는지를 의미하는 역문서 빈도(DF)를 통해 "다른 문서에는 등장하지 않지만 특정 문서에서만 자주 등장하는 단어"를 찾아 문서 내 단어의 가중치를 계산하는 방법
- 용도는 문서의 핵심어를 추출, 문서들 사이의 유사도를 계산, 검색 결과의 중요도를 정하는 작업 등에 활용할 수 있음
- TF(d, t) : 특정 문서 d에서 특정 단어 t의 등장 횟수
- DF(t) : 특정 단어 t가 등장한 문서의 수
- IDF(d, t) : DF(t)에 반비례하는 수 --> log(n / 1 + df(t))
- TF(d, t) * IDF(d, t) = TF-IDF(d, t)

### TF-IDF를 사용하는 이유
~~~
doc1 : {I like this movie. I love this movie, It was the best movie I've ever seen}
doc2 : {I don't like this movie. This is the worst movie I've ever seen.}
~~~
- Item이라는 컨텐츠를 벡터로 "Feature Extract" 과정을 수행해줌
- 빈도수를 기반으로 가장 많이 나오는 중요한 단어들을 잡아줌. 이러한 방법을 Cunter Vectorizer라고 함
- 하지만 Counter Vectorizer는 단순 빈도만을 계산하기에 조사, 관사처럼 의미는 없지만 문장에 많이 등장하는 단어들도 높게 처주는 한계가 있음. 이러한 단어들에는 패널티를 줘서 적절하게 중요한 단어만을 잡아내는게 TF-IDF 기법임

### TF-IDF 예시
~~~
문서  내용
0    먹고 싶은 사과
1    먹고 싶은 바나나
2    길고 노란 바나나 바나나
3    저는 과일이 좋아요
~~~
- 문서내 단어의 TF값 계산
- 문서내 단어의 IDF값 계산
- TF-IDF값을 계산한 후, 유사도 계산법을 사용하여 가까운 문서들을 찾아내면 됨
- 직관적인 해석이 가능함
- 대규모 말뭉치를 다룰 때 메모리 상의 문제가 발생
  - 높은 차원을 가짐
  - 매우 sparse한 형태의 데이터임
- 자연어 처리 같은 경우, 불필요한 데이터가 많음. 불용어, 관사 등.. 