# chapter03_데이터 전처리
- 모델에 어떤 예측 변수가 들어가냐는 매우 중요
- 데이터 변환을 통한 모델 성능 향상 가능
- 특정한 변형 방법은 어떤 모델에는 좋고, 어떤 모델에는 안좋을 수 있음  
- feature engineering 경우, 종속 변수의 특징에 따라 다양하게 적용 가능  
  ex) 종속변수가 계절적 특성을 가지고 있을 경우 계절별로 그룹핑하면 좋은 결과를 이끌어 낼 수 있음  
  ex) 특정 달이 다른 달보다 성공률이 높은 경우  
- 어떤 특징 변형 방법이 좋다고 물어본다면, 그때 그때 다름

## 사례 연구 : 하이콘텐츠 스크리닝에서의 세포 분할
- 하이콘텐츠 스크리닝  
  세포에서 원하는 부분이 드러나게 하는 물질로 샘플 염색  
  연구자가 세포핵의 크기나 모양을 세고 싶으면, 샘플 세포의 DNA를 염색  
  세포를 자연상태 그대로 보존하는 물질을 사용해 이 세포 고정  
  빛이 왜곡되는 부분을 탐지기를 통해 탐지  
  특정 파동이 흩어지는 정도를 측정하는 도구를 사용해 샘플에서 필요한 정보를 얻을 수 있음
- 2019개의 세포로 이뤄진 데이터 세트 사용
- 1300개 --> 제대로 구분되지 않음, 719개 --> 잘 구분됨
- train Dataset : 1009개
- 모든 세포에 대해 116가지 특징을 측정. 이를 통해 세포의 분할 정도를 측정하도록 함

## 개별 예측 변수에 대한 데이터 변형
### 1. 중심화와 척도화(Scaling)
- 장점
  - 중심화는 평균을 0, 표준편차를 1로 바꾸는 작업인데, 계산 시의 수치적 안정성을 향상시켜 줌
  - PLS 같은 모델은 예측 변수가 공통 척도를 갖는 것이 유리
- 단점
  - 데이터가 더 이상 원래 값이 아니게 돼 각각의 값을 해석하기 어려워짐

### 2. 왜도 해결을 위한 변형
- 오른쪽 또는 왼쪽으로 쏠리는 현상  
  일반적으로 오른쪽으로 쏠린다는 것은 데이터가 주로 왼쪽에 포진되어 있다는 것
- 로그, 제곱근, 역과 같은 변환을 통해 왜도를 줄일 수 있음
- Box-Cox 변환도 가능
  - 보다 직관적이고, 수치적 오류를 적게 일으키며 각각의 예측 변수를 변형하는 것보다 효과적 

## 여러 예측 변수 변형
### 1. 이상값 제거를 위한 데이터 변형
- 이상값 발견시 과학적으로 유의한 값인지 먼저 확인
- 기록상 오류가 있지 않은지 확인
- 성급하게 값을 삭제하지 말아야 함. 특히 데이터 값이 매우 적은 경우 더 유의해야 함
- 확인 사항
  - 데이터 샘플 수가 적을 때, 사실은 데이터가 한쪽으로 치우친 경우  
    왜도를 확인 할 정도로 데이터가 충분치 않았던 경우
  - 샘플 연구하에서 전체 데이터 중 일부분만을 보여주는 것일 수도 있음 
  - 데이터 수집 방법에 따라 데이터의 주류 바깥에 자리 잡은 유효한 데이터 "군집"이 다른 샘플들과 다른 집단에 속해 있을 수도 있음
- 이상값에 대응하는 몇가지 예측 모델 존재
  - 트리 기반 분류 모델
  - SVM 모델  
    예측 방정식을 만들 때 훈련용 샘플 세트의 일부는 무시  
    이런 제외된 샘플은 데이터의 주류 바깥에 있거나 판단 범위에서 떨어져 있는 경우
- 모델이 이상값에 민감한 경우, 예측 변수를 <b/>공간 변형</b>을 할 수 있음  
  - 모든 샘플을 공간의 가운데서 동일한 거리를 두도록 보정  
  - 분모는 예측 변수 분포의 중심값으로부터 거리의 제곱값을 취함
  - 중심을 구하거나 나누는 것과 달리 예측 변수를 수정할 때는 한 그룹단위로 해야함
  - 예측 변수에 공간 변형을 한 후에 변수를 제거하면 문제 발생할 수 있음

### 2. 데이터 축소와 특징 추출
- 보통 신호 추출, 특징 추출 기법이라고도 부름
- 일반적으로 사용되는 축소 기법 : PCA
- 예측 변수간의 상관성이 높은 경우, 주성분 분석을 통해 축소 할 수 있음
- 주성분 분석(PCA)
  - PCA를 사용할 때에는 이에 대한 충분한 이해를 바탕으로 사용해야 함  
    PCA에서 생성된 예측 변수는 각 변수에 대한 추가 내용(측정 척도, 분포 등)이나 모델링 객체에 대한 정보(응답 변수 등)를 고려하지 않고 만들어졌기 때문 
  - 분산이 큰 예측 변수부터 압축하는 식으로 만들어짐  
    원예측 변수가 서로 다른 단위를 사용하고 있을 때는 큰 분산을 가지는(상대적으로 단위가 큰) 예측 변수에 대한 가중값이 클 것  
    --> 데이터 간의 중요한 상관관계에 기반한다기보다 측정 단위 기준의 데이터 구조를 밝히는 쪽에 좀 더 초점을 맞추고 있다는 말이기도 함
- 주성분 분석 사용시 고려할 점
  - 예측 변수의 쏠린 형태를 변형하는 것이 좋음
  - 예측 변수의 중심화와 척도화를 진행
  - PCA 진행
- PCA는 비지도 학습이므로, 응답변수는 고려하지 않는다는 점  
  예측 변수의 분산과 응답 변수와의 관계가 전혀 없다면,  
  계산돼 나온 PC는 응답 변수와 적절한 관계가 만들어지지 않을 것임
  --> 이 때는 PLS 같은 지도 기법을 사용해서 어느정도 해결 가능
- PCA 주성분 데이터끼리의 산점도 그래프 등을 그려봄으로써 데이터 품질을 보장하고 전반적인 문제에 대한 직관을 얻기 위한 매우 중요한 단계
- 그래프를 통해 군집화하거나 이상값을 찾아낼 수 있음
- 성분을 그래프로 나타낼 때는 데이터의 분산이 작을 수록 척도가 작게 나타날 수 있으므로 유의
- PCA의 선형식을 통해 어떤 예측 변수가 더 중요도를 가지는지 확인할 수 있는데,
  중요한 것은 가중치가 높다고 해서 예측 결과에 중요한 영향을 미친다고는 볼 수 없음  
  --> why? 분산 자체가 반응 변수에 영향을 미치지 않을 수도 있기 때문 

## 결측값 처리 
- 어떻게 값이 누락됐는지 이해하는 것이 중요
- 결측값의 패턴 자체도 정보를 줄 수 있는데, 이를 정보성 결측값(informative missingness)라고 함  
ex) 약이 효과가 없을 때 환자는 의사를 찾아오지 않거나 중도 하차하는 경우, 해당 결측값 자체는 의미를 가지고 있음  
- 결측값은 중도 절단값(censoring)과 구분되어야 함
- ** 중도 절단 값 : 원값이 누락됐지만 다른 무언가를 통해 값을 추정할 수 있는 값  
ex) 계측치 한계로 인해 중도 절단 된 경우  
ex) 우편으로 영화 디스크를 대여하는 회사에서 고객이 영화를 갖고 있는 기간 --> 중도 절단 값

- 전통적 통계 모델에서는 해석과 추론에 초점을 맞추므로 중도 절단은 중도 절단 방법에 따른 가설을 만들어 이를 결과에 고려하는 것이 일반적
- 예측 모델에서는 결측값으로 두거나 중도 절단값을 관측값으로 사용  
 ex) 샘플에 계측치 한도보다 작은 값이 있으므로, 계측치 한도 값을 대체하는 경우
- 결측값은 경험상 보통 샘플보단 예측 변수 자체에 관련된 경우가 많았음
  --> 전체 예측 변수에 걸쳐 임의로 발생하는 것이 아니라, 특정 예측 변수들에 집중해서 나타나곤 함
- 결측치를 제거할 정도로 샘플이 많지 않은 경우 다음 두 가지 방식 사용 가능ㄴ
  - 예측 모델, 특히 트리 기반 기법에서 다른 방식으로 다룰 수 있음
  - 결측 값 대체

### 결측값 대체
- 결측 값 대체의 경우, 통계학에서는 명확한 가설 검정 절차 맥락에서는 결측값을 그대로 사용하는 경향이 있음. 대치는 소수의 결측치에서만 사용
- 결측값 대치법은 다른 예측 변수값을 기반으로 예측 변수값을 추정하기 위한 모델링에서의 한가지 단계
- 가장 적절한 대치 방법은 훈련 데이터 세트를 활용해 데이터 세트의 각 예측 변수별 대치 모델을 구축하는 것
- 결측값이 많지 않다면, 에측 변수간 EDA를 수행하는 것도 좋은 방법  
  ex) PCA 같은 방법이나 시각화를 통해 예측 변수 간 강한 연관 관계가 있는지를 판단 할 수 있음  
      결측값이 있는 변수가 결측값이 거의 없는 예측 변수와 강한 상관성이 있는 경우, 몇가지 모델들이 대치법에 효과적으로 사용 됨
- 결측 값 대치를 위해 KNN 알고리즘 많이 사용  
  - 장점  
   대치된 데이터 값이 훈련 데이터 세트 값의 범위 안으로 한정됨
  - 단점  
   전체 훈련 데이터 세트에서 매번 결측값을 대치
- KNN은 결측값의 양이나 튜닝 변수에 크게 영향을 받지 않는다는 것을 밝혀냄
- 결측값을 예측하기 위해 상관성이 높은 두 예측 변수를 선형회귀모델을 만들 수 있음

## 예측 변수 제거
- 예측 변수 제거시 이점
  - 예측 변수가 적을수록 연산 속도가 적음
  - 두 예측 변수가 높은 상관관계를 갖는다면,  두 변수가 동일한 정보를 내포하고 있다고 볼 수 있음
  - 어떤 모델은 퇴화 분포 형태의 예측 변수를 사용해 성능이 나빠 질 수 있음
- 단일 값을 가지는 예측 변수는 제거하는 것이 대부분 좋음
  - 트리 기반 모형에서는 절대 사용되지 않는 변수
  - 회귀 모형에서는 계산 오류 발생
- 매우 낮은 빈도로 나타나는 값들로 채워져 있을 때도 제거 고려
  -  ex) 대부분의 샘플에는 값이 0 이고 몇개의 샘플에만 값이 있을 때는 리샘플링시 train Dataset에 전부 값이 0인 데이터로 편성될 수 있음 
  - 데이터의 고유한 값의 수가 샘플의 수에 비해 작은 경우에 해당  
    ex) 단어가 나타나는 문서의 수를 나타낸 예측 변수가 있다고 했을 때, 빈도 0: 523, 빈도 2 : 6 인 것 처럼 빈도의 불균형이 심한 경우를 말함
- 0에 가까운 분산을 갖는 예측 변수를 감지하는 통상적인 법칙
  - 샘플 크기 내 고유한 값의 비율이 낮다(10% 미만)  
    ex) 빈도 4개 / 총문서 531 = 0.8% 
  - 가장 일반적으로 나타나는 값의 빈도 대비 두 번째로 일반적인 값의 빈도 비가 20 이상
- 위의 두 경우에 해당하고, 모델이 이러한 예측 변수에 민감하다면 모델에서 해당 변수를 제거하는 것이 좋음

## 예측 변수 간의 상관관계
- 데이터 세트가 너무 많은 예측 변수들로 구성되어 있다면 PCA 같은 기법을 통해 문제의 크기를 정리할 수 있음
- 일반적으로 높은 상관관계를 보이는 예측 변수는 피하는 것이 좋음
  - 중복되는 예측 변수는 종종 모델에 정보량보다 복잡도를 높이는데 기여
  - 수학적으로도 불리
    - 선형 회귀 같은 경우, 높은 상관관계를 가지는 변수를 사용하면 불안정한 모델 생성되거나  
      수치 오류를 발생시키거나 예측 성능이 낮아지는 등의 문제 발생
- 고전적 회귀 분석에서는 선형 회귀에서의 다중 공선성을 진단하기 위한 여러가지 도구 존재
  - 공선성이 있는 예측 변수는 모델에서 추정하는 변수의 분산에 영향을 미칠 수 있어  
    분산팽창지수(VIF)라는 통계값을 사용해 영향력 있는 예측 변수 구분
- VIF의 문제점
  - 선형 모델에 사용하기 위하여 만들어져 예측 변수보다 많은 샘플 필요
  - 어느 쪽의 변수가 제거되어야 하는지 판단하지 않음  
    --> 이 문제를 해결하기 위해 간단한 알고리즘을 적용할 수 있음  
    1. 예측 변수의 상관계수 행렬을 구함
    2. 절댓값이 가장 큰 상관계수를 보이는 예측 변수 쌍을 구함(A, B)
    3. A와 다른 변수 간의 평균 상관계수를 구함. B에 대해서도 동일하게 수행
    4. A의 평균 상관계수가 더 크다면 A를 지우고, 아니라면 B를 지움
    5. 이후 특정 수치를 넘는 상관계수의 절대값이 나타나지 않을 때까지 1~4번 반복
- 예측 변수 간의 상관관계에 민감한 모델을 사용하는 경우에는 보통 한도를 0.75로 잡음

## 예측 변수 추가
- 범주형 데이터 같은 경우 가변수(dummay variable)로 변환해야함
- 모든 가변수를 포함하여 모델을 생성하는 경우, 절편이 포함된 선형 회귀 같은 경우, 문제가 발생할 수 있음. why? --> 가변수의 계산 값이 항상 1이기 때문
- 위와 같은 오류가 발생하지 않는 모델 같은 경우는 가변수를 모두 포함해 모델의 해석력을 높일 수 있음
- 예측 데이터에 데이터를 복잡하게 조합한 것을 추가하는 기법도 있음  
  - ex) 분류 모델의 경우, 각 클래스별 예측 변수의 중심값인 "클래스 중심"을 구하고, 예측 변수에 대한 각 클래스 중심으로부터의 거리를 구해 거리값을 모델에 추가
- 트리 기반 모델에서 분기점을 해석할 때에는 예측 변수의 모든 정보가 가변수화 돼 있는 경우가 더 쉬움. 따라서 모든 가변수를 만드는 것을 추천

## 예측 변수 구간화
- 데이터세트를 단순화하는 일반적인 방법 중 하나
- 다음과 같은 범주 중 2개 이상이면 SIRS(전신성 염증반응증후군)으로 판단
  - 체온이 36.C미만이거나 38.C 초과
  - 분당 심장 박동 수가 90회 초과
  - 분당 호흡 수가 20회 초과
  - 백혈구 수가 4,000개/mm^3 미만이거나 12,000개/mm^3 초과

- 위와 같은 설문지 형태의 구간화의 장점
  - 판단 규칙이 간단하거나 모델 해석이 간단함
  - 모델러가 예측 변수와 결과 간의 정확한 관계를 미리 알아야 할 필요가 없음
  - 응답률이 높을 수 있음  
  ex) 파상풍을 맞은 날짜보다 맞은 기간 범위를 물어보면 더 대답할 확률이 높아짐

- 위와 같은 구간화의 단점
  - 모델의 유의한 성능 저하가 나타날 수 있음  
    why? 많은 모델링 기법은 예측 변수와 결과값 간의 복잡한 관계를 잘 판단함  
    괜히 구간화하여 이러한 능력을 제한할 필요가 없음
  - 예측 변수가 범주화 될 경우, 모델이 너무 단순해짐
- 결론적으로 해석쪽으로 원한다면 범주화를 고려해도 좋고, 성능쪽으로 원한다면 피하는 것이 좋음

