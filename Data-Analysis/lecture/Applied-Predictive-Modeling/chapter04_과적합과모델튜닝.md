# chapter04_과적합과 모델 튜닝
## 4.1 과적합 문제
- 대부분의 모델링 문제에서는 보통 훨씬 고차원 데이터를 다루게 되는데,  
  모델이 훈련 데이터 세트에 얼마나 과적합돼 있는지를 파악하기 위한 방안을 갖추는 것이 매우 중요


## 4.2 모델 튜닝
- 많은 모델에서는 데이터에서 바로 추정이 어려운 매개변수들이 있음  
  --> hyper paramter
- SVM 같은 경우 cost라는 매개변수가 존재하는데, 값이 커지면 모델은 모든 변수에 대해 값을 정확히 매기기 위해 선이 매우 길어짐  
값이 작아지는 경우 모델은 좀 더 무난한 형태가 됨
- KNN 기법 같은 경우 K parameter는 동률이 나올 경우를 생각하여, 홀수만 지정
- 유전 알고리즘이나 simplex search 기법 등을 이용하여 최적의 튜닝 변수도 찾을 수 있음  
  이런 과정을 통해 알고리즘적으로 최적의 튜닝 파라미터 값을 구할 수 있음


## 4.3 데이터 분할
- 모델의 일반적인 구축 단계는 다음과 같음
  - 예측 데이터 전처리
  - 모델 변수 추정
  - 모델에 사용할 예측 변수 선정
  - 모델 성능 평가
  - 예측 규칙 미세 조정(ROC 곡선 등)

- 모델링 시 가장 먼저 결정해야 할 것은 어떤 샘플을 사용할 지 결정해야함 
### 임의 샘플링
- 데이터를 훈련 세트와 테스트 세트로 나누는 가장 간단한 방법
### 층위 임의 샘플링
- 데이터 분할 결과를 고려해야 할 경우 사용
- 연속형 데이터도 층별 샘플링이 가능함  
  숫자값을 여러 그룹으로 나누고, 이 그룹 내에서 임의 샘플링 진행
### 최대 비유사도 샘플링(maximum dissimilarity sampling)
- 예측 변수값을 기준으로 나눔
- 두 샘플간의 비유사성을 여러 방법으로 구할 수 있는데, 가장 간단한 방법은 두 샘플 예측 변수 간 거리를 구함
- 비유사도를 데이터 분할에 사용하는 경우
  1. 테스트 세트는 1개의 세트(row)로 부터 시작
  2. 초기 샘플과 어디에도 속하지 않은 샘플 간의 비유사도를 계산
  3. 계산된 샘플 중 가장 비유사도가 큰 데이터 샘플을 test Dataset에 포함  
  4. 샘플들을 테스트 세트에 더 할당한 후에는 그룹 간의 비 유사성을 구함    
     비유사도의 평균값이나 최솟값을 사용
  5. 원하는 크기의 테스트 데이터 샘플이 나올 때 까지 1~4번을 반복

## 4.4 리샘플링 기법
### K-fold cross validation
- k는 보통 5나 10을 선택하지만 정해진 것은 없음
  - repeated K-fold cross validation
  - LOOCV(leave-one-out cross-validation)
- k겹 교차 검증은 일반적으로 다른 방법에 비해 분산이 크기 때문에 많이 사용되지 않음  
  만약 훈련 데이터셋이 클 경우, 분산과 편향도는 크게 중요하지 않음
- 작은 k값을 사용해 생긴 편향도는 부트스트랩의 편향도와 유사하지만, 분산은 훨씬 큼

### 일반화 교차 검증(Generalized cross-validation)



### 반복적 훈련/테스트 세트 분할
- leave-group-out cross-validation 이나 몬테 카를로 교차 검증 등으로 알려져 있음
- 간단하게 데이터를 모델링 및 예측에 사용할 용도로 여러 개로 분할
- 각 train/test 에 들어갈 데이터의 비율은 사용자가 정함. 보통 반복 횟수에 따라 달라짐
-  k-fold와 다른 점은 샘플이 추출된 부분 집합에 여러번 나타날 수 있음
- 보통 반복 횟수가 k-fold cross-validation 보다 큼
- 리샘플링 기법의 편향도는 부분 집합 데이터의 양이 모델링 세트의 양에 근접할수록 줄어듬  
  통상적으로 좋은 비율은 75 ~ 80%임  
  반복 횟수가 클 때는 비율이 더 높아도 괜찮음
- 보다 안정적인 성능 추정값을 얻고 싶다면, 반복 횟수를 보다 크게 잡아(50 ~ 200회)를 추천
- 부분 집합 데이터의 비율이 커질수록 반복 횟수도 증가해야 성능 추정값의 불확실성을 줄일 수 있음

### 부트스트랩(bootstrap)
- 부트스트랩 샘플은 원데이터 샘플의 크기와 동일
- 따라서 어떤 샘플은 부트스트랩 샘플에 여러번 나타나기도 하고, 전혀 나타나지 않을 수 있음
- 선택되지 않은 샘플은 out-of-bag sample이라고 부름
- 부트스트랩 리샘플링을 반복함에 따라 선택된 샘플로 모델이 만들어지고, 범위 외 샘플을 사용해 예측
- 부트스트랩 오차율은 k-fold cross validation에 비해 불확실성이 적게 나타남
- 평균적으로 부트스트랩 샘플 중 63.2%의 데이터가 한 번 이상 나타나므로 , k ~= 2 인 경우의 k-fold cross-validation과 비슷한 편향도를 보임
- 훈련 세트 크기가 작은 경우, 편향이 생길 수 있으며, 훈련 세트 크기가 커질수록 줄어듬
- 편향 현상을 제거하기 위한 방안이 제시됨
  - 632 기법
    - 편향 문제가 단순 부트스트랩 추정과 훈련 데이터 세트를 예측에 재사용해 추정하는 결과를 합쳐  
      성능 추정값을 구하는데서 발생했다고 지적

## 사례 연구: 신용 평가
- 신용 평가 데이터는 좋음(good), 나쁨(bad)으로 신용 평가가 구분돼 붙은 1,000개의 샘플이 있음
- 모델 정확도를 평가할 때, 모델이 선정되려면 최소 기본 정확도가 70% 이상은 되어야 함

## 최종 튜닝 변수 설정
- 복잡한 모델은 훈련 세트에 최적화 될 수 있음을 염두
- 복잡한 모델 보단 단순한 쪽을 선택하는 것이 좋고, 덜 복잡한 모델을 선택하는 다른 방법은  
  적당한 성능의 보다 단순한 모델을 선택하는 식임
- 단순한 모델을 고르기 위한 방법 중 하나는 1-표준오차 기법이 존재  
  최적의 수치와 이에 따른 표준 오차를 구하고, 성능이 최적 수치에서 1 표준 오차 내인 모델 중 가장 단순한 모델을 구하는 방법
- 단순한 모델을 고르는 방법은 사용자가 기준을 만들어 선택 할 수도 있음
- 각각의 평균 정확도의 표준편차는 다른 기법들에 비해 10-fold cross validation이 큼  
  why? 표준편차는 리샘플링을 사용한 횟수에 직결된다는 것을 기억하자
- 연산 소요 시간은 각기 다름

## 추천하는 데이터 분할 방식
- 단일의 독립 테스트 세트를 만들어 적용할 때 중요한 기술적 유형이 있음
  - 테스트 세트는 모델을 단독으로 평가하므로, 결과의 불확실성을 정의하는 데는 한계가 있음
  - 비율적으로 큰 테스트 세트는 성능 추정시 편중 정도가 증가할 수 있음  
    why? train dataset의 크기가 상대적으로 작아져 모델이 학습하는데 충분치 않기 때문. --> underfitting 발생  
  - 샘플 크기가 작을 경우, 
    - 모델값을 적절하게 결정하는 데 모든 데이터가 필요할 수 있음
    - 테스트 데이터셋이 아주 다른 결과를 내는 등 상대적으로 불확실성이 커질 수 있음
  - 리샘플링 방식을 통해 모델이 다른 샘플 데이터에 대해서도 얼마나 잘 동작할지 잘 예측 할 수 있음

- 추천 분할 방식
  - 샘플의 크기가 작을 경우,
    - repeated k-fold cross-validation을 추천  
      why? 편향도나 분산값이 좋고, 주어진 샘플 크기 대비 연산 비용이 크지 않음
  - 모델 성능 비교 및 제일 좋은 모델을 선택할 경우,
    - 부트스트랩 방식 추천  
      why? 분산이 매우 낮기 때문
  - 샘플의 크기가 클 경우,
    - k-fold cross validation 추천  
      why? 리샘플링 기법 간의 차이는 크지 않기 떄문에 연산 효율성이 가장 좋은 것 선택

- hyper paramter 튜닝 중 모델 성능을 평가할 때 편향이 발생할 수 있음  
  but 데이터가 큰 경우, 편향도가 경험상 작았음

## 모델 선택
- 데이터의 특성과 질문 유형에 따라 달라지지만, 다음과 같이 최종 모델의 유형을 결정하는 법을 제안
  1. 가장 해석하기 어렵고, 가장 유연한 boost Tree, SVM 같은 모델부터 시작
  2. 조금이라도 non-blackbox인 MARS, 부분 최소 자승법, 일반화 가법 모델, Naive Bayes 같은 모델 탐색
  3. 더 복잡한 방법의 성능에 어느 정도 근접하면서 가장 단순한 모델을 찾음

- 이러한 방법을 사용하면 모델러들은 <b/>'성능 천장'</b>을 발견 할 수 있음
- 모델을 사용하는 사람마다 각 방법의 장점(계산 복잡도, 예측 용이성, 해석력 등)별 선호도가 있었음
  - SVM, Random Forest, XGBoost 등은 정확도 위주
  - MARS, LM 등은 해석력 위주

### 짝 비교 기법(paired comparison)
- 리샘플링 결과 기반 기법들을 통계적으로 비교하는 기법 
- 동일하게 리샘플링된 데이터 세트를 사용해 정확도를 측정하여 두 개의 모델 성능을 비교하는 방식


## 추가설명
- 모델 성능 추정시 편중(Bias)란?  
  train dataset이 상대적으로 작아 학습이 덜되어 underfitting이 발생함에 따라 생기는 bias를 말함
- 모델 성능 추정시 분산(variance)란?  
  train dataset에 너무 overfitting 되면, test dataset으로 모델 성능 판단시, 어떤 test dataset은 매우 잘맞추고,
  어떤 test dataset은 못맞추는 현상이 발생하므로, variance가 커짐




